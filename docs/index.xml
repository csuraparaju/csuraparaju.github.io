<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Krish&#39;s blog</title>
<link>https://csuraparaju.github.io/</link>
<atom:link href="https://csuraparaju.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Krish Suraparaju&#39;s personal website and blog.</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Wed, 15 Oct 2025 04:00:00 GMT</lastBuildDate>
<item>
  <title>The Metropolis-Hastings Algorithm for Sampling</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/metropolis-hastings/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In a previous blog <a href="https://csuraparaju.github.io/posts/sampling/">post</a>, I discussed various algorithms for sampling from a probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. These algorithms all made a simple assumption: we can compute <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> efficiently for all <img src="https://latex.codecogs.com/png.latex?x"> in our sample space <img src="https://latex.codecogs.com/png.latex?%5Cchi">. However, this assumption might be too strong for certain cases. What if computing <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)"> is computationally intractable? We can’t use the methods developed in that post. What do we do instead?</p>
</section>
<section id="protein-folding" class="level2">
<h2 class="anchored" data-anchor-id="protein-folding">Protein Folding</h2>
<p>As an example of a situation where <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is uncomputable, let’s study protein folding. You might recall that a protein is a chain of amino acids (also called residues) that folds into a certain structure. One of the simplest models for this process is the Hydrophobic-Polar (HP) lattice model, where we have a chain of <img src="https://latex.codecogs.com/png.latex?N"> residues, each classified as either hydrophobic (H) which is water-repelling, or polar (P) which is water-loving. Chemistry tells us that hydrophobic residues tend to cluster together, and so the final folded structure of an acid sequence is heavily influenced by <img src="https://latex.codecogs.com/png.latex?H"> and <img src="https://latex.codecogs.com/png.latex?P"> residues. We can see this effect in the diagram below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="hydrophobic_collapse.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://csuraparaju.github.io/posts/metropolis-hastings/hydrophobic_collapse.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%"></a></p>
</figure>
</div>
<p>On the left is an unfolded amino acid sequence, with hydrophobic residues in black and polar particles in white. Observe that after folding, the hydrophobic residues clustered together on the “inside” of the protein, while the polar residues are closer to the surface of the protein.</p>
<p>Mathematically, the folded structure of this amino acid can be modeled as a graph on a 2D integer lattice where each residue is mapped to a point in space. However, we can’t just consider any arbitrary graph. To keep this model realistic, we need to have the restriction that adjacent amino acids in the sequence are adjacent in the lattice (since the amino acid sequence cannot be permuted when folding), and no two residues occupy the same point in space. We can define this more formally as follows:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>HP Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>An amino acid sequence is a string <img src="https://latex.codecogs.com/png.latex?s%20%5Cin%20%5C%7BH,%20P%5C%7D%5EN"> of length <img src="https://latex.codecogs.com/png.latex?N">. A folded configuration of this sequence is a function <img src="https://latex.codecogs.com/png.latex?%5Comega%20:%20%5B1,%20%5Ccdots,%20N%5D%20%5Cto%20%5Cmathbb%7BZ%7D%5E2"> where <img src="https://latex.codecogs.com/png.latex?%5Comega(i)"> gives the position of the <img src="https://latex.codecogs.com/png.latex?i">-th amino acid in <img src="https://latex.codecogs.com/png.latex?s"> on the lattice, such that</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%7C%7C%5Comega(i)%20-%20%5Comega(i+1)%7C%7C_2%20=%201"> for all <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%3C%20N"> (adjacent residues in the sequence are adjacent in the lattice)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Comega(i)%20%5Cneq%20%5Comega(j)"> for all <img src="https://latex.codecogs.com/png.latex?i%20%5Cneq%20j"> (two residues cannot occupy the same point in space)</li>
</ul>
<p>Chemistry tells us that the energy of a configuration <img src="https://latex.codecogs.com/png.latex?%5Comega"> depends on hydrophobic contacts. We say two residues at positions <img src="https://latex.codecogs.com/png.latex?%5Comega(i)"> and <img src="https://latex.codecogs.com/png.latex?%5Comega(j)"> are in contact if they are adjacent on the lattice but not consecutive in the sequence so that <img src="https://latex.codecogs.com/png.latex?%7Ci%E2%88%92j%7C%20%3E%201">. This way, we only count the interactions of residues that moved around in 2D space to cluster together, rather than ones that were already next to each other. The energy is then: <img src="https://latex.codecogs.com/png.latex?E(s,%20%5Comega)%20=%20%5Csum_%7Bi%20%5Csim%20j%7D%20E_%7Bs_i,%20s_j%7D"> where <img src="https://latex.codecogs.com/png.latex?i%20%5Csim%20j"> iff <img src="https://latex.codecogs.com/png.latex?%5Comega(i)"> is in contact with <img src="https://latex.codecogs.com/png.latex?%5Comega(j)">, and <img src="https://latex.codecogs.com/png.latex?E_%7BHH%7D%20=%20-1">, <img src="https://latex.codecogs.com/png.latex?E_%7BHP%7D%20=%20E_%7BPH%7D%20=%20E_%7BPP%7D%20=%200">, so only <img src="https://latex.codecogs.com/png.latex?HH"> contacts contribute to lowering the energy, and giving us a way to quantify the clustering behavior of <img src="https://latex.codecogs.com/png.latex?H"> residues.</p>
</div>
</div>
<p>Below is an example sequence <img src="https://latex.codecogs.com/png.latex?HPPHPH"> and possible folded configurations <img src="https://latex.codecogs.com/png.latex?%5Comega">. <img src="https://latex.codecogs.com/png.latex?H"> amino acids are colored green and <img src="https://latex.codecogs.com/png.latex?P"> amino acids are in silver. The configuration <img src="https://latex.codecogs.com/png.latex?%5Comega"> that has the minimum energy of all the listed configurations is circled in red, since it has three <img src="https://latex.codecogs.com/png.latex?HH"> contacts, most out of the others. Image courtesy of <a href="https://math.mit.edu/classes/18.417/Slides/HP-protein-prediction.pdf">MIT: 18.417</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="hp_model.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://csuraparaju.github.io/posts/metropolis-hastings/hp_model.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
<p>Given this set up, our goal is to study the folded protein structure of an amino acid sequence. Specifically, we want to understand what a typical folded configuration looks like at temperature <img src="https://latex.codecogs.com/png.latex?T">. Thermochemistry tells us that protein folding can actually be thought of sampling from a distribution proportional to the Boltzmann distribution: <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_u(%5Comega%20%7C%20s)%20:=%20%5Cmathbb%7BP%7D%5B%5Comega%20%7C%20s%5D%20=%20e%5E%7B-%5Cfrac%7BE(s,%20%5Comega)%7D%7Bk_B%20T%7D%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?T"> is the temperature, <img src="https://latex.codecogs.com/png.latex?k_B"> is the Boltzmann constant. To turn <img src="https://latex.codecogs.com/png.latex?%5Cpi_u"> into a probability distribution, we need to compute a normalizing constant <img src="https://latex.codecogs.com/png.latex?%0AZ%20=%20%5Csum_%7B%5Comega%7D%5Cpi_u(%5Comega%20%7C%20s)%0A"> and define <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(%5Comega%20%7C%20s)%20:=%20%5Cfrac%7B1%7D%7BZ%7D%20%5Cpi_u%20(%5Comega%20%7C%20s)%0A"></p>
<p>We can now use the algorithm developed at the end of this <a href="https://csuraparaju.github.io/posts/sampling/">post</a> to draw samples from this distribution and understand what typical folded protein configurations look like at temperature <img src="https://latex.codecogs.com/png.latex?T">. There’s just one problem: the normalizing constant <img src="https://latex.codecogs.com/png.latex?Z">.</p>
</section>
<section id="intractability-of-z" class="level2">
<h2 class="anchored" data-anchor-id="intractability-of-z">Intractability of <img src="https://latex.codecogs.com/png.latex?Z"></h2>
<p>We need to compute <img src="https://latex.codecogs.com/png.latex?%5Cpi_u(%5Comega%20%7C%20s)"> for all possible <img src="https://latex.codecogs.com/png.latex?%5Comega"> given a specific <img src="https://latex.codecogs.com/png.latex?s"> to find <img src="https://latex.codecogs.com/png.latex?Z">. What could happen if the number of possible configurations is computationally intractable? In this case, it turns out that there are about <img src="https://latex.codecogs.com/png.latex?3%5EN"> different <img src="https://latex.codecogs.com/png.latex?%5Comega">’s. This is because we can place the first residue anywhere on the lattice, and so the second residue has <img src="https://latex.codecogs.com/png.latex?4"> choices (up, down, left, right). The third residue has at most three choices (since we can’t backtrack to residue <img src="https://latex.codecogs.com/png.latex?1">’s position), and similarly with the fourth residue. We can continue this argument for <img src="https://latex.codecogs.com/png.latex?N"> and get the rough bound <img src="https://latex.codecogs.com/png.latex?3%5EN"> for the number of possible configurations.</p>
<p>This is a huge number! Even if we have a relatively small amino acid sequence, with <img src="https://latex.codecogs.com/png.latex?N%20=%20100">, there are roughly <img src="https://latex.codecogs.com/png.latex?3%5E%7B100%7D"> possible configurations to try! Even our fastest computers today will take about <img src="https://latex.codecogs.com/png.latex?10%5E%7B22%7D"> years to finish calculating that a sum of that many terms. What’s worse is that there is no smarter way to compute <img src="https://latex.codecogs.com/png.latex?Z">. We need to evaluate <img src="https://latex.codecogs.com/png.latex?%5Cpi_u"> at every single <img src="https://latex.codecogs.com/png.latex?%5Comega">. This makes it impossible to use our previously developed algorithms since it is hopeless to try to compute <img src="https://latex.codecogs.com/png.latex?%5Cpi%20(%5Comega%20%7C%20s)">. We need to think of another way to solve this problem.</p>
<p>If we can’t compute <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega%20%7C%20s)"> directly, are there quantities that we can compute? Well, we can evaluate <img src="https://latex.codecogs.com/png.latex?%5Cpi_u%20(%5Comega%20%7C%20s)"> since that just requires counting the <img src="https://latex.codecogs.com/png.latex?HH"> contacts for a single <img src="https://latex.codecogs.com/png.latex?%5Comega">. Does this help us come up with a way to understand how <img src="https://latex.codecogs.com/png.latex?%5Cpi"> looks like? Suppose we have two configurations, <img src="https://latex.codecogs.com/png.latex?%5Comega"> and <img src="https://latex.codecogs.com/png.latex?%5Comega'">. While we don’t know their individual probabilities <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega%20%7C%20s)"> or <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega'%20%7C%20s)">, observe what happens when we look at their ratio:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpi(%5Comega'%20%7C%20s)%7D%7B%5Cpi(%5Comega%20%7C%20s)%7D%20=%20%5Cfrac%7B%5Cpi_u(%5Comega'%20%7C%20s)/Z%7D%7B%5Cpi_u(%5Comega%20%7C%20s)/Z%7D%20=%20%5Cfrac%7B%5Cpi_u(%5Comega'%20%7C%20s)%7D%7B%5Cpi_u(%5Comega%20%7C%20s)%7D%0A"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?Z"> terms cancel out entirely. We are left with a ratio of unnormalized weights (<img src="https://latex.codecogs.com/png.latex?%5Cpi_u">), which is just a simple calculation of <img src="https://latex.codecogs.com/png.latex?HH"> contacts. Now, notice that the fraction</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpi(%5Comega'%20%7C%20s)%7D%7B%5Cpi(%5Comega%20%7C%20s)%7D%0A"></p>
<p>tells us the relative probability of two configurations. If this ratio is greater than <img src="https://latex.codecogs.com/png.latex?1">, then <img src="https://latex.codecogs.com/png.latex?%5Comega'"> has higher probability than <img src="https://latex.codecogs.com/png.latex?%5Comega'">. If it’s less than <img src="https://latex.codecogs.com/png.latex?1"> then <img src="https://latex.codecogs.com/png.latex?%5Comega"> has higher probability. Using this, we can compare the likelihood of two configurations.</p>
<p>This should hopefully give you an idea. Since we can’t look at the whole landscape of <img src="https://latex.codecogs.com/png.latex?3%5EN"> configurations to pick a perfect sample, we start with a single, arbitrary configuration <img src="https://latex.codecogs.com/png.latex?%5Comega_0">. It’s almost certainly not a typical sample from <img src="https://latex.codecogs.com/png.latex?%5Cpi">, but that doesn’t matter yet. We want to produce a sequence of configurations <img src="https://latex.codecogs.com/png.latex?%5Comega_0,%20%5Comega_1,%20%5Comega_2,%20%5Cdots,%20%5Comega_t"> such that, as <img src="https://latex.codecogs.com/png.latex?t%20%5Cto%20%5Cinfty">, the configurations look less like our arbitrary start and more like they were drawn from <img src="https://latex.codecogs.com/png.latex?%5Cpi">. To do this, at each step <img src="https://latex.codecogs.com/png.latex?t">, we should propose a new candidate <img src="https://latex.codecogs.com/png.latex?%5Comega'">. Then, we should decide if we should update our current sample to <img src="https://latex.codecogs.com/png.latex?%5Comega'">, or keep <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> as the candidate. But how do we make this decision?</p>
<p>Hopefully you see that we can use our ratio <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpi(%5Comega'%20%7C%20s)%7D%7B%5Cpi(%5Comega_t%20%7C%20s)%7D"> as a way to filter. If the ratio is high, <img src="https://latex.codecogs.com/png.latex?%5Comega'"> is more likely than our current state. We should probably update our sample to <img src="https://latex.codecogs.com/png.latex?%5Comega'">. If the ratio is low, it means <img src="https://latex.codecogs.com/png.latex?%5Comega'"> is less likely. We should probably reject it and keep <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> as our sample for this step.</p>
<p>While our initial choice <img src="https://latex.codecogs.com/png.latex?%5Comega_0"> might be a bad representative of the distribution, the process is self-correcting. Each time we make an update, we are moving the sequence toward states that is more representative under <img src="https://latex.codecogs.com/png.latex?%5Cpi">. If we run this process for long enough, the first arbitrary choice <img src="https://latex.codecogs.com/png.latex?%5Comega_0"> gets drowned out. Eventually, the configurations the algorithm visits become more and more like they are samples drawn from the true distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>It turns out that <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> defined in this way is a Markov Chain. If you are unfamiliar with Markov Chains, I highly suggest you read my previous blog <a href="https://csuraparaju.github.io/posts/markov-chains/">post</a> on them. Be warned: the rest of this posts assumes you are familiar with the big ideas/theorems from that post!</p>
</section>
<section id="the-metropolis-hastings-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-metropolis-hastings-algorithm">The Metropolis-Hastings Algorithm</h2>
<p>Note: From here on, I’ll remove the dependence on <img src="https://latex.codecogs.com/png.latex?s"> for notational clarity, writing <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega)"> instead of <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega%20%7C%20s)">.</p>
<p>Notice that the process discussed at the end of the previous section can be thought of as a Markov Chain. We start out with some initial configuration <img src="https://latex.codecogs.com/png.latex?%5Comega_0">, and for each time step decide if we need to resample an <img src="https://latex.codecogs.com/png.latex?%5Comega_1"> or not. If we keep running this process, we’ll have a Markov Chain <img src="https://latex.codecogs.com/png.latex?%5Comega_t">. I’ll leave it as an exercise for you to prove that this process indeed does have the Markov Property.</p>
<p>To turn this self-correcting sequence idea into an algorithm, we need to formalize how we pick a new candidate and how we decide to accept it.</p>
<p>First, we need a way to pick a candidate <img src="https://latex.codecogs.com/png.latex?%5Comega'"> given our current state <img src="https://latex.codecogs.com/png.latex?%5Comega_t">. We call this the proposal distribution, denoted <img src="https://latex.codecogs.com/png.latex?Q(%5Comega'%20%7C%20%5Comega_t)">. In the case of our protein lattice, a proposal might be as simple as picking one residue at random and try to move it to an adjacent empty spot. This is just a mechanism to explore the space; <img src="https://latex.codecogs.com/png.latex?Q"> doesn’t need to know anything about the <img src="https://latex.codecogs.com/png.latex?HH"> contacts or the true distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Once we have our candidate <img src="https://latex.codecogs.com/png.latex?%5Comega'">, we need an exact rule for the filter. We want this rule to guarantee that the stationary distribution of our chain is exactly <img src="https://latex.codecogs.com/png.latex?%5Cpi">. This rule is the Acceptance Probability, <img src="https://latex.codecogs.com/png.latex?%5Calpha(%5Comega_t,%20%5Comega')">. It tells us the exact probability with which we should move to the new state. If we don’t move, we simply stay at <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> for another time step.</p>
<p>Combining the ideas gives us the Metropolis-Hastings algorithm</p>
<div class="algorithm">
<p><strong>Metropolis-Hastings Algorithm</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Comega_0%20%5Cgets"> initial configuration</p>
<p>for <img src="https://latex.codecogs.com/png.latex?t%20=%200"> to <img src="https://latex.codecogs.com/png.latex?T-1">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> Propose a new configuration <img src="https://latex.codecogs.com/png.latex?%5Comega'"> with probability <img src="https://latex.codecogs.com/png.latex?Q(%5Comega_t,%20%5Comega')"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> Define the Acceptance Probability <img src="https://latex.codecogs.com/png.latex?A(%5Comega_t,%20%5Comega')"> by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AA(%5Comega_t,%20%5Comega')%20:=%20%5Cmin%5Cleft%5C%7B1,%20%5Cfrac%7B%5Cpi_u(%5Comega')Q(%5Comega',%20%5Comega_t)%7D%7B%5Cpi_u(%5Comega_t)Q(%5Comega_t,%20%5Comega')%7D%5Cright%5C%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%5Cquad"> Choose <img src="https://latex.codecogs.com/png.latex?U_%7Bt%20+%201%7D%20%5Csim%20%5Ctext%7BUnif%7D(%5B0,%201%5D)"> independently and update</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Comega_%7Bt%20+%201%7D%20=%20%5Cbegin%7Bcases%7D%0A%5Comega'%20&amp;&amp;%20%5Ctext%7Bif%20%7D%20U_%7Bt+1%7D%20%5Cleq%20A%20%5C%5C%0A%5Comega_t%20&amp;&amp;%20%5Ctext%7Botherwise%7D%5C%5C%0A%5Cend%7Bcases%7D%0A"></p>
<p>Note: <img src="https://latex.codecogs.com/png.latex?T"> should be chosen large enough for the chain to reach its stationary distribution.</p>
</div>
<p>This algorithm is a more formal version of the intuition we’ve developed so far. It defines a new Markov Chain <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> (called the MH chain) and simulates this chain for a very long time.</p>
<p>But is this method actually correct? That is, will our sequence <img src="https://latex.codecogs.com/png.latex?%5Comega_0,%20%5Comega_1,%20%5Ccdots,%20%5Comega_t,%20%5Comega_%7Bt%20+%201%7D,%20%5Ccdots"> eventually look like it resembles <img src="https://latex.codecogs.com/png.latex?%5Cpi"> if we follow this algorithm? Well, we have a nice theorem (discussed in this <a href="https://csuraparaju.github.io/posts/markov-chains/">post</a>) that says irreducible and aperiodic Markov chains converge to their stationary distribution, no matter the starting distribution. So, if we can show that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a stationary distribution of <img src="https://latex.codecogs.com/png.latex?%5Comega_t">, and <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> is irreducible and aperiodic, then we can apply the theorem to conclude that <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bt%20%5Cto%20%5Cinfty%7D%20%5Cmathbb%7BP%7D%5B%5Comega_t%20=%20%5Comega%5D%20=%20%5Cpi(%5Comega)%0A"></p>
<p>The hard part in this is showing that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is indeed a stationary distribution of <img src="https://latex.codecogs.com/png.latex?%5Comega_t">. There are some useful propositions we should prove first, to help us establish this result.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 1
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is a distribution and <img src="https://latex.codecogs.com/png.latex?P"> is the transition matrix of a Markov Chain such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmu(x)%20P(x,%20y)%20=%20%5Cmu(y)%20P(y,%20x)%0A"> for every <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">, then <img src="https://latex.codecogs.com/png.latex?%5Cmu"> must be a stationary distribution of <img src="https://latex.codecogs.com/png.latex?P">.</p>
</div>
</div>
<p><em>Proof</em>:</p>
<p>Note that we have <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A(%5Cmu%20P)%20(y)%20&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%5Cmu(x)%20P(x,%20y)%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(y)%20P(y,%20x)%20%5Ctag%7BBy%20assumption%7D%5C%5C%0A&amp;=%20%5Cmu(y)%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20P(y,%20x)%20%5C%5C%0A&amp;=%20%5Cmu(y)%20%5Ctag%7BSince%20rows%20of%20$P$%20sum%20to%201%7D%0A%5Cend%7Balign*%7D"> Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is a stationary distribution.</p>
<p>Now, we can prove our correctness theorem:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>The MH Markov Chain <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> as defined above has <img src="https://latex.codecogs.com/png.latex?%5Cpi"> as a stationary distribution.</p>
</div>
</div>
<p><em>Proof</em>:</p>
<p>Note: Since ratios <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega')%20/%20%5Cpi(%5Comega)"> equal <img src="https://latex.codecogs.com/png.latex?%5Cpi_u(%5Comega')/%20%5Cpi_u(%5Comega)">, the acceptance probabilities computed with <img src="https://latex.codecogs.com/png.latex?%5Cpi_u"> in the algorithm is equivalent to those computed with <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Therefore, we can write the proof using <img src="https://latex.codecogs.com/png.latex?%5Cpi"> without loss of generality</p>
<p>First we need to identify the transition matrix of the Markov Chain <img src="https://latex.codecogs.com/png.latex?%5Comega_t">. Observe that for any two configurations <img src="https://latex.codecogs.com/png.latex?%5Comega"> and <img src="https://latex.codecogs.com/png.latex?%5Comega'">, if <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cneq%20%5Comega'">, then the probability that <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> transitions from <img src="https://latex.codecogs.com/png.latex?%5Comega"> to <img src="https://latex.codecogs.com/png.latex?%5Comega'"> is <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')A(%5Comega,%20%5Comega')"> since we propose <img src="https://latex.codecogs.com/png.latex?%5Comega'"> independently and accept <img src="https://latex.codecogs.com/png.latex?%5Comega'"> independently. Now, if <img src="https://latex.codecogs.com/png.latex?%5Comega%20=%20%5Comega'">, then the probability that <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> transitions from <img src="https://latex.codecogs.com/png.latex?%5Comega"> to <img src="https://latex.codecogs.com/png.latex?%5Comega'"> is same as the probability that we do not make a transition at all. This is <img src="https://latex.codecogs.com/png.latex?%0A1%20-%20%5Csum_%7B%5Comega''%20%5Cneq%20%5Comega%7D%20Q(%5Comega,%20%5Comega'')%20A(%5Comega,%20%5Comega'')%0A"> Therefore, the transition matrix of the MH chain is given by <img src="https://latex.codecogs.com/png.latex?%0AP(%5Comega,%20%5Comega')%20=%20%5Cbegin%7Bcases%7D%0AQ(%5Comega,%20%5Comega')%20A(%5Comega,%20%5Comega')%20&amp;&amp;%20%5Ctext%7Bif%20%7D%20%5Comega%20%5Cneq%20%5Comega'%20%5C%5C%0A1%20-%20%5Csum_%7B%5Comega''%20%5Cneq%20%5Comega%7D%20Q(%5Comega,%20%5Comega'')%20A(%5Comega,%20%5Comega'')%20&amp;&amp;%20%5Ctext%7Bif%20%7D%20%5Comega%20=%20%5Comega'%0A%5Cend%7Bcases%7D%0A"></p>
<p>Now, if <img src="https://latex.codecogs.com/png.latex?%5Comega%20=%20%5Comega'"> then we trivially have the condition <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cpi(%5Comega)%20P(%5Comega,%20%5Comega')%20&amp;=%20%5Cpi(%5Comega')%20P(%5Comega',%20%5Comega)%0A%5Cend%7Balign*%7D"></p>
<p>Therefore, by proposition <img src="https://latex.codecogs.com/png.latex?1">, we can conclude that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a stationary distribution of <img src="https://latex.codecogs.com/png.latex?P"> in this case. Now, assume that <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cneq%20%5Comega'">, and WLOG (by symmetry) that</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%20&amp;%5Cleq%20%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%20%5C%5C%0A1%20&amp;%5Cleq%20%5Cfrac%7B%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%7D%7B%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%7D%20%5Ctag%7BAll%20terms%20are%20non-negative%7D%0A%5Cend%7Balign*%7D"></p>
<p>This must imply that in this case, <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AA(%5Comega,%20%5Comega')%20&amp;=%20%5Cmin%5Cleft%5C%7B1,%20%5Cfrac%7B%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%7D%7B%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%7D%5Cright%5C%7D%20=%201%0A%5Cend%7Balign*%7D"></p>
<p>Additionally, we must have <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AA(%5Comega',%20%5Comega)%20&amp;=%20%5Cmin%5Cleft%5C%7B1,%20%5Cfrac%7B%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%7D%7B%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%7D%5Cright%5C%7D%20=%20%5Cfrac%7B%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%7D%7B%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%7D%0A%5Cend%7Balign*%7D%0A"> Now, we can put everything together and conclude that <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cpi(%5Comega)P(%5Comega,%5Comega')%20&amp;=%20%5Cpi(%5Comega)%5BQ(%5Comega,%5Comega')%5Cunderbrace%7BA(%5Comega,%5Comega')%7D_%7B=1%7D%20%5D%20%5C%5C%0A&amp;=%20%5Cpi(%5Comega)%20Q(%5Comega,%5Comega')%20%20%5C%5C%0A%5Cend%7Balign*%7D%0A"></p>
<p>But we can also write:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cpi(%5Comega')P(%5Comega',%5Comega)%20&amp;=%20%5Cpi(%5Comega')%20Q(%5Comega',%5Comega)A(%5Comega',%5Comega)%20%5C%5C%0A&amp;=%20%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%20%5Cleft%5B%5Cfrac%7B%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%7D%7B%5Cpi(%5Comega')%20Q(%5Comega',%20%5Comega)%7D%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cpi(%5Comega)%20Q(%5Comega,%20%5Comega')%0A%5Cend%7Balign*%7D%0A"> Therefore, we have that <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(%5Comega)P(%5Comega,%5Comega')%20=%20%5Cpi(%5Comega')P(%5Comega',%5Comega)%0A"></p>
<p>and so using proposition <img src="https://latex.codecogs.com/png.latex?1">, we can conclude that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is in-fact a stationary distribution of <img src="https://latex.codecogs.com/png.latex?P">. The other case is symmetric, and so this completes the proof.</p>
<p>Now, all that remains to prove correctness and gain apply the limiting theorem of Markov Chains and stationary distribution is to show that the MH chain is irreducible and aperiodic.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 2
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the proposal distribution <img src="https://latex.codecogs.com/png.latex?Q"> satisfies <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?%5Comega,%20%5Comega'%20%5Cin%20%5Cchi">, then the MH chain is irreducible and aperiodic.</p>
</div>
</div>
<p><em>Proof</em>:</p>
<p><strong>Irreducibility</strong>: We need to show that for any two configurations <img src="https://latex.codecogs.com/png.latex?%5Comega"> and <img src="https://latex.codecogs.com/png.latex?%5Comega'">, there exists some <img src="https://latex.codecogs.com/png.latex?t"> such that <img src="https://latex.codecogs.com/png.latex?P%5Et(%5Comega,%20%5Comega')%20%3E%200">, meaning we can reach <img src="https://latex.codecogs.com/png.latex?%5Comega'"> from <img src="https://latex.codecogs.com/png.latex?%5Comega"> in finitely many steps.</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')%20%3E%200"> by assumption, we can propose <img src="https://latex.codecogs.com/png.latex?%5Comega'"> from <img src="https://latex.codecogs.com/png.latex?%5Comega"> with positive probability. The acceptance probability satisfies <img src="https://latex.codecogs.com/png.latex?A(%5Comega,%20%5Comega')%20%3E%200"> whenever <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')%20%3E%200"> since we’ve shown in the previous post that <img src="https://latex.codecogs.com/png.latex?%5Cpi%20%3E%200"> and we assumed that <img src="https://latex.codecogs.com/png.latex?Q%20%3E%200">. Therefore, <img src="https://latex.codecogs.com/png.latex?%0AP(%5Comega,%20%5Comega')%20=%20Q(%5Comega,%20%5Comega')%20A(%5Comega,%20%5Comega')%20%3E%200%0A"> This means we can reach any state from any other state in one step, making the chain irreducible.</p>
<p><strong>Aperiodicity</strong>: Recall that a chain is aperiodic if for some state <img src="https://latex.codecogs.com/png.latex?%5Comega">, we have <img src="https://latex.codecogs.com/png.latex?%5Cgcd%5C%7Bt%20:%20P%5Et(%5Comega,%20%5Comega)%20%3E%200%5C%7D%20=%201">.</p>
<p>Notice that there is always a positive probability of rejecting the proposed state and staying at <img src="https://latex.codecogs.com/png.latex?%5Comega">. Specifically, the probability of staying at <img src="https://latex.codecogs.com/png.latex?%5Comega"> is at least the probability of proposing some <img src="https://latex.codecogs.com/png.latex?%5Comega'"> and then rejecting it. Since there always exists some <img src="https://latex.codecogs.com/png.latex?%5Comega'"> such that <img src="https://latex.codecogs.com/png.latex?A(%5Comega,%20%5Comega')%20%3C%201">, we have <img src="https://latex.codecogs.com/png.latex?%0AP(%5Comega,%20%5Comega)%20%5Cgeq%20Q(%5Comega,%20%5Comega')%20(1%20-%20A(%5Comega,%20%5Comega'))%20%3E%200%0A"> for some <img src="https://latex.codecogs.com/png.latex?%5Comega'">. This means we can return to <img src="https://latex.codecogs.com/png.latex?%5Comega"> in one step with positive probability, which immediately implies the chain is aperiodic (since <img src="https://latex.codecogs.com/png.latex?%5Cgcd"> of any set containing <img src="https://latex.codecogs.com/png.latex?1"> is <img src="https://latex.codecogs.com/png.latex?1">).</p>
<p>Combining our results, we get a powerful convergence theorem:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Convergence Theorem
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?%5Comega,%20%5Comega'">, then the MH chain is irreducible and aperiodic with unique stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Therefore, for any starting distribution <img src="https://latex.codecogs.com/png.latex?%5Comega_0">, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bt%20%5Cto%20%5Cinfty%7D%20%5Cmathbb%7BP%7D%5B%5Comega_t%20=%20%5Comega%5D%20=%20%5Cpi(%5Comega)%0A"> In other words, the distribution of <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> converges to <img src="https://latex.codecogs.com/png.latex?%5Cpi"> regardless of where we start!</p>
</div>
</div>
<p>This is the key result that makes the Metropolis-Hastings algorithm work: if we run the chain long enough, the samples we generate will be approximately distributed according to <img src="https://latex.codecogs.com/png.latex?%5Cpi">, even though we never computed the normalizing constant <img src="https://latex.codecogs.com/png.latex?Z">.</p>
</section>
<section id="back-to-protein-folding" class="level2">
<h2 class="anchored" data-anchor-id="back-to-protein-folding">Back to Protein Folding</h2>
<p>Now that we’ve established the theoretical guarantees of the Metropolis-Hastings algorithm, let’s return to the HP model for protein folding. We’ve shown that MH can sample from <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega)"> without computing the normalizing constant <img src="https://latex.codecogs.com/png.latex?Z">, but we still need to solve an important problem: what should our proposal chain <img src="https://latex.codecogs.com/png.latex?Q"> be?</p>
<p>Recall that we need <img src="https://latex.codecogs.com/png.latex?Q(%5Comega,%20%5Comega')%20%3E%200"> for all configurations <img src="https://latex.codecogs.com/png.latex?%5Comega,%20%5Comega'"> to guarantee convergence. In theory, convergence happens eventually, however in practice convergence may happen very slowly, and we might have to wait for a very long time before the distribution of <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> is close enough to <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Therefore, we need to choose the proposal mechanism <img src="https://latex.codecogs.com/png.latex?Q"> in a way that improves the rate of convergence to the stationary distribution. However, coming up with these mechanisms is usually problem specific, and estimating the rate of convergence for a given proposal mechanism is not easy. In general, a good proposal distribution should satisfy the following requirements:</p>
<ol type="1">
<li>We want to propose moves that are likely to be accepted, so we don’t waste time rejecting proposals</li>
<li>We want to explore the configuration space efficiently, not just make tiny local changes</li>
<li>We need to be able to reach any configuration from any other configuration through a finite sequence of moves</li>
</ol>
<p>For the HP model, we will use a proposal mechanism initially developed by Madras and Sokal called pivot moves when working on problems in statistical physics. Refer to this <a href="https://link.springer.com/article/10.1007/BF01022990">paper</a> for more details. What is important for us to know is that the pivot move set is complete (so any configuration can be transformed into any other configuration) and very fast at changing the global shape of the protein. The con is that for a very dense protein, the acceptance rate is low because rotating a large tail usually causes a collision.</p>
<section id="pivot-moves" class="level3">
<h3 class="anchored" data-anchor-id="pivot-moves">Pivot Moves</h3>
<p>The intuition behind a pivot move is that we treat a portion of the protein as fixed and swing the other end around a pivot.</p>
<p>Imagine the protein chain laid out on the lattice. You pick a random residue <img src="https://latex.codecogs.com/png.latex?i"> to be your pivot point. You keep the segment from <img src="https://latex.codecogs.com/png.latex?1"> to <img src="https://latex.codecogs.com/png.latex?i"> fixed in place. Then, you take the residues from <img src="https://latex.codecogs.com/png.latex?i+1"> to <img src="https://latex.codecogs.com/png.latex?N"> and apply a symmetrical transformation of the square lattice such as a <img src="https://latex.codecogs.com/png.latex?90%5E%5Ccirc"> rotation or a reflection using the pivot point as the origin.</p>
<p>More formally, a pivot move <img src="https://latex.codecogs.com/png.latex?%5Comega%20%5Cto%20%5Comega'"> can be described as:</p>
<ol type="1">
<li>Choose <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5C%7B1,%20%5Cdots,%20N%5C%7D"> uniformly at random.</li>
<li>Choose an element <img src="https://latex.codecogs.com/png.latex?g"> from the symmetry group of the square lattice, which is the Dihedral group <img src="https://latex.codecogs.com/png.latex?D_4">. These symmetries include rotations by <img src="https://latex.codecogs.com/png.latex?90%5E%5Ccirc,%20180%5E%5Ccirc,%20270%5E%5Ccirc"> and four axes of reflection.</li>
<li>For all <img src="https://latex.codecogs.com/png.latex?k%20%3E%20i">, the new position <img src="https://latex.codecogs.com/png.latex?%5Comega'(k)"> is updated as <img src="https://latex.codecogs.com/png.latex?%5Comega'(k)%20=%20g(%5Comega(k)%20-%20%5Comega(i))%20+%20%5Comega(i)"></li>
</ol>
<p>Below is a visualization of a pivot move. It depicts a pivot move for the amino acid at index <img src="https://latex.codecogs.com/png.latex?i%20=%203"> with a 90 degree clockwise rotation. Notice how only residues <img src="https://latex.codecogs.com/png.latex?3"> and <img src="https://latex.codecogs.com/png.latex?4"> changed positions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="pivot_moves.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://csuraparaju.github.io/posts/metropolis-hastings/pivot_moves.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</section>
<section id="metropolis-hastings-for-hp-model" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-hastings-for-hp-model">Metropolis-Hastings for HP Model</h3>
<p>With our proposal distribution defined, the Metropolis-Hastings algorithm for the HP model becomes:</p>
<ol type="1">
<li>Start with an initial valid configuration <img src="https://latex.codecogs.com/png.latex?%5Comega_0"> (e.g., a straight chain)</li>
<li>At each iteration <img src="https://latex.codecogs.com/png.latex?t">:
<ul>
<li>Propose a new configuration <img src="https://latex.codecogs.com/png.latex?%5Comega'"> by performing a pivot move on <img src="https://latex.codecogs.com/png.latex?%5Comega_t"></li>
<li>Compute the energy difference <img src="https://latex.codecogs.com/png.latex?%5CDelta%20E%20=%20E(s,%20%5Comega')%20-%20E(s,%20%5Comega_t)"></li>
<li>Calculate the acceptance probability: <img src="https://latex.codecogs.com/png.latex?A(%5Comega_t,%20%5Comega')%20=%20%5Cmin%5Cleft%5C%7B1,%20%5Cfrac%7Be%5E%7B-E(s,%20%5Comega')/k_B%20T%7D%20%5Ccdot%20Q(%5Comega',%20%5Comega_t)%7D%7Be%5E%7B-E(s,%20%5Comega_t)/k_B%20T%7D%20%5Ccdot%20Q(%5Comega_t,%20%5Comega')%7D%20%5Cright%5C%7D%20=%20%5Cmin%5C%7B1,%20e%5E%7B-%5CDelta%20E%20/%20k_B%20T%7D%20%5Ccdot%20R%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?R%20=%20Q(%5Comega',%20%5Comega_t)%20/%20Q(%5Comega_t,%20%5Comega')"></li>
<li>Accept or reject based on <img src="https://latex.codecogs.com/png.latex?A(%5Comega_t,%20%5Comega')"></li>
</ul></li>
<li>Run for <img src="https://latex.codecogs.com/png.latex?T"> iterations until convergence</li>
</ol>
<p>Notice how the energy calculation simplifies. We only need to count the <img src="https://latex.codecogs.com/png.latex?HH"> contacts in <img src="https://latex.codecogs.com/png.latex?%5Comega'"> and <img src="https://latex.codecogs.com/png.latex?%5Comega_t">, which is much faster than computing the full partition function <img src="https://latex.codecogs.com/png.latex?Z">. We can write an implementation in python for running this algorithm and visualize how our <img src="https://latex.codecogs.com/png.latex?%5Comega_t"> changes over time for a specific amino acid string. Full disclosure: I’ve generated the following animation by giving Anthropic’s Claude model this algorithm and asking it to write a simulation for the sequence <img src="https://latex.codecogs.com/png.latex?s%20=%20HPPHPPHHPPHHPPHH">. Click on the play button below to see how the configurations evolve over time.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="800" height="600" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title="My Animation"><source src="protein_folding.webm"></video></div>
<p>This should line up with our theoretical result. Recall that our probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Comega)%20%5Cpropto%20e%5E%7B-E(%5Comega)%7D">. This function looks something like</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="e_minus_x.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="https://csuraparaju.github.io/posts/metropolis-hastings/e_minus_x.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
<p>up to different scales and translations. Therefore, the configurations that are most likely under <img src="https://latex.codecogs.com/png.latex?%5Cpi"> are ones that minimize the energy function <img src="https://latex.codecogs.com/png.latex?E">. Since we defined <img src="https://latex.codecogs.com/png.latex?E"> to only count <img src="https://latex.codecogs.com/png.latex?HH"> contacts, the configuration that minimizes energy are the ones with the most <img src="https://latex.codecogs.com/png.latex?HH"> contacts. This is exactly what we see in the video above! As <img src="https://latex.codecogs.com/png.latex?t"> grows, we see that we sample configurations which have more and more <img src="https://latex.codecogs.com/png.latex?HH"> contacts. Remember the diagram at the beginning showing how hydrophobic residues cluster together on the inside while polar residues remain on the outside? Now we know exactly why this happens.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The main benefit of this approach is its generality. Whether we’re studying protein folding, sampling from complex Bayesian posteriors, or exploring other high-dimensional probability distributions, the same algorithm can be used. The theoretical guarantees from Markov Chains tells us that given enough time, our samples will be representatives of the true distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
<p>Of course, there are practical challenges we didn’t fully address. How many iterations <img src="https://latex.codecogs.com/png.latex?T"> do we need before convergence? How do we detect convergence? Can we do better than pivot moves for protein folding? These are active areas of research in computational statistics and biophysics. Variants like Hamiltonian Monte Carlo, Gibbs sampling, and replica exchange methods build on the MH framework to improve convergence rates for specific problem classes. Check out those approaches if this was interesting to you!</p>


</section>

 ]]></description>
  <category>math</category>
  <category>cs</category>
  <guid>https://csuraparaju.github.io/posts/metropolis-hastings/</guid>
  <pubDate>Wed, 15 Oct 2025 04:00:00 GMT</pubDate>
</item>
<item>
  <title>PageRank and Markov Chains</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/markov-chains/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Markov Chains are fundamental mathematical models for sequence of random events, where the probability of the next event depends only on the current event, not the entire past history. These models revolutionized computer science, economics, and even bioinformatics. In this post, I will cover much of the theory required to study these models. I will limit my discussion to mostly finite state space markov chains, but the proofs can be extended for the infinite state space as well.</p>
</section>
<section id="motivating-example" class="level2">
<h2 class="anchored" data-anchor-id="motivating-example">Motivating Example</h2>
<p>Imagine you’re Larry Page in 1996 researching a topic, say “jaguars.” There is a new database of information on the rise - the world wide web - so you decided to start there. You search through hundreds of thousands of web pages and find thousands that mention “jaguar”. However, you find pages about the animal, the car, the football team, a rock band, and random pages that just happen to use the word. You can’t manually read through all these pages to find the best ones.</p>
<p>Among all these pages that match your keywords, you want to automatically rank them in order of relative importance so you can start your search. Let’s model this formally: Suppose you have <img src="https://latex.codecogs.com/png.latex?N"> webpages, interconnected via hyperlinks. A subset of these pages are relevant to your query. How do you rank this subset by importance?</p>
<p>An obvious thing to do would be to randomly pick, say the first 5 pages, from the subset and read them. The issue with this approach is that you could get really unlucky and pick 5 pages which are totally unrelated to your topic, like obscure pages that barely mention jaguars. But maybe using randomness wasn’t the problem here. Maybe the problem was that you were picking uniformly at random, treating all pages as equally likely to be useful. What if instead, you used randomness in a smarter way?</p>
<p>Imagine you’re browsing the web, starting from one of the pages that mentions “jaguar.” You read the page, then randomly click on one of its outbound links, moving to a new page. You repeat this process, following the web’s hyperlinks. But you don’t browse forever in a single path. Occasionally, say with probability <img src="https://latex.codecogs.com/png.latex?%5Calpha"> you get bored, close your browser, and jump to a completely random page in the subset to start exploring again. If you simulate this process for a long time, you will find that the surfer visits certain pages much more frequently than others. For example, you will find that pages with many incoming links (or links from other “popular” pages) will be visited more often.</p>
<p>The “importance” of a page can be defined as the long-run proportion of time this random surfer spends on that page. This simple heuristic is the core logic behind PageRank, the algorithm that powered Google’s initial success. We will see that the random surfer model is also a Markov model, and that the long-run proportion of time spent by the random surfer corresponds to the stationary distribution.</p>
<p>To get more intuition for how this works, I’ve written a simulation below. Each node in the graph represents a webpage (Notated by “Pg n” where n is the page number). A directed edge <img src="https://latex.codecogs.com/png.latex?i%20%5Cto%20j"> represents a hyperlink in webpage <img src="https://latex.codecogs.com/png.latex?i"> pointing to webpage <img src="https://latex.codecogs.com/png.latex?j">. Initially, the surfer starts out on a random page. As the simulation progresses, we track the number of times the surfer visited each page, and redraw its size as a function of the relative visits. In the end, the bigger a node is, the more it was visited by the surfer (and therefore, the more “important” it is).</p>
<div class="gradio-iframe-container">
  <iframe src="https://csurapar-page-rank-vis.hf.space?__theme=light" frameborder="0"></iframe>
</div>
<p>A question you might now ask is, will this random surfer always produce the same long term behavior? That is, is the heuristic we use to define the “importance” of a page going to change between each run of the random surfer? If it does, then this algorithm would not be any good, since we were looking for a definitive ranking of the importance of webpages.</p>
<p>To answer this question, we need to study Markov chains and the stationary distribution more rigorously.</p>
</section>
<section id="markov-chains" class="level2">
<h2 class="anchored" data-anchor-id="markov-chains">Markov Chains</h2>
<p>Let’s start with the definition of a Markov Chain.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition 1
</div>
</div>
<div class="callout-body-container callout-body">
<p>A Markov chain on a state space <img src="https://latex.codecogs.com/png.latex?%5Cchi"> is a family of random variables <img src="https://latex.codecogs.com/png.latex?X_0,%20X_1,%20%5Ccdots"> such that for all <img src="https://latex.codecogs.com/png.latex?n%20%5Cin%20%5Cmathbb%7BN%7D">, and <img src="https://latex.codecogs.com/png.latex?x_0,%20%5Ccdots,%20x_n,%20x_%7Bn+1%7D%20%5Cin%20%5Cchi"> we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BX_%7Bn+1%7D%20=%20x_%7Bn+1%7D%20%5Cmid%20%5C%7BX_k%20=%20x_k%20%5Cmid%200%20%5Cleq%20k%20%5Cleq%20n%5C%7D%5D%20=%20%5Cmathbb%7BP%7D%5BX_%7Bn+1%7D%20=%20x_%7Bn+1%7D%20%5Cmid%20X_n%20=%20x_n%5D%0A"></p>
</div>
</div>
<p>The above property is also known as the Markov Property. A time homogeneous Markov chain is a Markov chain where <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BX_%7Bn+1%7D%20=%20y%20%5Cmid%20X_n%20=%20x%5D%20=%20%5Cmathbb%7BP%7D%5BX_%7B1%7D%20=%20y%20%5Cmid%20X_0%20=%20x%5D%0A"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi"> and <img src="https://latex.codecogs.com/png.latex?n%20%5Cin%20%5Cmathbb%7BN%7D">. In the remaining of the discussion we will only consider time homogeneous Markov chains because they can be represented as a single transition matrix, not dependent on time <img src="https://latex.codecogs.com/png.latex?%0AP(x,%20y)%20:=%20%5Cmathbb%7BP%7D%5BX_1%20=%20y%20%5Cmid%20X_0%20=%20x%5D%0A"> where <img src="https://latex.codecogs.com/png.latex?x,%20y"> are the indices into the matrix <img src="https://latex.codecogs.com/png.latex?P">. This seems a bit silly, but reframing the problem this way allows us to use powerful tools/techniques from linear algebra. Using purely probabilistic methods when working with Markov chains is often very ugly, and the matrix formulation is preferred.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">None</span>Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s try to come up with a transition matrix for the random web surfer Markov Chain we discussed above. Suppose that our state space is the set of all <img src="https://latex.codecogs.com/png.latex?N"> webpages that are relevant to our query. So, <img src="https://latex.codecogs.com/png.latex?%5Cchi%20=%20%5C%7B1,%202,%20%5Ccdots,%20N%20%5C%7D">. For each page <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5Cchi">, let <img src="https://latex.codecogs.com/png.latex?d_i"> denote the number of outbound links from that page. We allow our surfer to follow a random outbound link from the current page with probability <img src="https://latex.codecogs.com/png.latex?(1%20-%20%5Calpha)">. We also want the surfer to pick a uniformly random page from all <img src="https://latex.codecogs.com/png.latex?N"> pages and go there with probability <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</p>
<p>Therefore, the transition probability from page <img src="https://latex.codecogs.com/png.latex?i"> to page <img src="https://latex.codecogs.com/png.latex?j"> is: <img src="https://latex.codecogs.com/png.latex?%0AP(i,%20j)%20=%20%5Cbegin%7Bcases%7D%20(1-%20%5Calpha)%20%5Ccdot%20%5Cfrac%7B1%7D%7Bd_i%7D%20+%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20&amp;&amp;%20%5Ctext%7B%20if%20there%20exists%20a%20link%20from%20%7D%20i%20%5Cto%20j%20%5C%5C%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20&amp;&amp;%20%5Ctext%7B%20otherwise%7D%20%5Cend%7Bcases%7D%0A"></p>
</div>
</div>
<p>Let’s prove some basic propositions about this new object we defined.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 1
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any <img src="https://latex.codecogs.com/png.latex?x_0,%20%5Ccdots,%20x_n%20%5Cin%20%5Cchi">, <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BX_n%20=%20y%20%5Cmid%20X_0%20=%20x%5D%20=%20P%5En%20(x,%20y)%0A"> where <img src="https://latex.codecogs.com/png.latex?P%5En"> means the nth power of the matrix <img src="https://latex.codecogs.com/png.latex?P">.</p>
</div>
</div>
<p>Proof: We proceed by induction on <img src="https://latex.codecogs.com/png.latex?n">. The base cases <img src="https://latex.codecogs.com/png.latex?n=1"> follows by definition. So assume that the result holds for <img src="https://latex.codecogs.com/png.latex?n">. We need to show it holds for <img src="https://latex.codecogs.com/png.latex?n+1">. We first condition on <img src="https://latex.codecogs.com/png.latex?X_n">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bflalign%7D%0A%5Cmathbb%7BP%7D%5BX_%7Bn%20+%201%7D%20=%20y%20%5Cmid%20X_0%20=%20x%5D%20&amp;=%20%5Csum_%7Bz%20%5Cin%20%5Cchi%7D%20%5Cmathbb%7BP%7D%5BX_%7Bn+1%7D%20=%20y%20%5Cmid%20X_n%20=%20z,%20X_0%20=%20x%5D%20%5Ccdot%20%5Cmathbb%7BP%7D%5BX_n%20=%20z%20%5Cmid%20X_0%20=%20x%5D%20&amp;&amp;%5C%5C%0A&amp;=%20%5Csum_%7Bz%20%5Cin%20%5Cchi%7D%20%5Cmathbb%7BP%7D%5BX_%7Bn+1%7D%20=%20y%20%5Cmid%20X_n%20=%20z%5D%20%5Ccdot%20%5Cmathbb%7BP%7D%5BX_n%20=%20z%20%5Cmid%20X_0%20=%20x%5D%20%5Ctag%7BMarkov%20Property%7D%20&amp;&amp;%5C%5C%0A&amp;=%20%5Csum_%7Bz%20%5Cin%20%5Cchi%7D%20P(z,%20y)%20%5Ccdot%20%5Cmathbb%7BP%7D%5BX_n%20=%20z%20%5Cmid%20X_0%20=%20x%5D%20%5Ctag%7BDefinition%20of%20$P$%7D%20&amp;&amp;%5C%5C%0A&amp;=%20%5Csum_%7Bz%20%5Cin%20%5Cchi%7D%20P(z,%20y)%20%5Ccdot%20P%5En%20(x,%20z)%20%5Ctag%7BInduction%20hypothesis%7D%20&amp;&amp;%5C%5C%0A&amp;=%20P%5E%7Bn+1%7D%20(x,%20y)%20%5Ctag%7BMatrix%20mult.%7D%0A%5Cend%7Bflalign%7D%0A"> as desired.</p>
<p>Now, we can characterize how a Markov chain <img src="https://latex.codecogs.com/png.latex?X_n"> with transition matrix <img src="https://latex.codecogs.com/png.latex?P"> and initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> over time.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 2
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?X_0%20%5Csim%20%5Cmu_0"> (i.e., <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5BX_0%20=%20x%5D%20=%20%5Cmu_0%20(x)"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">) then <img src="https://latex.codecogs.com/png.latex?X_n%20%5Csim%20%5Cmu_0%20P%5En"> (taken as a matrix product)</p>
</div>
</div>
<p>Proof: We need to show that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5BX_n%20=%20y%5D%20=%20(%5Cmu_0%20P%5En)(y)"> for all <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cchi">.</p>
<p>We condition on the initial state <img src="https://latex.codecogs.com/png.latex?X_0">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BX_n%20=%20y%5D%20&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmathbb%7BP%7D%5BX_n%20=%20y%20%5Cmid%20X_0%20=%20x%5D%20%5Ccdot%20%5Cmathbb%7BP%7D%5BX_0%20=%20x%5D%20%5Ctag%7BLaw%20of%20total%20probability%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20P%5En(x,%20y)%20%5Ccdot%20%5Cmathbb%7BP%7D%5BX_0%20=%20x%5D%20%5Ctag%7BProposition%201%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20P%5En(x,%20y)%20%5Ccdot%20%5Cmu_0(x)%20%5Ctag%7BDefinition%20of%20$%5Cmu_0$%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu_0(x)%20%5Ccdot%20P%5En(x,%20y)%20%20%5C%5C%0A&amp;=%20(%5Cmu_0%20P%5En)(y)%20%5Ctag%7BMatrix%20mult.%7D%0A%5Cend%7Balign*%7D"></p>
<p>as desired.</p>
</section>
<section id="the-stationary-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-stationary-distribution">The Stationary Distribution</h2>
<p>Let’s return to our PageRank example. Notice that the surfer never stops exploring the web. They keep browsing indefinitely, either following links or opening up new pages in a browser. This raises a natural question: What happens to the distribution of the surfer’s location after a very long time?</p>
<p>From Proposition 2, we can come up with the recurrence relation: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmu_%7Bn%20+%201%7D%20=%20%5Cmu_0%20P%5E%7Bn%20+%201%7D%20=%20(%5Cmu_0%20P%5En)%20P%20=%20%5Cmu_n%20P%0A"> As <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">, does this converge to anything (i.e., does there exists a limiting distribution)? And if so, does the limiting distribution depend on where we started <img src="https://latex.codecogs.com/png.latex?%5Cmu_0">?</p>
<p>Often, a good way to answer questions like these is to assume something about the statement and see what happens. So, lets assume that there exists a limiting distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmu_n%20%5Cto%20%5Cpi"> as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">. This leads us to proposition 3:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 3
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a limiting distribution of the Markov chian <img src="https://latex.codecogs.com/png.latex?P">, then <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cpi%20P"></p>
</div>
</div>
<p><em>Proof</em>: We take limits on both sides of our recurrence relation: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cmu_%7Bn%20+%201%7D%20&amp;=%20%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D(%5Cmu_n%20P)%20%5C%5C%0A%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cmu_%7Bn%20+%201%7D%20&amp;=%20(%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cmu_n)%20P%20%20%20%5Ctag%7BMatrix%20mul%20is%20continuous%7D%5C%5C%0A%5Cpi%20&amp;=%20%5Cpi%20P%20%20%20%5Ctag%7BSince%20$%5Cmu_n%20%5Cto%20%5Cpi$%7D%5C%5C%0A%5Cend%7Balign*%7D"></p>
<p>That’s a pretty interesting result! Any limiting distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> of a Markov chain (if it exists) is such that <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cpi%20P">. This special kind of distribution is known as <strong>stationary distribution</strong>. Let’s see if we can use this property to help us answer our original question. Specifically, is the converse of proposition 3 true? Is it the case that any distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> such that <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cpi%20P"> must be a limiting distribution? If it was, then we’d be done! We would have found a way to completely characterize what the limiting distribution of a Markov Chain looks like.</p>
<p>But sadly, that is not the case.</p>
<section id="a-counter-example" class="level3">
<h3 class="anchored" data-anchor-id="a-counter-example">A Counter Example</h3>
<p>Consider a Markov chain on <img src="https://latex.codecogs.com/png.latex?%5Cchi%20=%20%5C%7B0,%201,%202,%203%5C%7D"> with transition matrix <img src="https://latex.codecogs.com/png.latex?%0AP%20=%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%200%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%201%5C%5C%0A%5Cend%7Bpmatrix%7D%0A"></p>
<p>Notice that the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_1%20=%20%5Cbegin%7Bpmatrix%7D%20%5Csqrt%7B2%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Csqrt%7B2%7D%20%5Cend%7Bpmatrix%7D"> (read as a <img src="https://latex.codecogs.com/png.latex?1%20%5Ctimes%204"> row matrix) of <img src="https://latex.codecogs.com/png.latex?P"> has the property discussed in proposition 3 because:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cpi_1%20P%20&amp;=%20%5Cbegin%7Bpmatrix%7D%20%5Csqrt%7B2%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Csqrt%7B2%7D%20%5Cend%7Bpmatrix%7D%20%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%200%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%201%5C%5C%0A%5Cend%7Bpmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bpmatrix%7D%20%5Csqrt%7B2%7D%20&amp;%200%20&amp;%200%20&amp;%20%5Csqrt%7B2%7D%20%5Cend%7Bpmatrix%7D%20%5C%5C%0A&amp;=%20%5Cpi_1%0A%5Cend%7Balign*%7D%0A"></p>
<p>Additionally, the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi_2%20=%20%5Cbegin%7Bpmatrix%7D%20e%20&amp;%200%20&amp;%200%20&amp;%20e%20%5Cend%7Bpmatrix%7D"> of <img src="https://latex.codecogs.com/png.latex?P"> also has the property discussed in proposition 3 because:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cpi_2%20P%20&amp;=%20%5Cbegin%7Bpmatrix%7D%20e%20&amp;%200%20&amp;%200%20&amp;%20e%5Cend%7Bpmatrix%7D%20%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%200%20&amp;%200%20&amp;%200%20%5C%5C%0A%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%200%20%5C%5C%0A0%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20&amp;%20%5Cfrac%7B1%7D%7B3%7D%20%5C%5C%0A0%20&amp;%200%20&amp;%200%20&amp;%201%5C%5C%0A%5Cend%7Bpmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bpmatrix%7D%20e%20&amp;%200%20&amp;%200%20&amp;%20e%20%5Cend%7Bpmatrix%7D%20%5C%5C%0A&amp;=%20%5Cpi_2%0A%5Cend%7Balign*%7D%0A"></p>
<p>This is a problem because if the converse of proposition 3 were true, then any distribution that is stationary would be the limiting distribution of <img src="https://latex.codecogs.com/png.latex?P">. But we’ve just shown that both <img src="https://latex.codecogs.com/png.latex?%5Cpi_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi_2"> are stationary distributions, and they are different! A limiting distribution must be unique (a Markov chain can’t converge to two different distributions at the same time), so this shows that not every stationary distribution is a limiting distribution.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">None</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>You may be wondering how I came up with the two different stationary distribution for <img src="https://latex.codecogs.com/png.latex?P">. Notice that we can treat <img src="https://latex.codecogs.com/png.latex?%5Cpi"> as a row-vector. In linear algebra terms, the stationary property means that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a left-eigenvector of <img src="https://latex.codecogs.com/png.latex?P"> with eigenvalue <img src="https://latex.codecogs.com/png.latex?1">. So, one way to find a stationary distribution is to compute a left-eigen vector with eigen-value <img src="https://latex.codecogs.com/png.latex?1"> of <img src="https://latex.codecogs.com/png.latex?P">. Of course, not all left-eigenvectors of <img src="https://latex.codecogs.com/png.latex?P"> will be a valid probability distribution (some may have negative entries, or worse, complex entires). So you need to be careful when computing eigenvectors and treating them as probability distributions.</p>
</div>
</div>
</section>
</section>
<section id="irreducibility" class="level2">
<h2 class="anchored" data-anchor-id="irreducibility">Irreducibility</h2>
<p>We’ve seen that a stationary distribution need not be unique for a Markov Chain, which is a problem because if a stationary distribution could ever hope be a limiting distribution, then it has to be the <em>unique</em> stationary distribution. Let’s study the conditions under which a stationary distribution is <em>the</em> stationary distribution of a Markov Chain.</p>
<p>Going from the example above, you might notice that the issue there was that there were states which were “absorbing”, meaning once our chain entered that state, it remained there forever. For example, state <img src="https://latex.codecogs.com/png.latex?0"> is absorbing because <img src="https://latex.codecogs.com/png.latex?P(0,%201)%20=%20P(0,%202)%20=%20P(0,%203)%20%20=%200"> and so the chain would never transition out of that state. Similarly, state <img src="https://latex.codecogs.com/png.latex?3"> is also an absorbing state. If we eliminate this behavior, then we might have a unique stationary distribution?</p>
<p>This observation immediately leads us to the following definition of irreducibility</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition 3
</div>
</div>
<div class="callout-body-container callout-body">
<p>A Markov chain is irreducible if every state is eventually reachable from every other state. That is, for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">, there exists some <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%200"> such that <img src="https://latex.codecogs.com/png.latex?P%5En(x,%20y)%20%3E%200">.</p>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">None</span>Example: PageRank is Irreducible
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall our PageRank transition matrix: <img src="https://latex.codecogs.com/png.latex?%0AP(i,%20j)%20=%20%5Cbegin%7Bcases%7D%20(1-%20%5Calpha)%20%5Ccdot%20%5Cfrac%7B1%7D%7Bd_i%7D%20+%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20&amp;&amp;%20%5Ctext%7B%20if%20there%20exists%20a%20link%20from%20%7D%20i%20%5Cto%20j%20%5C%5C%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20&amp;&amp;%20%5Ctext%7B%20otherwise%7D%20%5Cend%7Bcases%7D%0A"></p>
<p>This chain is irreducible. From any page <img src="https://latex.codecogs.com/png.latex?i">, there’s at least a probability <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Calpha%7D%7BN%7D"> of jumping to any other page <img src="https://latex.codecogs.com/png.latex?j"> in one step (via the random jump mechanism). Therefore <img src="https://latex.codecogs.com/png.latex?P(i,%20j)%20%5Cgeq%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i,%20j">, which immediately implies every state is accessible from every other state.</p>
</div>
</div>
<p>Now, let’s try to link irreducibility with stationary distributions. Specifically, lets see if irreducibility gives us any information about what a stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> may look like. Because we are assuming that every state is reachable from every other state, one could guess that <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, since otherwise, there would be a state <img src="https://latex.codecogs.com/png.latex?x"> for which <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5BX_n%20=%20x%5D%20=%200"> and make this state unreachable.</p>
<p>This leads us to the following proposition:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 4
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a stationary distribution and <img src="https://latex.codecogs.com/png.latex?P"> is the transition matrix of an irreducible Markov Chain <img src="https://latex.codecogs.com/png.latex?P">, then <img src="https://latex.codecogs.com/png.latex?%5Cpi%20(x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">.</p>
</div>
</div>
<p><em>Proof</em>: Assume for contradiction that there exists an <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi"> such that <img src="https://latex.codecogs.com/png.latex?%5Cpi%20(x)%20=%200">. Because <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is stationary, this means we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A0%20=%20%5Cpi%20(x)%20&amp;=%20(%5Cpi%20P)%20(x)%20%5C%5C%0A&amp;=%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Cpi%20(y)%20P(y,%20x)%20&amp;%20%5Ctag%7BMatrix%20mult.%7D%5C%5C%0A&amp;=%20%5Csum_%7B%5Csubstack%7By%20%5Cin%20%5Cchi%20%5C%5C%20y%20%5Cneq%20x%7D%7D%20%5Cpi%20(y)%20P(y,%20x)%20%5Ctag%7BSince%20$%5Cpi(x)$%20=%200%7D%20%5C%5C%0A%5Cend%7Balign*%7D%0A"> Notice that each term on the right hand side is non negative (since <img src="https://latex.codecogs.com/png.latex?%5Cpi(y)%20%5Cgeq%200"> and <img src="https://latex.codecogs.com/png.latex?P(y,%20x)%20%5Cgeq%200">). The only way a sum of non negative term can be zero is if each term is zero. This means we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi(y)%20P(y,%20x)%20=%200%0A"> for all <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cchi"> and so either <img src="https://latex.codecogs.com/png.latex?%5Cpi(y)%20=%200"> or <img src="https://latex.codecogs.com/png.latex?P(y,%20x)%20=%200">. Now, because we assumed <img src="https://latex.codecogs.com/png.latex?P"> was irreducible, there must exist a <img src="https://latex.codecogs.com/png.latex?y%20%5Cneq%20x"> in <img src="https://latex.codecogs.com/png.latex?%5Cchi"> such that <img src="https://latex.codecogs.com/png.latex?P(y,%20x)%20%3E%200"> (otherwise, <img src="https://latex.codecogs.com/png.latex?x"> would be unreachable from any other state, no matter how many steps the chain takes). Therefore, for this <img src="https://latex.codecogs.com/png.latex?y">, it must be the case that <img src="https://latex.codecogs.com/png.latex?%5Cpi(y)%20=%200">.</p>
<p>We can now apply the same argument to <img src="https://latex.codecogs.com/png.latex?y"> and get that there is a <img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cchi"> such that <img src="https://latex.codecogs.com/png.latex?P(z,%20y)%20%3E%200">, forcing <img src="https://latex.codecogs.com/png.latex?%5Cpi(z)%20=%200">. We can continue this argument inductively, and since every state is reachable from every other state (by irreducibility), we can show that <img src="https://latex.codecogs.com/png.latex?%5Cpi(w)%20=%200"> for all <img src="https://latex.codecogs.com/png.latex?w%20%5Cin%20%5Cchi">.</p>
<p>But this contradicts the fact that <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a probability distribution, which requires</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bw%20%5Cin%20%5Cchi%7D%20%5Cpi(w)%20=%201%0A"></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, as desired.</p>
<p>Using this proposition, we can now prove a big theorem which tells us when a Markov Chain has a unique stationary distribution.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Theorem 1 (Uniqueness)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?P"> is the transition matrix of an irreducible Markov Chain, then there exists a unique stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
</div>
</div>
<p><em>Proof:</em></p>
<p>I will skip the existence part of this theorem because it gets extremely technical and not suited for this post. If you are curious, you should check out <a href="https://math.uchicago.edu/~may/REU2017/REUPapers/Freedman.pdf">this</a> resource</p>
<p>Lets prove uniqueness. Suppose <img src="https://latex.codecogs.com/png.latex?%5Cpi_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi_2"> are two stationary distributions of <img src="https://latex.codecogs.com/png.latex?P">. Because our state space is finite, and <img src="https://latex.codecogs.com/png.latex?%5Cpi_1(x),%20%5Cpi_2(x)%20%3E%200"> by proposition 4, for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, we can choose</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_0%20:=%5Ctext%7Barg%7D%5Cmin%5Climits_%7Bx%20%5Cin%20%5Cchi%7D%5C,%5Cleft(%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20%5Cright)%0A"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?y"> be an arbitrary state in <img src="https://latex.codecogs.com/png.latex?%5Cchi">. By irreducibility, there exists an <img src="https://latex.codecogs.com/png.latex?n%20%5Cin%20N"> such that <img src="https://latex.codecogs.com/png.latex?P%5En%20(y,%20x_0)%20%3E%200">. This means that the Markov Chain can reach to <img src="https://latex.codecogs.com/png.latex?x_0"> from <img src="https://latex.codecogs.com/png.latex?y"> in <img src="https://latex.codecogs.com/png.latex?n"> steps via a finite step of transitions <img src="https://latex.codecogs.com/png.latex?y%20=%20x_n%20%5Cto%20x_%7Bn-1%7D%20%5Cto%20%5Cdots%20%5Cto%20x_1%20%5Cto%20x_0"> where <img src="https://latex.codecogs.com/png.latex?P(x_i,%20x_%7Bi-1%7D)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?i%20=%201,%202,%20%5Cldots,%20n">.</p>
<p>We will prove by induction on the path length that <img src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpi_1(y)%7D%7B%5Cpi_2(y)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_0)%7D%7B%5Cpi_2(x_0)%7D%20"> The base case is trivial since it follows by definition. So, assume <img src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_0)%7D%7B%5Cpi_2(x_0)%7D%20"> for some <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%3C%20n">.</p>
<p>Now, we can write:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bflalign%7D%0A%5Cpi_1(x_%7Bi%7D)%20=%20(%5Cpi_1%20P)%20(x_%7Bi%7D)%20&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cpi_1(x)%20P(x,%20x_%7Bi%7D)%20%5Ctag%7B$%5Cpi_1$%20is%20stationary%7D%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%20%5Ctag%7BMultiply%20RHS%20by%20$%5Cpi_2(x)%20/%20%5Cpi_2(x)$%7D%5C%5C%0A&amp;%5Cgeq%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%20%5Ctag%7B$x_%7Bi%7D$%20achieves%20minimum,%20I.H.%7D%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%20%5Ctag%7BRatio%20is%20constant%7D%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Cpi_2(x_%7Bi%7D)%20%5Ctag%7B$%5Cpi_2$%20is%20stationary%7D%5C%5C%0A&amp;=%20%5Cpi_1(x_%7Bi%7D)%0A%5Cend%7Bflalign%7D%0A"></p>
<p>Notice that because LHS = RHS, the inequality in between must be an equality. Therefore: <img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%20=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%0A"></p>
<p>Rearranging: <img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cleft%5B%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20-%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Cright%5D%20%5Cpi_2(x)%20P(x,%20x_%7Bi%7D)%20=%200%0A"></p>
<p>Now, note that <img src="https://latex.codecogs.com/png.latex?%5Cpi_2(x)%20%3E%200"> by proposition 4, and because <img src="https://latex.codecogs.com/png.latex?x_%7Bi%7D"> achieves the minimum, we have <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20-%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20%5Cgeq%200"> and <img src="https://latex.codecogs.com/png.latex?P(x,%20x_%7Bi%7D)%20%5Cgeq%200"> by definition. Therefore, the only way a sum of non-negative terms is zero is if each term is zero. Thus, for any state <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi"> with <img src="https://latex.codecogs.com/png.latex?P(x,%20x_%7Bi%7D)%20%3E%200">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20-%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20=%200%20%5Cimplies%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_%7Bi%7D)%7D%7B%5Cpi_2(x_%7Bi%7D)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_0)%7D%7B%5Cpi_2(x_0)%7D%0A"></p>
<p>In particular, since <img src="https://latex.codecogs.com/png.latex?P(x_%7Bi%20+%201%7D,%20x_%7Bi%7D)%20%3E%200"> by our path construction, we have: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpi_1(x_%7Bi+1%7D)%7D%7B%5Cpi_2(x_%7Bi+1%7D)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_0)%7D%7B%5Cpi_2(x_0)%7D%0A"></p>
<p>and by the principle of induction, we can conclude that this holds for all <img src="https://latex.codecogs.com/png.latex?1%20%5Cleq%20i%20%3C%20n">. Therefore, we can conclude that <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpi_1(y)%7D%7B%5Cpi_2(y)%7D%20=%20%5Cfrac%7B%5Cpi_1(x_%7Bn%7D)%7D%7B%5Cpi_2(x_%7Bn%7D)%7D%20=%5Cfrac%7B%5Cpi_1(x_0)%7D%7B%5Cpi_2(x_0)%7D%0A"> as desired.</p>
<p>Now, since <img src="https://latex.codecogs.com/png.latex?y"> was an arbitrary state, we now know that <img src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpi_1(x)%7D%7B%5Cpi_2(x)%7D%20=%20c%20"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, where <img src="https://latex.codecogs.com/png.latex?c"> is some positive constant. This means <img src="https://latex.codecogs.com/png.latex?%5Cpi_1(x)%20=%20c%20%5Ccdot%20%5Cpi_2(x)"> for all <img src="https://latex.codecogs.com/png.latex?x">. Now, since both <img src="https://latex.codecogs.com/png.latex?%5Cpi_1,%20%5Cpi_2"> are probability distributions, we have: <img src="https://latex.codecogs.com/png.latex?%0A1%20=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cpi_1(x)%20=%20%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20c%20%20%5Cpi_2(x)%20=%20c%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cpi_2(x)%20=%20c%0A"></p>
<p>Therefore <img src="https://latex.codecogs.com/png.latex?c%20=%201">, which implies <img src="https://latex.codecogs.com/png.latex?%5Cpi_1(x)%20=%20%5Cpi_2(x)"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, as desired.</p>
<section id="another-counter-example" class="level3">
<h3 class="anchored" data-anchor-id="another-counter-example">Another Counter Example</h3>
<p>That was a lot of hard work! But are we done? Surely now that we have a unique stationary distribution, we can claim that the limiting distribution converges to this stationary distribution right?</p>
<p>Well, not so fast. Let’s consider another example, where <img src="https://latex.codecogs.com/png.latex?%5Cchi%20=%20%5C%7B0,%201%5C%7D"> and the transition matrix is <img src="https://latex.codecogs.com/png.latex?%0AP%20=%20%5Cbegin%7Bpmatrix%7D%0A0%20&amp;%201%20%5C%5C%0A1%20&amp;%200%0A%5Cend%7Bpmatrix%7D%0A"> which is the chain that transitions from <img src="https://latex.codecogs.com/png.latex?0%20%5Cto%201%20%5Cto%200%20%5Cto%201%20%5Ccdots"> deterministically. Notice that the distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cbegin%7Bpmatrix%7D%20%5Cfrac%7B1%7D%7B2%7D%20&amp;%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bpmatrix%7D"> is a stationary distribution since <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cpi%20P%20&amp;=%20%5Cbegin%7Bpmatrix%7D%20%5Cfrac%7B1%7D%7B2%7D%20&amp;%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bpmatrix%7D%20%5Cbegin%7Bpmatrix%7D%0A0%20&amp;%201%20%5C%5C%0A1%20&amp;%200%0A%5Cend%7Bpmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bpmatrix%7D%20%5Cfrac%7B1%7D%7B2%7D%20&amp;%20%5Cfrac%7B1%7D%7B2%7D%5Cend%7Bpmatrix%7D%20=%20%5Cpi%0A%5Cend%7Balign*%7D"></p>
<p>Additionally, this chain is clearly irreducible, since every state is reachable from every other state. Therefore, <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the unique stationary distribution from theorem 1. But it is not a limiting distribution. To see why, first note that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AP%5E2%20&amp;=%20%5Cbegin%7Bpmatrix%7D%200%20&amp;%201%20%5C%5C%201%20&amp;%200%20%5Cend%7Bpmatrix%7D%20%5Cbegin%7Bpmatrix%7D%200%20&amp;%201%20%5C%5C%201%20&amp;%200%20%5Cend%7Bpmatrix%7D%20=%20%5Cbegin%7Bpmatrix%7D%201%20&amp;%200%20%5C%5C%200%20&amp;%201%20%5Cend%7Bpmatrix%7D%20=%20I%0A%5Cend%7Balign*%7D"></p>
<p>Therefore, for any <img src="https://latex.codecogs.com/png.latex?n%20%5Cin%20%5Cmathbb%7BN%7D">: <img src="https://latex.codecogs.com/png.latex?%0AP%5En%20=%20%5Cbegin%7Bcases%7D%0AP%5E%7B2k%7D%20=%20(P%5E2)%5Ek%20=%20I%20&amp;%20%5Ctext%7Bif%20%7D%20n%20%5Ctext%7B%20is%20even%7D%20%5C%5C%0AP%5E%7B2k%20+%201%7D%20=%20P%5E%7B2k%7D%20P%20=%20P%20&amp;%20%5Ctext%7Bif%20%7D%20n%20%5Ctext%7B%20is%20odd%7D%0A%5Cend%7Bcases%7D%0A"></p>
<p>Now consider an initial distribution (read as a 1x2 matrix) <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%20=%20%5Cbegin%7Bpmatrix%7D1%20&amp;%200%5Cend%7Bpmatrix%7D">. This means we can make the simplification: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmu_0%20P%5En%20&amp;=%20%5Cbegin%7Bpmatrix%7D1%20&amp;%200%5Cend%7Bpmatrix%7DP%5En%20%5C%5C%0A&amp;=%20%5Cbegin%7Bcases%7D%0A%5Cbegin%7Bpmatrix%7D1%20&amp;%200%5Cend%7Bpmatrix%7D%20&amp;%20%5Ctext%7Bif%20%7D%20n%20%5Ctext%7B%20is%20even%7D%20%5C%5C%0A%5Cbegin%7Bpmatrix%7D0%20&amp;%201%5Cend%7Bpmatrix%7D%20&amp;%20%5Ctext%7Bif%20%7D%20n%20%5Ctext%7B%20is%20odd%7D%0A%5Cend%7Bcases%7D%0A%5Cend%7Balign*%7D"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%20P%5En"> alternates between <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bpmatrix%7D1%20&amp;%200%5Cend%7Bpmatrix%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bpmatrix%7D0%20&amp;%201%5Cend%7Bpmatrix%7D">, the sequence does not converge as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">. Therefore, no limiting distribution exists.</p>
</section>
</section>
<section id="aperidocity" class="level2">
<h2 class="anchored" data-anchor-id="aperidocity">Aperidocity</h2>
<p>What went wrong here? The chain is irreducible and has a unique stationary distribution, but it still doesn’t converge to that distribution. The problem is that the chain exhibits periodic behavior. It oscillates between states <img src="https://latex.codecogs.com/png.latex?0"> and <img src="https://latex.codecogs.com/png.latex?1"> in a predictable cycle, never settling down into a stable long-run pattern.</p>
<p>This observation motivates our final piece of the puzzle: aperiodicity.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition 4
</div>
</div>
<div class="callout-body-container callout-body">
<p>The period of a state <img src="https://latex.codecogs.com/png.latex?x"> is the greatest common divisor (gcd) of all possible return times to that state. That is, <img src="https://latex.codecogs.com/png.latex?%0Ad(x)%20=%20%5Cgcd%5C%7Bn%20%5Cgeq%201%20:%20P%5En(x,%20x)%20%3E%200%5C%7D%0A"></p>
<p>A state <img src="https://latex.codecogs.com/png.latex?x"> is aperiodic if <img src="https://latex.codecogs.com/png.latex?d(x)%20=%201">. A Markov chain is aperiodic if all states are aperiodic.</p>
</div>
</div>
<p>Intuitively, we can think of the set <img src="https://latex.codecogs.com/png.latex?%5C%7Bn%20%5Cgeq%201%20:%20P%5En(x,%20x)%20%3E%200%5C%7D"> as containing all the time steps at which the chain can return to state <img src="https://latex.codecogs.com/png.latex?x">. If this set is <img src="https://latex.codecogs.com/png.latex?%5C%7B2,%204,%206,%208,%20%5Cldots%5C%7D"> (only even numbers), then <img src="https://latex.codecogs.com/png.latex?%5Cgcd%20=%202"> and the state has period 2. If the set is <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%202,%203,%204,%20%5Cldots%5C%7D"> (all positive integers), then <img src="https://latex.codecogs.com/png.latex?%5Cgcd%20=%201"> and the state is aperiodic.</p>
<p>In our example above, state <img src="https://latex.codecogs.com/png.latex?0"> has period 2 because the chain can only return to state <img src="https://latex.codecogs.com/png.latex?0"> in an even number of steps: <img src="https://latex.codecogs.com/png.latex?P%5E2(0,%200)%20=%201">, <img src="https://latex.codecogs.com/png.latex?P%5E4(0,%200)%20=%201">, and so on. The same is true for state <img src="https://latex.codecogs.com/png.latex?1">. This causes the distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%20P%5En"> to oscillate rather than converge as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">None</span>Example: PageRank is Aperiodic
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall our PageRank transition matrix where every entry satisfies <img src="https://latex.codecogs.com/png.latex?P(i,%20j)%20%5Cgeq%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20%3E%200">. This means that for any state <img src="https://latex.codecogs.com/png.latex?i">, we have <img src="https://latex.codecogs.com/png.latex?P%5E1(i,%20i)%20%5Cgeq%20%5Cfrac%7B%5Calpha%7D%7BN%7D%20%3E%200">. Therefore, <img src="https://latex.codecogs.com/png.latex?1"> is in the set of possible return times, which immediately gives us <img src="https://latex.codecogs.com/png.latex?%5Cgcd%20=%201"> (think about what the greatest common divisor of <img src="https://latex.codecogs.com/png.latex?1"> and any number is). So every state is aperiodic, making the entire chain aperiodic.</p>
</div>
</div>
<p>Maybe now we can prove convergence? Is Irreducibility and Aperiodicity enough? It turns out yes, but to answer that formally, we need a bit more machinery.</p>
<section id="convergence" class="level3">
<h3 class="anchored" data-anchor-id="convergence">Convergence</h3>
<p>Before we can even hope to prove convergence, we first need a way to measure how “close” two probability distributions are to each other. One intuitive approach is to ask: what’s the largest possible difference in the probabilities that <img src="https://latex.codecogs.com/png.latex?%5Cmu,%20%5Cnu"> assign to any event?</p>
<p>Formally, for any event <img src="https://latex.codecogs.com/png.latex?A%20%5Csubseteq%20%5Cchi">, we could compute <img src="https://latex.codecogs.com/png.latex?%7C%5Cmu(A)%20-%20%5Cnu(A)%7C">. If we take the worst case scenario over all events, we get <img src="https://latex.codecogs.com/png.latex?%0A%5Cmax_%7BA%20%5Csubseteq%20%5Cchi%7D%20%7C%5Cmu(A)%20-%20%5Cnu(A)%7C%0A"> This tells us, in the most extreme case, how much these distributions disagree about the probability of an event. Let’s see if we can simplify this definition a little bit:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmu(A)%20-%20%5Cnu(A)%20=%20%5Csum_%7Bx%20%5Cin%20A%7D%20%5Cmu(x)%20-%20%5Csum_%7Bx%20%5Cin%20A%7D%20%5Cnu(x)%20=%20%5Csum_%7Bx%20%5Cin%20A%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%0A"> Notice that this difference is maximized if we choose <img src="https://latex.codecogs.com/png.latex?A"> to contain exactly the points for which <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)%20%3E%20%5Cnu(x)">. Let’s call this set <img src="https://latex.codecogs.com/png.latex?A%5E+%20=%20%5C%7Bx%20%5Cin%20%5Cchi%20%5Cmid%20%5Cmu(x)%20%3E%20%5Cnu(x)%5C%7D">. Then, we can write <img src="https://latex.codecogs.com/png.latex?%0A%5Cmax_%7BA%20%5Csubseteq%20%5Cchi%7D%20%7C%5Cmu(A)%20-%20%5Cnu(A)%7C%20=%20%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%0A"> Now, since both <img src="https://latex.codecogs.com/png.latex?%5Cmu,%20%5Cnu"> are probability distributions, they must sum to one, so we have <img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%5Cmu(x)%20-%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cnu(x)%20=%200%0A"> This means that when we partition the sum over all points, we can write <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A&amp;0%20=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20=%20%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20+%20%5Csum_%7Bx%20%5Cnot%20%5Cin%20A%5E+%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20%5C%5C%0A%5Cend%7Balign*%7D"></p>
<p>which implies that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20=%20-%20%5Csum_%7Bx%20%5Cnot%20%5Cin%20A%5E+%7D%20%5B%5Cmu(x)%20-%20%5Cnu(x)%5D%20=%20%5Csum_%7Bx%20%5Cnot%20%5Cin%20A%5E+%7D%20%5B%5Cnu(x)%20-%20%5Cmu(x)%5D%0A"></p>
<p>Now, when we take the absolute difference over all points in <img src="https://latex.codecogs.com/png.latex?%5Cchi">, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20&amp;=%20%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20+%20%5Csum_%7Bx%20%5Cnot%20%5Cin%20A%5E+%7D%20%7C%5Cnu(x)%20-%20%5Cmu(x)%7C%20%5C%5C%0A&amp;=%202%20%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%0A%5Cend%7Balign*%7D"></p>
<p>Rearranging, we can finally write: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A2%20%5Csum_%7Bx%20%5Cin%20A%5E+%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20&amp;=%202%20%5Cmax_%7BA%20%5Csubseteq%20%5Cchi%7D%20%7C%5Cmu(A)%20-%20%5Cnu(A)%7C%20=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%5C%5C%0A%5Cmax_%7BA%20%5Csubseteq%20%5Cchi%7D%20%7C%5Cmu(A)%20-%20%5Cnu(A)%7C%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%0A%5Cend%7Balign*%7D"></p>
<p>This leads us to the convenient definition of distances between two distributions, known as the total variation difference:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition 5 (Total Variation Distance)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any two probability distributions <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu"> over <img src="https://latex.codecogs.com/png.latex?%5Cchi">, we define the total variation distance between <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu"> by <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%20:=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%0A"></p>
</div>
</div>
<p>The total variation distance is a metric on the space of probability distributions. It ranges from <img src="https://latex.codecogs.com/png.latex?0"> (when <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20%5Cnu">) to <img src="https://latex.codecogs.com/png.latex?1"> (when <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu"> are completely different). You should check that it is indeed a valid distance metric.</p>
<p>We can now formalize what we mean by “convergence to the stationary distribution” using this notion of distance.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Definition 6 (Convergence in Total Variation)
</div>
</div>
<div class="callout-body-container callout-body">
<p>We say that the distribution of <img src="https://latex.codecogs.com/png.latex?X_n"> converges to <img src="https://latex.codecogs.com/png.latex?%5Cpi"> in total variation as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty"> if for all starting distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0">, <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmu_0%20P%5En%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cto%200%20%5Ctext%7B%20as%20%7D%20n%20%5Cto%20%5Cinfty%0A"></p>
</div>
</div>
<p>This is equivalent to saying that for any state <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmathbb%7BP%7D%5BX_n%20=%20x%5D%20-%20%5Cpi(x)%7C%20=%200%0A"></p>
<p>Now we can restate the running question asked throughout this blog more precisely: we want to show that if <img src="https://latex.codecogs.com/png.latex?P"> is irreducible and aperiodic and <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> is an arbitrary initial distribution for <img src="https://latex.codecogs.com/png.latex?P">, then <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%20P%5En%20%5Cto%20%5Cpi"> in total variation.</p>
</section>
<section id="machinery-for-convergence" class="level3">
<h3 class="anchored" data-anchor-id="machinery-for-convergence">Machinery For Convergence</h3>
<p>To prove our formalized statement, we will need several technical results. Let’s build them up step by step.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 5
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a Markov chain is aperiodic, then there exists <img src="https://latex.codecogs.com/png.latex?N%20%5Cin%20%5Cmathbb%7BN%7D"> such that for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi"> and all <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N">, we have <img src="https://latex.codecogs.com/png.latex?P%5En(x,%20x)%20%3E%200"></p>
</div>
</div>
<p><em>Proof Sketch</em>:</p>
<p>Fix a state <img src="https://latex.codecogs.com/png.latex?x">. By definition of aperiodicity, we have <img src="https://latex.codecogs.com/png.latex?%5Cgcd%5C%7Bn%20%5Cgeq%201%20:%20P%5En(x,%20x)%20%3E%200%5C%7D%20=%201">. Let <img src="https://latex.codecogs.com/png.latex?S%20=%20%5C%7Bn%20%5Cgeq%201%20:%20P%5En(x,%20x)%20%3E%200%5C%7D"> be this set.</p>
<p>A classical result from number theory tells us that because <img src="https://latex.codecogs.com/png.latex?S"> consists of positive integers and <img src="https://latex.codecogs.com/png.latex?%5Cgcd(S)%20=%201">, then there exists some <img src="https://latex.codecogs.com/png.latex?N%20%5Cin%20%5Cmathbb%7BN%7D"> such that every integer <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N"> can be written as a non-negative integer linear combination of elements from <img src="https://latex.codecogs.com/png.latex?S"> (called the <a href="https://artofproblemsolving.com/wiki/index.php/Chicken_McNugget_Theorem?srsltid=AfmBOoo5b6Xz_eOFhntrmSXPZCTWQYJRxB4eI3eZH6yG3c9vPjVjfuX_">Chicken McNugget Theorem</a>). In other words, for all <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N">, we can write <img src="https://latex.codecogs.com/png.latex?%0An%20=%20%5Csum_%7Bi=1%7D%5E%7Bk%7D%20m_i%20s_i%0A"> where <img src="https://latex.codecogs.com/png.latex?s_1,%20%5Cldots,%20s_k%20%5Cin%20S"> and <img src="https://latex.codecogs.com/png.latex?m_1,%20%5Cldots,%20m_k"> are non-negative integers.</p>
<p>Now, for each <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N">, this means we can return to <img src="https://latex.codecogs.com/png.latex?x"> at time <img src="https://latex.codecogs.com/png.latex?n"> by taking the path that returns to <img src="https://latex.codecogs.com/png.latex?x"> at times <img src="https://latex.codecogs.com/png.latex?s_1">, or the path that returns to <img src="https://latex.codecogs.com/png.latex?x"> at time <img src="https://latex.codecogs.com/png.latex?2s_1,%203%20s_1%20%5Cldots,%20m_1%20s_1">. Similarly, we could also take the path that returns at times <img src="https://latex.codecogs.com/png.latex?m_1%20s_1%20+%20s_2,%20m_1%20s_1%20+%202s_2,%20%5Cldots,%20m_1%20s_1%20+%20m_2%20s_2">, and so on. At each of these intermediate times, the chain is at state <img src="https://latex.codecogs.com/png.latex?x">, and we know <img src="https://latex.codecogs.com/png.latex?P%5E%7Bs_i%7D(x,%20x)%20%3E%200"> for each <img src="https://latex.codecogs.com/png.latex?i"> because <img src="https://latex.codecogs.com/png.latex?s_i%20%5Cin%20S">.</p>
<p>By the Markov property, the probability of following this entire path is the product of the transition probabilities:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AP%5En(x,x)%20%5Cgeq%20%5Cunderbrace%7BP%5E%7Bs_1%7D(x,%20x)%20%5Ccdot%20P%5E%7Bs_1%7D(x,%20x)%20%5Ccdots%20P%5E%7Bs_1%7D(x,%20x)%7D_%7Bm_1%20%5Ctext%7B%20times%7D%7D%20%5Ccdot%20%5Cunderbrace%7BP%5E%7Bs_2%7D(x,%20x)%20%5Ccdots%20P%5E%7Bs_2%7D(x,%20x)%7D_%7Bm_2%20%5Ctext%7B%20times%7D%7D%20%5Ccdots%20%3E%200%0A%5Cend%7Balign*%7D%0A"></p>
<p>Therefore, <img src="https://latex.codecogs.com/png.latex?P%5En(x,%20x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N">, as desired.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 6
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a Markov chain is irreducible and aperiodic, then there exists <img src="https://latex.codecogs.com/png.latex?N%20%5Cin%20%5Cmathbb%7BN%7D"> such that <img src="https://latex.codecogs.com/png.latex?P%5EN(x,%20y)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">.</p>
</div>
</div>
<p><em>Proof Sketch</em>:</p>
<p>By Proposition 5, we know that for each state <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">, there exists some <img src="https://latex.codecogs.com/png.latex?N_x"> such that <img src="https://latex.codecogs.com/png.latex?P%5En(x,%20x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N_x">. Because our state space is finite, we can take <img src="https://latex.codecogs.com/png.latex?N_1%20=%20%5Cmax_%7Bx%20%5Cin%20%5Cchi%7D%20N_x">, which ensures that <img src="https://latex.codecogs.com/png.latex?P%5En(x,%20x)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi"> and all <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%20N_1">.</p>
<p>Now, by irreducibility, for any two states <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">, there exists some <img src="https://latex.codecogs.com/png.latex?n_%7Bxy%7D%20%5Cin%20%5Cmathbb%7BN%7D"> such that <img src="https://latex.codecogs.com/png.latex?P%5E%7Bn_%7Bxy%7D%7D(x,%20y)%20%3E%200">. Again, because the state space is finite, we can take <img src="https://latex.codecogs.com/png.latex?N_2%20=%20%5Cmax_%7Bx,%20y%20%5Cin%20%5Cchi%7D%20n_%7Bxy%7D">.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?N%20=%20N_1%20+%20N_2">. For any states <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">, we can go from <img src="https://latex.codecogs.com/png.latex?x"> to <img src="https://latex.codecogs.com/png.latex?y"> in exactly <img src="https://latex.codecogs.com/png.latex?N"> steps by first spending <img src="https://latex.codecogs.com/png.latex?N_2"> steps to get from <img src="https://latex.codecogs.com/png.latex?x"> to <img src="https://latex.codecogs.com/png.latex?y">, and then spending <img src="https://latex.codecogs.com/png.latex?N_1"> steps returning from <img src="https://latex.codecogs.com/png.latex?y"> to <img src="https://latex.codecogs.com/png.latex?y">. We know both of these are possible with positive probability, so: <img src="https://latex.codecogs.com/png.latex?%0AP%5EN(x,%20y)%20%5Cgeq%20P%5E%7BN_2%7D(x,%20y)%20%5Ccdot%20P%5E%7BN_1%7D(y,%20y)%20%3E%200%0A"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?x,%20y"> were arbitrary this holds for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">, as desired.</p>
<p>The next result is a key technical tool that shows how the transition matrix acts as a contraction in total variation distance.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 7
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <img src="https://latex.codecogs.com/png.latex?P"> be the transition matrix of a Markov chain. If <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu"> are any two probability distributions, then <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmu%20P%20-%20%5Cnu%20P%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cleq%20%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A"></p>
</div>
</div>
<p><em>Proof Sketch</em>:</p>
<p>We need to show that applying one step of the Markov chain cannot increase the total variation distance between two distributions. Let’s compute:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5C%7C%5Cmu%20P%20-%20%5Cnu%20P%5C%7C_%7B%5Ctext%7BTV%7D%7D%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%7C(%5Cmu%20P)(y)%20-%20(%5Cnu%20P)(y)%7C%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Cleft%7C%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20P(x,%20y)%20-%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cnu(x)%20P(x,%20y)%20%5Cright%7C%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Cleft%7C%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20(%5Cmu(x)%20-%20%5Cnu(x))%20P(x,%20y)%20%5Cright%7C%20%5C%5C%0A&amp;%5Cleq%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20P(x,%20y)%20%5Ctag%7BBy%20triangle%20inequality%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20P(x,%20y)%20%5Ctag%7BSwap%20sums%20bc%20$%7C%5Cchi%7C%20%3C%20%5Cinfty$%7D%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%7C%5Cmu(x)%20-%20%5Cnu(x)%7C%20%5Ccdot%201%20%5Ctag%7Brows%20of%20$P$%20sum%20to%201%7D%5C%5C%0A&amp;=%20%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>as desired.</p>
<p>This is very important! It tells us that the Markov Chain’s transition matrix can decrease distances but never increases them. However, to get convergence, we need something stronger. We need the chain to actually contract distances, not just preserve them. This is where irreducibility and aperiodicity come in.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Proposition 8 (Harris Lemma)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose there exists a probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and a number <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%3E%200"> such that <img src="https://latex.codecogs.com/png.latex?P(x,%20y)%20%5Cgeq%20%5Cdelta%20%5Cgamma(y)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">. Then for any two probability distributions <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu">, <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmu%20P%20-%20%5Cnu%20P%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cleq%20(1%20-%20%5Cdelta)%20%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A"></p>
</div>
</div>
<p><em>Proof Sketch</em>:</p>
<p>The main idea is to decompose the transition matrix <img src="https://latex.codecogs.com/png.latex?P"> into two parts: one that both distributions agree on, and a remaining part <img src="https://latex.codecogs.com/png.latex?P_2"> that has the differences. More precisely, I claim we can write: <img src="https://latex.codecogs.com/png.latex?%0AP(x,%20y)%20=%20%5Cdelta%20%5Cgamma(y)%20+%20(1%20-%20%5Cdelta)%20P_2(x,%20y)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?P_2"> is itself a stochastic matrix defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP_2(x,%20y)%20:=%20%5Cfrac%7BP(x,%20y)%20-%20%5Cdelta%20%5Cgamma(y)%7D%7B1%20-%20%5Cdelta%7D%0A"></p>
<p>We need to show that this is a stochastic matrix, which means showing that the entries are non negative, and the rows sum up to one. Since we’re given that <img src="https://latex.codecogs.com/png.latex?P(x,%20y)%20%5Cgeq%20%5Cdelta%20%5Cgamma(y)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">, we have: <img src="https://latex.codecogs.com/png.latex?%0AP(x,%20y)%20-%20%5Cdelta%20%5Cgamma(y)%20%5Cgeq%200%0A"></p>
<p>And since <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%3C%201"> (otherwise the proposition is trivial), we have <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Cdelta%20%3E%200">. Therefore:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP_2(x,%20y)%20=%20%5Cfrac%7BP(x,%20y)%20-%20%5Cdelta%20%5Cgamma(y)%7D%7B1%20-%20%5Cdelta%7D%20%5Cgeq%200%0A"></p>
<p>Finally, the rows of <img src="https://latex.codecogs.com/png.latex?P_2"> sum to <img src="https://latex.codecogs.com/png.latex?1"> since for any fixed <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20%5Cchi">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Csum_%7By%20%5Cin%20%5Cchi%7D%20P_2(x,%20y)%20&amp;=%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Cfrac%7BP(x,%20y)%20-%20%5Cdelta%20%5Cgamma(y)%7D%7B1%20-%20%5Cdelta%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B1%20-%20%5Cdelta%7D%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5BP(x,%20y)%20-%20%5Cdelta%20%5Cgamma(y)%5D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B1%20-%20%5Cdelta%7D%20%5Cleft%5B%5Csum_%7By%20%5Cin%20%5Cchi%7D%20P(x,%20y)%20-%20%5Cdelta%20%5Csum_%7By%20%5Cin%20%5Cchi%7D%20%5Cgamma(y)%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B1%20-%20%5Cdelta%7D%20%5B1%20-%20%5Cdelta%20%5Ccdot%201%5D%20%5C%5C%0A&amp;=%201%0A%5Cend%7Balign%7D"></p>
<p>where we used that <img src="https://latex.codecogs.com/png.latex?P"> is stochastic (so <img src="https://latex.codecogs.com/png.latex?%5Csum_y%20P(x,%20y)%20=%201">) and <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> is a probability distribution (so <img src="https://latex.codecogs.com/png.latex?%5Csum_y%20%5Cgamma(y)%20=%201">).</p>
<p>Now, when we apply <img src="https://latex.codecogs.com/png.latex?P"> to any distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu">, we see</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A(%5Cmu%20P)(y)%20&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20P(x,%20y)%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20%5B%5Cdelta%20%5Cgamma(y)%20+%20(1%20-%20%5Cdelta)%20P_2(x,%20y)%5D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20%5Cdelta%20%5Cgamma(y)%20+%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20(1%20-%20%5Cdelta)%20P_2(x,%20y)%20%5C%5C%0A&amp;=%20%5Cdelta%20%5Cgamma(y)%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20+%20(1%20-%20%5Cdelta)%20%5Csum_%7Bx%20%5Cin%20%5Cchi%7D%20%5Cmu(x)%20P_2(x,%20y)%20%5C%5C%0A&amp;=%20%5Cdelta%20%5Cgamma(y)%20%5Ccdot%201%20+%20(1%20-%20%5Cdelta)%20(%5Cmu%20P_2)(y)%20%5C%5C%0A&amp;=%20%5Cdelta%20%5Cgamma(y)%20+%20(1%20-%20%5Cdelta)%20(%5Cmu%20P_2)(y)%0A%5Cend%7Balign%7D"></p>
<p>Plugging everything in, we can now show the result: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5C%7C%5Cmu%20P%20-%20%5Cnu%20P%5C%7C_%7B%5Ctext%7BTV%7D%7D%20&amp;=%20%5Cleft%5C%7C%20(%5Cdelta%20%5Cgamma%20+%20(1%20-%20%5Cdelta)%20%5Cmu%20P_2)%20-%20(%5Cdelta%20%5Cgamma%20+%20(1%20-%20%5Cdelta)%20%5Cnu%20P_2%20)%5Cright%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;=%20%5Cleft%5C%7C%20%5Cdelta%20%5Cgamma%20+%20(1%20-%20%5Cdelta)%20%5Cmu%20P_2%20-%20%5Cdelta%20%5Cgamma%20-%20(1%20-%20%5Cdelta)%20%5Cnu%20P_2%20%5Cright%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;=%20(1%20-%20%5Cdelta)%20%5C%7C%5Cmu%20P_2%20-%20%5Cnu%20P_2%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;%5Cleq%20(1%20-%20%5Cdelta)%20%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Ctag%7BBy%20Proposition%207%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>This shows that the distance contracts by a factor of at least <img src="https://latex.codecogs.com/png.latex?(1%20-%20%5Cdelta)"> in each step.</p>
<p>Now we have all the tools we need to prove the main convergence theorem</p>
</section>
<section id="proof-of-convergence" class="level3">
<h3 class="anchored" data-anchor-id="proof-of-convergence">Proof of Convergence</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Theorem 2 (Convergence)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <img src="https://latex.codecogs.com/png.latex?P"> be the transition matrix of an irreducible and aperiodic Markov chain with stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Then for any initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and any state <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cchi">: <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20(%5Cmu_0%20P%5En)(y)%20=%20%5Cpi(y)%0A"></p>
<p>In other words, <img src="https://latex.codecogs.com/png.latex?%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5Cmu_n%20=%20%5Cpi">, regardless of where we started.</p>
</div>
</div>
<p><em>Proof Sketch of Theorem 2</em>:</p>
<p>By Proposition 6, since our chain is irreducible and aperiodic, there exists some <img src="https://latex.codecogs.com/png.latex?N%20%5Cin%20%5Cmathbb%7BN%7D"> such that <img src="https://latex.codecogs.com/png.latex?P%5EN(x,%20y)%20%3E%200"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cchi">.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20=%20%5Cmin_%7Bx,%20y%20%5Cin%20%5Cchi%7D%20P%5EN(x,%20y)">. This minimum exists and is positive because we’re taking the minimum over a finite set of positive numbers. Now define <img src="https://latex.codecogs.com/png.latex?%5Cgamma(y)%20=%20%5Cfrac%7B1%7D%7B%7C%5Cchi%7C%7D"> to be the uniform distribution over all states.</p>
<p>I claim that <img src="https://latex.codecogs.com/png.latex?P%5EN(x,%20y)%20%5Cgeq%20%5Cdelta%20%5Cgamma(y)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y">. To see this, note that: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AP%5EN(x,%20y)%20&amp;%5Cgeq%20%5Cdelta%20%5Ctag%7BBy%20defn%7D%20%5C%5C%0A&amp;=%20%5Cdelta%20%5Ccdot%20%5Cfrac%7B1%7D%7B%7C%5Cchi%7C%7D%20%5Ccdot%20%7C%5Cchi%7C%20%5C%5C%0A&amp;%5Cgeq%20%5Cdelta%20%5Ccdot%20%5Cfrac%7B1%7D%7B%7C%5Cchi%7C%7D%20%5Ctag%7BB/c%20$%7C%5Cchi%7C%20%3E%200$%20%7D%20%5C%5C%0A&amp;=%20%5Cdelta%20%5Cgamma(y)%0A%5Cend%7Balign*%7D"></p>
<p>Therefore, by Proposition 8, for any two distributions <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cnu">: <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmu%20P%5EN%20-%20%5Cnu%20P%5EN%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cleq%20(1%20-%20%5Cdelta)%20%5C%7C%5Cmu%20-%20%5Cnu%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A"></p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> be any arbitrary initial distribution for <img src="https://latex.codecogs.com/png.latex?P">. We can use induction and the above observation to show that for all <img src="https://latex.codecogs.com/png.latex?k%20%5Cgeq%201">, we have: <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C(%5Cmu_0%20P%5E%7BkN%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cleq%20(1%20-%20%5Cdelta)%5Ek%20%5C%7C%5Cmu_0%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A"></p>
<p>The base case <img src="https://latex.codecogs.com/png.latex?k%20=%201"> follows from Proposition 8: <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C(%5Cmu_0%20P%5E%7BN%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20=%20%5C%7C(%5Cmu_0%20P%5E%7BN%7D)%20-%20%5Cpi%20P%5EN%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Cleq%20(1%20-%20%5Cdelta)%20%5C%7C%5Cmu_0%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A"></p>
<p>Now assume the result holds for <img src="https://latex.codecogs.com/png.latex?k">. We need to show it holds for <img src="https://latex.codecogs.com/png.latex?k+1">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5C%7C(%5Cmu_0%20P%5E%7B(k+1)N%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20&amp;=%20%5C%7C(%5Cmu_0%20P%5E%7Bk%20N%7D%20P%5EN)%20-%20(%5Cpi%20P%5EN)%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;%5Cleq%20(1%20-%20%5Cdelta)%20%5C%7C(%5Cmu_0%20P%5E%7BkN%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Ctag%7BProposition%208%7D%20%5C%5C%0A&amp;%5Cleq%20(1%20-%20%5Cdelta)%20%5Ccdot%20(1%20-%20%5Cdelta)%5Ek%20%5C%7C%5Cmu_0%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Ctag%7BInduction%20hypothesis%7D%20%5C%5C%0A&amp;=%20(1%20-%20%5Cdelta)%5E%7Bk+1%7D%20%5C%7C%5Cmu_0%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A%5Cend%7Balign%7D"></p>
<p>which completes the induction. Now, since <img src="https://latex.codecogs.com/png.latex?0%20%3C%20(1%20-%20%5Cdelta)%20%3C%201">, we have <img src="https://latex.codecogs.com/png.latex?(1%20-%20%5Cdelta)%5Ek%20%5Cto%200"> as <img src="https://latex.codecogs.com/png.latex?k%20%5Cto%20%5Cinfty">. This means: <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bk%20%5Cto%20%5Cinfty%7D%20%5C%7C(%5Cmu_0%20P%5E%7BkN%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20=%200%0A"></p>
<p>To handle times that aren’t multiples of <img src="https://latex.codecogs.com/png.latex?N">, note that for any <img src="https://latex.codecogs.com/png.latex?n%20%5Cgeq%200">, we can write <img src="https://latex.codecogs.com/png.latex?n%20=%20kN%20+%20r"> where <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20r%20%3C%20N">. Then: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5C%7C(%5Cmu_0%20P%5En)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20&amp;=%20%5C%7C(%5Cmu_0%20P%5E%7BkN%20+%20r%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;=%20%5C%7C(%5Cmu_0%20P%5E%7BkN%7D)%20P%5Er%20-%20%5Cpi%20P%5Er%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5C%5C%0A&amp;%5Cleq%20%5C%7C(%5Cmu_0%20P%5E%7BkN%7D)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20%5Ctag%7BProposition%207%7D%20%5C%5C%0A&amp;%5Cleq%20(1%20-%20%5Cdelta)%5Ek%20%5C%7C%5Cmu_0%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%0A%5Cend%7Balign%7D"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?k%20%5Cto%20%5Cinfty"> as <img src="https://latex.codecogs.com/png.latex?n%20%5Cto%20%5Cinfty">, we conclude that: <img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bn%20%5Cto%20%5Cinfty%7D%20%5C%7C(%5Cmu_0%20P%5En)%20-%20%5Cpi%5C%7C_%7B%5Ctext%7BTV%7D%7D%20=%200%0A"></p>
<p>This proves that the distribution converges to <img src="https://latex.codecogs.com/png.latex?%5Cpi"> in total variation, completing the main proof.</p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">None</span>Remark
</div>
</div>
<div class="callout-body-container callout-body">
<p>The convergence is exponentially fast with rate <img src="https://latex.codecogs.com/png.latex?(1%20-%20%5Cdelta)">, which depends on the smallest entry of <img src="https://latex.codecogs.com/png.latex?P%5EN">. This tells us not just that convergence happens, but also gives us a quantitative bound on how quickly the chain converges.</p>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We began this journey with a simple question: will the PageRank random surfer always produce the same long term behavior? After developing the full theory of Markov chains, we can now definitively answer this question as yes.</p>
<p>The PageRank chain is both irreducible (every page is reachable from every other page due to the random jump mechanism) and aperiodic (we can return to any page in one step with positive probability). By Theorem 2, this guarantees that:</p>
<ol type="1">
<li>There exists a unique stationary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"></li>
<li>Starting from any initial distribution <img src="https://latex.codecogs.com/png.latex?%5Cmu_0">, the chain converges to <img src="https://latex.codecogs.com/png.latex?%5Cpi"> in total variation</li>
<li>The convergence is exponentially fast</li>
</ol>
<p>This means that no matter where the random surfer starts, after enough time, the proportion of visits to each page will converge to the same values given by <img src="https://latex.codecogs.com/png.latex?%5Cpi">. These limiting proportions define a well-defined, consistent ranking of webpage importance.</p>
<p>But the power of this theory extends far beyond web search. Irreducible and aperiodic Markov chains appear throughout science, engineering and financial markets. For example, in statistical physics, they can model particle systems reaching thermal equilibrium. In machine learning, they allow for MCMC sampling algorithms like Metropolis-Hastings. In queueing theory, they predict long-run behavior of service systems. Finally, in finance, they model the evolution of asset prices and credit ratings.</p>
<p>Having this rigorous theoretical background on Markov Chains now allows you to tackle all these problems.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>cs</category>
  <guid>https://csuraparaju.github.io/posts/markov-chains/</guid>
  <pubDate>Wed, 15 Oct 2025 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Algorithms for Sampling from a probability distribution</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/sampling/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Drawing samples from a probability distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a problem that arises often in computer science. For example in generative AI, language models sample tokens from learned probability distributions to generate new text. In computer graphics, many graphical engines randomly sample light paths bouncing around to simulate realistic lighting. But what exactly does it mean for a computer or an algorithm to sample from a probability distribution? How can a deterministic machine like a modern computer sample “randomly” from a distribution?</p>
</section>
<section id="hardware-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="hardware-assumptions">Hardware Assumptions</h2>
<p>On most modern computers, one bit represents an atom of data or computation. So, if we want our computer to generate random samples, we would probably need it to be able to generate uniformly random bits. Let’s assume we have a black box machine that does exactly that.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Above assumption is not an unrealistic one since we have <a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator">hardware random bit generators</a> which generate random bits from physical process that produce entropy.</p>
</div>
</div>
</div>
<p>More formally, let <img src="https://latex.codecogs.com/png.latex?B%20%5Cin%20%5C%7B0,%201%5C%7D"> be a uniform random bit with <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5BB%20=%200%5D%20=%20%5Cmathbb%7BP%7D%5BB%20=%201%5D%20=%20%5Cfrac%7B1%7D%7B2%7D"> and assume that we can sample from <img src="https://latex.codecogs.com/png.latex?B">.</p>
</section>
<section id="the-uniform-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-uniform-distribution">The Uniform Distribution</h2>
<p>Let’s start the discussion by coming up with algorithms for sampling integers uniformly randomly in a fixed range.</p>
<section id="integer-in-range-0-2n" class="level3">
<h3 class="anchored" data-anchor-id="integer-in-range-0-2n">Integer in range <img src="https://latex.codecogs.com/png.latex?%5B0,%202%5EN%5D"></h3>
<p>First, let’s see if we can generate a uniformly random integer <img src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5C%7B0,%20%5Ccdots,%202%5EN%20-%201%5C%7D">. That is, we want to come up with an algorithm to produce <img src="https://latex.codecogs.com/png.latex?M"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5BM%20=%20m%5D%20=%20%5Cfrac%7B1%7D%7B2%5EN%7D"> for all <img src="https://latex.codecogs.com/png.latex?m%20%5Cin%20%5C%7B0,%20%5Ccdots,%202%5EN%20-%201%5C%7D">.</p>
<p>Well, given that we only have a random bit generator, there is really only one thing we can do here. We should generate <img src="https://latex.codecogs.com/png.latex?N"> independent bits using our random bit generator and concatenate them together to get the base 2 representation of the integer we return.</p>
<div class="algorithm">
<p><strong>Algorithm 1</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?M%20%5Cgets%200"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?i%20=%201"> to <img src="https://latex.codecogs.com/png.latex?N"> do</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad%20M%20%5Cgets%20M%20+%20B_i%20%5Ccdot%202%5Ei"></p>
<p>return <img src="https://latex.codecogs.com/png.latex?M"></p>
</div>
<p>Let’s check that this gives us the desired result. Let <img src="https://latex.codecogs.com/png.latex?B_1,%20B_2,%20%5Cdots,%20B_N"> be the bits we generated. Now, note that for any integer <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20%5C%7B0,%20%5Ccdots,%202%5EN%20-%201%5C%7D">, there exists a unique binary representation: <img src="https://latex.codecogs.com/png.latex?%0Ak%20=%20%5Csum_%7Bi%20=%201%7D%5E%7BN%7D%20b_i%20%5Ccdot%202%5Ei%0A"> where each <img src="https://latex.codecogs.com/png.latex?b_i%20%5Cin%20%5C%7B0,%201%5C%7D"> is the <img src="https://latex.codecogs.com/png.latex?i">-th bit of <img src="https://latex.codecogs.com/png.latex?k">. The probability that <img src="https://latex.codecogs.com/png.latex?M%20=%20k"> is given as: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BM%20=%20k%5D%20&amp;=%20%5Cmathbb%7BP%7D%5BB_0%20=%20b_0,%20B_1%20=%20b_1,%20%5Ccdots,%20B_n%20=%20b_n%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BP%7D%5BB_0%20=%20b_0%5D%20%5Ccdot%20%5Cmathbb%7BP%7D%5BB_1%20=%20b_1%5D%20%5Ccdots%20%5Cmathbb%7BP%7D%5BB_N%20=%20b_n%5D%20%5Ctag%7BBy%20independence%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Ccdot%20%5Cfrac%7B1%7D%7B2%7D%20%5Ccdots%20%5Cfrac%7B1%7D%7B2%7D%20%5Ctag%7B$N$%20times%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%5EN%7D%0A%5Cend%7Balign*%7D"> as desired.</p>
</section>
<section id="integer-in-arbitrary-range" class="level3">
<h3 class="anchored" data-anchor-id="integer-in-arbitrary-range">Integer in Arbitrary Range</h3>
<p>Great, now lets move onto a slightly more difficult problem: generating a uniformly random integer in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,%20%5Cdots,%20M%5C%7D">, where <img src="https://latex.codecogs.com/png.latex?M"> is not necessarily a power of <img src="https://latex.codecogs.com/png.latex?2">.</p>
<p>A first idea would be to use the same approach as before and concatenate a string of <img src="https://latex.codecogs.com/png.latex?N"> random bits. This, however, does not work. For example, if <img src="https://latex.codecogs.com/png.latex?N%20=%203"> and <img src="https://latex.codecogs.com/png.latex?M%20=%204"> then our random bit generator produces outcomes in the range <img src="https://latex.codecogs.com/png.latex?%5C%7B0,%20%5Cdots,%202%5E3%20-%201%5C%7D%20=%20%5C%7B0,%201,%202,%203,%204,%205,%206,%207%5C%7D">. Each of these <img src="https://latex.codecogs.com/png.latex?8"> outcomes occurs with probability <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B8%7D">. However, we wanted an integer in the range <img src="https://latex.codecogs.com/png.latex?%5C%7B0,%201,%202,%203,%204%5C%7D">, and the probability of each outcome to be exactly <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B5%7D">.</p>
<p>Another approach would be to generate <img src="https://latex.codecogs.com/png.latex?N"> random bits as before, and then return <img src="https://latex.codecogs.com/png.latex?N%20%5Cmod%20(M%20+%201)">. I leave it as an exercise to figure out why this approach also does not work (Hint: use the same counter example from above).</p>
<p>Notice that the issue in the above two approach is that our algorithm considers values that are outside the given range. To fix this problem, one idea would be to simply ignore an integers that are outside the range. More formally, define the algorithm as</p>
<div class="algorithm">
<p><strong>Algorithm 2</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?P%20%5Cgets%20%5Clceil%20%5Clog_2%20(M%20+%201)%20%5Crceil"></p>
<p>repeat</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> Generate <img src="https://latex.codecogs.com/png.latex?N%20%5Cin%20%5C%7B0,%20%5Ccdots,%202%5EP%20-%201%5C%7D"> using Algorithm 1</p>
<p>until <img src="https://latex.codecogs.com/png.latex?N%20%5Cleq%20M"></p>
<p>return <img src="https://latex.codecogs.com/png.latex?N"></p>
</div>
<p>This algorithm is called rejection sampling. A subtle difference between this algorithm and the previous algorithm is that its running time is non-deterministic. For example, we know that algorithm 1 terminates after generating <img src="https://latex.codecogs.com/png.latex?N"> random bits. Algorithm 2, however, has no such guarantees. It is technically possible that we get really really really unlucky and always generate <img src="https://latex.codecogs.com/png.latex?N%20%3E%20M"> and so the algorithm would never return a number.</p>
<p>So, let’s compute probability that the algorithm terminates, which only happens when <img src="https://latex.codecogs.com/png.latex?N%20%5Cleq%20M"> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BN%20%5Cleq%20M%5D%20&amp;=%20%5Csum_%7Bi%20%5Cleq%20M%7D%20%5Cmathbb%7BP%7D%5BN%20=%20i%5D%20%5Ctag%7BCDF%20of%20$N$%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bi%20=%200%7D%5EM%20%5Cfrac%7B1%7D%7B2%5EN%7D%20%5Ctag%7BFrom%20Algorithm%201%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B(M%20+%201)%7D%7B2%5EN%7D%0A%5Cend%7Balign*%7D"></p>
<p>Note that if <img src="https://latex.codecogs.com/png.latex?M%20%5Cll%20N"> then the exponential in the denominator dominates, and the probability of terminating is near zero. When <img src="https://latex.codecogs.com/png.latex?M%20%5Cgg%20N"> then the fraction is approximately linear, and so the algorithm terminates with higher probability.</p>
<p>However, in the case that the algorithm does return, then we can be sure that it is correct. We prove this below. Note that for any <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20%5C%7B0,%20%5Ccdots,%20M%5C%7D">, we have <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BN%20=%20k%5D%20&amp;=%20%5Cmathbb%7BP%7D%5BN%20=%20k%20%7C%20N%20%5Cleq%20M%5D%20%5Ctag%7BSince%20$N%20%5Cleq%20M$%20iff%20$N$%20is%20returned%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cmathbb%7BP%7D%5BN%20=%20k,%20N%20%5Cleq%20M%5D%7D%7B%5Cmathbb%7BP%7D%5BN%20%5Cleq%20M%5D%7D%20%5Ctag%7BDefinition%20of%20conditional%20prob.%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cmathbb%7BP%7D%5BN%20=%20k%5D%7D%7B%5Cmathbb%7BP%7D%5BN%20%5Cleq%20M%5D%7D%20%5Ctag%7B$k%20%5Cleq%20M$%20so%20if%20$N%20=%20k$,%20we%20know%20$N%20%5Cleq%20M$%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1/2%5EN%7D%7B(M+1)/2%5EN%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7BM%20+%201%7D%0A%5Cend%7Balign*%7D"> as desired.</p>
</section>
<section id="real-number-in-range-0-1" class="level3">
<h3 class="anchored" data-anchor-id="real-number-in-range-0-1">Real number in range <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"></h3>
<p>Now let’s move onto the hardest problem yet: sampling a real number in the range <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">. Note that every real number <img src="https://latex.codecogs.com/png.latex?r%20%5Cin%20%5B0,%201%5D"> can be represented as a binary expansion: <img src="https://latex.codecogs.com/png.latex?%0Ar%20=%20%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7BB_i%7D%7B2%5Ei%7D%20=%200.B_1B_2B_3%5Cdots%0A"> where each <img src="https://latex.codecogs.com/png.latex?B_i%20%5Cin%20%5C%7B0,%201%5C%7D">. Again, our first instinct should be to ask if generating an infinite string of bits and concatenating them gives us the desired result. In this case, it turns out to be true. However, the proof of this fact is non-trivial and takes quite a lot of work.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Main Claim
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <img src="https://latex.codecogs.com/png.latex?B_1,%20B_2,%20B_3,%20%5Cdots"> are independent uniform random bits, then <img src="https://latex.codecogs.com/png.latex?%0AU%20=%20%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7BB_i%7D%7B2%5Ei%7D%0A"> is uniformly distributed on <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">.</p>
</div>
</div>
<p>To prove this, we need to show that for any interval <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b)%20%5Csubseteq%20%5B0,%201%5D">, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BU%20%5Cin%20%5Ba,%20b)%5D%20=%20%5Ctext%7Blength%7D(%5Ba,%20b)%5D)%20=%20b%20-%20a%0A"></p>
<section id="partitioning-0-1-and-characterizing-the-partitions" class="level4">
<h4 class="anchored" data-anchor-id="partitioning-0-1-and-characterizing-the-partitions">Partitioning <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> and characterizing the partitions</h4>
<p>The proof uses a clever partitioning argument. We’ll divide <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> into dyadic intervals, which are defined to be intervals whose endpoints are fractions with powers of 2 in the denominator.</p>
<p>For any arbitrary <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20%5Cmathbb%7BN%7D">, we can partition <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> into <img src="https://latex.codecogs.com/png.latex?2%5Ek"> equal intervals: <img src="https://latex.codecogs.com/png.latex?%0AI_j%20=%20%5Cleft%5B%5Cfrac%7Bj%7D%7B2%5Ek%7D,%20%5Cfrac%7Bj+1%7D%7B2%5Ek%7D%5Cright)%20%5Cquad%20%5Ctext%7Bfor%20%7D%20j%20=%200,%201,%20%5Cdots,%202%5Ek%20-%201%0A"></p>
<p>Each interval has length <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%5Ek%7D">, and together they cover the entire unit interval. The larger <img src="https://latex.codecogs.com/png.latex?k">, the better this approximation gets.</p>
<p>Now, I claim that the first <img src="https://latex.codecogs.com/png.latex?k"> bits <img src="https://latex.codecogs.com/png.latex?(B_1,%20%5Cdots,%20B_k)"> of <img src="https://latex.codecogs.com/png.latex?U"> completely determine which dyadic interval <img src="https://latex.codecogs.com/png.latex?I_j"> contains the value <img src="https://latex.codecogs.com/png.latex?U">. Specifically: <img src="https://latex.codecogs.com/png.latex?%0AU%20%5Cin%20I_j%20=%20%5Cleft%5B%5Cfrac%7Bj%7D%7B2%5Ek%7D,%20%5Cfrac%7Bj+1%7D%7B2%5Ek%7D%5Cright)%20%5Ciff%20%5Csum_%7Bi=1%7D%5E%7Bk%7D%20%5Cfrac%7BB_i%7D%7B2%5E%7Bi%7D%7D%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D%0A"></p>
<p>To see why, we can decompose <img src="https://latex.codecogs.com/png.latex?U"> into two parts:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU%20=%20%5Cunderbrace%7B%5Csum_%7Bi=1%7D%5E%7Bk%7D%20%5Cfrac%7BB_i%7D%7B2%5Ei%7D%7D_%7B:=U_k%7D%20+%20%5Cunderbrace%7B%5Csum_%7Bi=k+1%7D%5E%7B%5Cinfty%7D%5Cfrac%7BB_i%7D%7B2%5Ei%7D%7D_%7B:=R_k%7D%0A"> where <img src="https://latex.codecogs.com/png.latex?U_k"> represents the first <img src="https://latex.codecogs.com/png.latex?k"> bits and <img src="https://latex.codecogs.com/png.latex?R_k"> represents the remaining bits. Now, note that The first <img src="https://latex.codecogs.com/png.latex?k"> bits can only produce discrete values because there are <img src="https://latex.codecogs.com/png.latex?2%5Ek"> possible bit strings of length <img src="https://latex.codecogs.com/png.latex?k">, and these values are <img src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7B0,%20%5Cfrac%7B1%7D%7B2%5Ek%7D,%20%5Cfrac%7B2%7D%7B2%5Ek%7D,%20%5Cldots,%20%5Cfrac%7B2%5Ek-1%7D%7B2%5Ek%7D%5Cright%5C%7D"> Observe that these are exactly the left endpoints of our dyadic intervals <img src="https://latex.codecogs.com/png.latex?I_j">.</p>
<p>Now, The remaining bits <img src="https://latex.codecogs.com/png.latex?R_k"> can be rewritten by factoring out <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%5Ek%7D">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AR_k%20&amp;=%20%5Csum_%7Bi=k+1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7BB_i%7D%7B2%5Ei%7D%20=%20%5Cfrac%7B1%7D%7B2%5Ek%7D%20%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7BB_%7Bk+i%7D%7D%7B2%5Ei%7D%0A%5Cend%7Balign*%7D"> Since <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7B%5Cinfty%7D%20%5Cfrac%7BB_%7Bk+i%7D%7D%7B2%5Ei%7D"> is a binary expansion taking values in <img src="https://latex.codecogs.com/png.latex?%5B0,%201)">, we have <img src="https://latex.codecogs.com/png.latex?0%20%5Cleq%20R_k%20%3C%20%5Cfrac%7B1%7D%7B2%5Ek%7D"> So, we have that <img src="https://latex.codecogs.com/png.latex?R_k"> is strictly less than the width of a dyadic interval.</p>
<p>Therefore, we can now claim that if <img src="https://latex.codecogs.com/png.latex?U_k%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D">, then <img src="https://latex.codecogs.com/png.latex?U%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D%20+%20R_k"> must satisfy <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bj%7D%7B2%5Ek%7D%20%5Cleq%20U%20%3C%20%5Cfrac%7Bj+1%7D%7B2%5Ek%7D"> placing <img src="https://latex.codecogs.com/png.latex?U"> in interval <img src="https://latex.codecogs.com/png.latex?I_j">. Conversely, if <img src="https://latex.codecogs.com/png.latex?U%20%5Cin%20I_j">, the constraint <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bj%7D%7B2%5Ek%7D%20%5Cleq%20U%20%3C%20%5Cfrac%7Bj+1%7D%7B2%5Ek%7D"> combined with the fact that <img src="https://latex.codecogs.com/png.latex?U_k"> is a discrete value in this range forces <img src="https://latex.codecogs.com/png.latex?U_k%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D">. Therefore, we have shown that</p>
<div class="callout callout-style-simple callout-tip no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><img src="https://latex.codecogs.com/png.latex?%0AU%20%5Cin%20I_j%20%5Ciff%20%5Csum_%7Bi%20=%201%7D%5Ek%20%5Cfrac%7BB_i%7D%7B2%5Ei%7D%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D%0A"></p>
</div>
</div>
</div>
</section>
<section id="probability-of-landing-in-i_j" class="level4">
<h4 class="anchored" data-anchor-id="probability-of-landing-in-i_j">Probability of landing in <img src="https://latex.codecogs.com/png.latex?I_j"></h4>
<p>Now we can calculate the probability that <img src="https://latex.codecogs.com/png.latex?U"> lands in a specific dyadic interval <img src="https://latex.codecogs.com/png.latex?I_j">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BU%20%5Cin%20I_j%5D%20&amp;=%20%5Cmathbb%7BP%7D%5Cleft%5B%5Csum_%7Bi=1%7D%5E%7Bk%7D%20%5Cfrac%7BB_i%7D%7B2%5E%7Bi%7D%7D%20=%20%5Cfrac%7Bj%7D%7B2%5Ek%7D%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cmathbb%7BP%7D%5BB_1%20=%20b_1,%20B_2%20=%20b_2,%20%5Cdots,%20B_k%20=%20b_k%5D%0A%5Cend%7Balign*%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?(b_1,%20%5Cdots,%20b_k)"> is the binary representation of <img src="https://latex.codecogs.com/png.latex?j">.</p>
<p>Since the bits are independent and each has probability <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BU%20%5Cin%20I_j%5D%20&amp;=%20%5Cprod_%7Bi=1%7D%5E%7Bk%7D%20%5Cmathbb%7BP%7D%5BB_i%20=%20b_i%5D%20%5C%5C%0A&amp;=%20%5Cprod_%7Bi=1%7D%5E%7Bk%7D%20%5Cfrac%7B1%7D%7B2%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%5Ek%7D%0A%5Cend%7Balign*%7D"></p>
<p>Note that this is exactly the length of the interval <img src="https://latex.codecogs.com/png.latex?I_j">! Therefore, we have show that at least for dyadic intervals, <img src="https://latex.codecogs.com/png.latex?U"> is uniformly distributed!</p>
</section>
<section id="extending-to-general-intervals" class="level4">
<h4 class="anchored" data-anchor-id="extending-to-general-intervals">Extending to General Intervals</h4>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Technical Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Extending this to the general interval <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b%5D"> is complicated and requires careful machinery taught usually in a course in real analysis. Therefore, will handwave a lot of the technicalities, but do keep in mind that there is more careful argument to be made here.</p>
</div>
</div>
</div>
<p>For a general interval <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b)%20%5Csubseteq%20%5B0,%201%5D">, we approximate it using dyadic intervals. Let <img src="https://latex.codecogs.com/png.latex?J_k"> be the set of indices where <img src="https://latex.codecogs.com/png.latex?I_j%20%5Csubseteq%20%5Ba,%20b)">. Then:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ba,%20b)%20%5Capprox%20%5Cbigcup_%7Bj%20%5Cin%20J_k%7D%20I_j%0A"></p>
<p>As <img src="https://latex.codecogs.com/png.latex?k"> increases, the dyadic intervals become finer, and this approximation improves.</p>
<p>Since the dyadic intervals are disjoint:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5Cleft%5BU%20%5Cin%20%5Cbigcup_%7Bj%20%5Cin%20J_k%7D%20I_j%5Cright%5D%20&amp;=%20%5Csum_%7Bj%20%5Cin%20J_k%7D%20%5Cmathbb%7BP%7D%5BU%20%5Cin%20I_j%5D%20%5C%5C%0A&amp;=%20%5Csum_%7Bj%20%5Cin%20J_k%7D%20%5Cfrac%7B1%7D%7B2%5Ek%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B%7CJ_k%7C%7D%7B2%5Ek%7D%0A%5Cend%7Balign*%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%7CJ_k%7C"> is the number of dyadic intervals that fit inside <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b)">.</p>
</section>
<section id="taking-the-limit" class="level4">
<h4 class="anchored" data-anchor-id="taking-the-limit">Taking the Limit</h4>
<p>Notice that <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%7CJ_k%7C%7D%7B2%5Ek%7D"> represents the total length of all dyadic intervals contained in <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%7CJ_k%7C%7D%7B2%5Ek%7D%20=%20%5Csum_%7Bj%20%5Cin%20J_k%7D%20%5Ctext%7Blength%7D(I_j)%0A"></p>
<p>As <img src="https://latex.codecogs.com/png.latex?k%20%5Cto%20%5Cinfty">, these intervals cover <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b)"> with increasing precision, so we expect</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clim_%7Bk%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B%7CJ_k%7C%7D%7B2%5Ek%7D%20=%20b%20-%20a%0A"></p>
<p>Therefore:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5BU%20%5Cin%20%5Ba,%20b)%5D%20&amp;=%20%5Clim_%7Bk%20%5Cto%20%5Cinfty%7D%20%5Cmathbb%7BP%7D%5Cleft%5BU%20%5Cin%20%5Cbigcup_%7Bj%20%5Cin%20J_k%7D%20I_j%5Cright%5D%20%5C%5C%0A&amp;=%20%5Clim_%7Bk%20%5Cto%20%5Cinfty%7D%20%5Cfrac%7B%7CJ_k%7C%7D%7B2%5Ek%7D%20%5C%5C%0A&amp;=%20b%20-%20a%20%5C%5C%0A&amp;=%20%5Ctext%7Blength%7D(%5Ba,%20b))%0A%5Cend%7Balign*%7D"></p>
<p>as desired</p>
</section>
<section id="from-theory-to-practice-finite-bit-algorithms" class="level4">
<h4 class="anchored" data-anchor-id="from-theory-to-practice-finite-bit-algorithms">From Theory to Practice: Finite Bit Algorithms</h4>
<p>The theoretical result above is beautiful, but it has a glaring practical issue: we cannot generate an infinite sequence of bits! In the real world, we need to work with a finite number of bits. Fortunately, our proof gives us a natural way to approximate the uniform distribution using only a finite number of bits.</p>
<p>The key insight is that after generating <img src="https://latex.codecogs.com/png.latex?N"> bits, we’ve already determined which of the <img src="https://latex.codecogs.com/png.latex?2%5EN"> dyadic intervals our value falls into. The remaining (ungenerated) bits would only refine our position within that interval, which has width <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%5EN%7D">. For large <img src="https://latex.codecogs.com/png.latex?N">, this interval is so small that the difference is negligible</p>
<p>This leads us to the following algorithm:</p>
<div class="algorithm">
<p><strong>Algorithm 3</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?U%20%5Cgets%200"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?i%20=%201"> to <img src="https://latex.codecogs.com/png.latex?N"> do</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> Generate random bit <img src="https://latex.codecogs.com/png.latex?B_i"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> <img src="https://latex.codecogs.com/png.latex?U%20%5Cgets%20U%20+%20B_i%20%5Ccdot%202%5E%7B-i%7D"></p>
<p>return <img src="https://latex.codecogs.com/png.latex?U"></p>
</div>
<p>This algorithm generates values of the form <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bk%7D%7B2%5EN%7D"> where <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20%5C%7B0,%201,%20%5Cldots,%202%5EN%20-%201%5C%7D">. In other words, it produces a <em>discrete</em> uniform distribution over <img src="https://latex.codecogs.com/png.latex?2%5EN"> equally-spaced points in <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">, rather than a truly continuous uniform distribution.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?U_N"> denote the output of Algorithm 3 and let <img src="https://latex.codecogs.com/png.latex?U_%7B%5Ctext%7Btrue%7D%7D"> denote a truly uniform random variable on <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">. How far off is our approximation? Well the maximum distance between any generated value and its “true” counterpart is at most the granularity of our discretization: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmax_%7Bx%20%5Cin%20%5B0,%201%5D%7D%20%7CU_N%20-%20x%7C%20%5Cleq%20%5Cfrac%7B1%7D%7B2%5EN%7D%0A"> So, if <img src="https://latex.codecogs.com/png.latex?N%20=%2064">, then we have: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cfrac%7B1%7D%7B2%5E%7B64%7D%7D%20&amp;%5Capprox%205.42%20%5Ctimes%2010%5E%7B-20%7D%20%5C%5C%0A%5Cend%7Balign*%7D"></p>
<p>This means our approximation error is about <img src="https://latex.codecogs.com/png.latex?10%5E%7B-20%7D">! For all practical purposes, this is indistinguishable from a true uniform distribution. In fact, 64-bit floating point numbers (the standard in most programming languages) have a precision of about <img src="https://latex.codecogs.com/png.latex?10%5E%7B-16%7D">, which is less precise than our sampling error.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Practical Implementation
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practice, most programming languages use 64-bit floating point numbers for representing real numbers. Algorithm 3 with <img src="https://latex.codecogs.com/png.latex?N%20=%2064"> bits produces values that are uniformly distributed on the set of representable floating point numbers in <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">, which is the best we can do given the finite precision of computer arithmetic.</p>
</div>
</div>
<p>Therefore, Algorithm 3 with <img src="https://latex.codecogs.com/png.latex?N%20=%2064"> bits gives us an excellent practical algorithm for sampling (approximately) uniform real numbers in <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">.</p>
</section>
</section>
</section>
<section id="arbitrary-probability-distribution" class="level2">
<h2 class="anchored" data-anchor-id="arbitrary-probability-distribution">Arbitrary Probability Distribution</h2>
<p>That was a lot of work! If it took so much effort just to sample from the uniform distribution, how can we possibly expect to come up with algorithms for complicated probability distributions? Well, it turns out that with not too much extra work we can sample from any arbitrary distribution <img src="https://latex.codecogs.com/png.latex?%5Cpi">. More formally, lets say <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a probability distribution on a finite state space <img src="https://latex.codecogs.com/png.latex?%5Cchi%20=%20%5C%7Bx_1,%20%5Ccdots%20x_n%5C%7D">. I want to generate an <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cchi"> such that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BX%20=%20x%5D%20=%20%5Cpi(x)%0A"> Now that we can sample from the uniform distribution on <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">, let’s use that. We will use another clever partitioning technique, similar to what we did for proving the uniform distribution result.</p>
<p>More specifically, we divide the interval <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> into <img src="https://latex.codecogs.com/png.latex?N"> consecutive segments. For each <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5C%7B1,%202,%20%5Cldots,%20N%5C%7D">, define segment <img src="https://latex.codecogs.com/png.latex?i"> as: <img src="https://latex.codecogs.com/png.latex?%0AS_i%20=%20%5Cleft%5B%5Csum_%7Bj=1%7D%5E%7Bi-1%7D%20%5Cpi(x_j),%20%5Csum_%7Bj=1%7D%5E%7Bi%7D%20%5Cpi(x_j)%5Cright)%0A"> where by convention, <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bj=1%7D%5E%7B0%7D%20%5Cpi(x_j)%20=%200">.</p>
<p>Observe that segment <img src="https://latex.codecogs.com/png.latex?S_i"> has length: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Blength%7D(S_i)%20=%20%5Csum_%7Bj=1%7D%5E%7Bi%7D%20%5Cpi(x_j)%20-%20%5Csum_%7Bj=1%7D%5E%7Bi-1%7D%20%5Cpi(x_j)%20=%20%5Cpi(x_i)%0A"></p>
<p>Moreover, these segments partition <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> since they are disjoint and: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbigcup_%7Bi=1%7D%5E%7BN%7D%20S_i%20=%20%5B0,%201)%0A"> which follows from the fact that <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cpi(x_i)%20=%201"> (since <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is a probability distribution). When we sample uniformly from <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">, the probability of landing in segment <img src="https://latex.codecogs.com/png.latex?i"> is exactly <img src="https://latex.codecogs.com/png.latex?%5Cpi(x_i)"> (since the segment has length <img src="https://latex.codecogs.com/png.latex?%5Cpi(x_i))."> So if we return <img src="https://latex.codecogs.com/png.latex?x_i"> whenever we land in segment <img src="https://latex.codecogs.com/png.latex?i">, we get the desired distribution!</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Intuition
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think of it like a dartboard: if you throw a dart uniformly at random on <img src="https://latex.codecogs.com/png.latex?%5B0,1%5D">, the probability of hitting a region is proportional to its length. By making segment <img src="https://latex.codecogs.com/png.latex?i"> have length <img src="https://latex.codecogs.com/png.latex?%5Cpi(x_i)">, we ensure the probability of hitting it matches our target probability.</p>
</div>
</div>
<p><em>Example:</em> Suppose <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BX%7D%20=%20%5C%7Bx_1,%20x_2,%20x_3,%20x_4,%20x_5%5C%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20(0.15,%200.25,%200.20,%200.30,%200.10)">. We segment the <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> interval as shown below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://csuraparaju.github.io/posts/sampling/segmented_numberline.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption>Segmented [0, 1] Interval</figcaption>
</figure>
</div>
<p>Now, we can sample a real number uniformly random, and return the segment that this number belongs to.</p>
<section id="formalizing-the-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="formalizing-the-algorithm">Formalizing the Algorithm</h4>
<p>To implement this efficiently, we use cumulative probabilities. Define: <img src="https://latex.codecogs.com/png.latex?%0AF_i%20=%20%5Csum_%7Bj=1%7D%5E%7Bi%7D%20%5Cpi(x_j)%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i%20=%201,%202,%20%5Cldots,%20N%0A"> with <img src="https://latex.codecogs.com/png.latex?F_0%20=%200">. Note that <img src="https://latex.codecogs.com/png.latex?F_i"> represents the cumulative probability up to and including <img src="https://latex.codecogs.com/png.latex?x_i">, so segment <img src="https://latex.codecogs.com/png.latex?i"> corresponds to the interval <img src="https://latex.codecogs.com/png.latex?%5BF_%7Bi-1%7D,%20F_i)">.</p>
<p>Our algorithm returns <img src="https://latex.codecogs.com/png.latex?x_i"> when <img src="https://latex.codecogs.com/png.latex?U"> falls in the interval <img src="https://latex.codecogs.com/png.latex?%5BF_%7Bi-1%7D,%20F_i)">:</p>
<div class="algorithm">
<p><strong>Algorithm 4</strong></p>
<p><strong>Preprocessing:</strong></p>
<p>Compute cumulative probabilities: <img src="https://latex.codecogs.com/png.latex?F_0%20%5Cgets%200"></p>
<p>for <img src="https://latex.codecogs.com/png.latex?i%20=%201"> to <img src="https://latex.codecogs.com/png.latex?N"> do</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> <img src="https://latex.codecogs.com/png.latex?F_i%20%5Cgets%20F_%7Bi-1%7D%20+%20%5Cpi(x_i)"></p>
<p><strong>Sampling:</strong></p>
<p>Generate <img src="https://latex.codecogs.com/png.latex?U%20%5Csim%20%5Ctext%7BUniform%7D%5B0,%201%5D"> using Algorithm 3</p>
<p>for <img src="https://latex.codecogs.com/png.latex?i%20=%201"> to <img src="https://latex.codecogs.com/png.latex?N"> do</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad"> if <img src="https://latex.codecogs.com/png.latex?F_%7Bi-1%7D%20%5Cleq%20U%20%3C%20F_i"> then</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cquad%5Cquad"> return <img src="https://latex.codecogs.com/png.latex?x_i"></p>
</div>
<p>Let’s verify that this algorithm produces the correct distribution. For any <img src="https://latex.codecogs.com/png.latex?x_i%20%5Cin%20%5Cmathcal%7BX%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cmathbb%7BP%7D%5B%5Ctext%7Boutput%7D%20=%20x_i%5D%20&amp;=%20%5Cmathbb%7BP%7D%5BF_%7Bi-1%7D%20%5Cleq%20U%20%3C%20F_i%5D%20%5C%5C%0A&amp;=%20F_i%20-%20F_%7Bi-1%7D%20%5Ctag%7B$U$%20is%20uniform%20on%20$%5B0,1%5D$%7D%20%5C%5C%0A&amp;=%20%5Cleft(%5Csum_%7Bj=1%7D%5E%7Bi%7D%20%5Cpi(x_j)%5Cright)%20-%20%5Cleft(%5Csum_%7Bj=1%7D%5E%7Bi-1%7D%20%5Cpi(x_j)%5Cright)%20%5C%5C%0A&amp;=%20%5Cpi(x_i)%0A%5Cend%7Balign*%7D"></p>
<p>The second equality uses the fact that for a uniform random variable <img src="https://latex.codecogs.com/png.latex?U"> on <img src="https://latex.codecogs.com/png.latex?%5B0,1%5D">, we have <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%5Ba%20%5Cleq%20U%20%3C%20b%5D%20=%20b%20-%20a">. Therefore, our algorithm produces samples distributed according to <img src="https://latex.codecogs.com/png.latex?%5Cpi">. Problem solved!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Computational Complexity
</div>
</div>
<div class="callout-body-container callout-body">
<p>The preprocessing step takes <img src="https://latex.codecogs.com/png.latex?O(N)"> time to compute cumulative probabilities. Each sample then requires <img src="https://latex.codecogs.com/png.latex?O(N)"> time in the worst case to find which segment <img src="https://latex.codecogs.com/png.latex?U"> falls into (via linear search). This can be improved to <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20N)"> per sample using binary search, since the cumulative probabilities <img src="https://latex.codecogs.com/png.latex?F_i"> are sorted.</p>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>math</category>
  <category>cs</category>
  <guid>https://csuraparaju.github.io/posts/sampling/</guid>
  <pubDate>Wed, 08 Oct 2025 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Automatic Differentiation using Dual Numbers</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/dual-numbers/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>I recently came across a curious algebraic structure called the Dual Number system, denoted as <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%20(%5Cepsilon)">. The dual numbers are an extension of the real numbers, with a very interesting property: evaluating a differentiable function <img src="https://latex.codecogs.com/png.latex?f"> at a dual number <img src="https://latex.codecogs.com/png.latex?x"> will give us both <img src="https://latex.codecogs.com/png.latex?f(x)"> and <img src="https://latex.codecogs.com/png.latex?f'(x)"> at the same time. This is a powerful idea in numerical analysis and machine learning, as it allows for efficient computation of derivatives.</p>
</section>
<section id="dual-numbers" class="level2">
<h2 class="anchored" data-anchor-id="dual-numbers">Dual Numbers</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Cneq%200"> be a new “number” such that <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5E2%20=%200"> <span class="citation" data-cites="Penn2022">(Penn 2022)</span>. Before we go any further, we must first ask ourselves if such a number can even exist. The answer is of course not if you want to work in complete fields like <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D">. No matter how small a real (or complex) number is, as long as it is non-zero, its square will also be non-zero. So why do we care about this number then?</p>
<p>One reason is that it can help us model the notion of an “infinitesimal” from calculus, which can be thought of as a non-zero number that is “smaller” than any other number. Using the definition above then, we note that even though <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is non-zero, its square is zero, and so the number is somehow “smaller” than any other real (or complex) number. In order to study this more, let’s assume that such a number actually exists and try to see if any laws we know about <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D"> does (or does not) break down. Then, let’s see how it can be used to compute derivatives of a function.</p>
<p>The structure we want to study is defined as follows <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BR%7D%20(%5Cepsilon)%20=%20%5C%7B%20a%20+%20b%20%5Cepsilon%20%5Cmid%20a,%20b%20%5Cin%20%5Cmathbb%7BR%7D%20%5C%7D%0A"></p>
</section>
<section id="basic-arithmetic" class="level2">
<h2 class="anchored" data-anchor-id="basic-arithmetic">Basic Arithmetic</h2>
<p>First, let’s define how to add, subtract, and multiply. Given a dual number <img src="https://latex.codecogs.com/png.latex?x%20=%20a%20+%20b%20%5Cepsilon"> and <img src="https://latex.codecogs.com/png.latex?y%20=%20c%20+%20d%20%5Cepsilon">, define: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Ax%20+%20y%20&amp;:=%20(a%20+%20c)%20+%20(b%20+%20d)%20%5Cepsilon%20%5C%5C%0Ax%20-%20y%20&amp;:=%20(a%20-%20c)%20+%20(b%20-%20d)%20%5Cepsilon%20%5C%5C%0Ax%20%5Ccdot%20y%20&amp;:=%20(a%20%5Ccdot%20c)%20+%20(a%20%5Ccdot%20d%20+%20d%20%5Ccdot%20c)%20%5Cepsilon%20%5C%5C%0A%5Cend%7Balign*%7D"></p>
<p>Whenever we create new objects and define operations on the objects, we should always check that they are well defined. That means, for each operation above we need to check that if <img src="https://latex.codecogs.com/png.latex?x%20=%20y"> and <img src="https://latex.codecogs.com/png.latex?z"> is any other dual number then <img src="https://latex.codecogs.com/png.latex?x%20+%20z%20=%20y%20+%20z"> and <img src="https://latex.codecogs.com/png.latex?x%20-%20z%20=%20y%20-%20z"> and <img src="https://latex.codecogs.com/png.latex?x%20%5Ccdot%20z%20=%20y%20%5Ccdot%20z">. In this case, checking this is very easy so I won’t do that. We can also divide <img src="https://latex.codecogs.com/png.latex?x%20/%20y">, given that <img src="https://latex.codecogs.com/png.latex?c%20%5Cneq%200">: <img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7Bx%7D%7By%7D%20:=%20%5Cfrac%7Ba%7D%7Bc%7D%20+%20%5Cfrac%7Bb%20%5Ccdot%20c%20-%20a%20%5Ccdot%20d%7D%7Bc%5E2%7D%0A"> Hopefully it is not hard to see that distributivity, associativity, and commutativity follow because we are simply using <img src="https://latex.codecogs.com/png.latex?+,%20-,%20%5Ccdot"> from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D">. This means all the nice properties we learned in high school about algebra on real numbers (ex. FOIL) still hold. So far so good.</p>
<p>However, things get interesting when we starting playing with multiplication and division operator a bit more. Specifically, what happens when we multiply two numbers <img src="https://latex.codecogs.com/png.latex?0%20+%20b%20%5Cepsilon"> and <img src="https://latex.codecogs.com/png.latex?0%20+%20d%20%5Cepsilon"> where <img src="https://latex.codecogs.com/png.latex?b,%20d%20%5Cneq%200">? Clearly these are non-zero numbers. However, note that <img src="https://latex.codecogs.com/png.latex?%0A(0%20+%20b%5Cepsilon)%20+%20(0%20+%20d%5Cepsilon)%20=%20(0%20%5Ccdot%200)%20+%20(0%20%5Ccdot%20d%20+%20b%20%5Ccdot%200)%5Cepsilon%20=%200%0A"> This s strange! Recall that in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BC%7D"> there are no zero divisors. That is, if <img src="https://latex.codecogs.com/png.latex?a,%20b%20%5Cin%20%5Cmathbb%7BR%7D%20(%5Ctext%7Bor%20%7D%20%5Cmathbb%7BC%7D)">, and <img src="https://latex.codecogs.com/png.latex?a%20%5Ccdot%20b%20=%200"> then either <img src="https://latex.codecogs.com/png.latex?a%20=%200"> or <img src="https://latex.codecogs.com/png.latex?b%20=%200">. We just saw that this need not be the case for dual numbers anymore! Finally, a place where dual number arithmetic breaks down. Nonetheless, it is still remarkable that most of the basic arithmetic operations and laws from <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D"> still hold, so let’s see what kind of algebra we can do on these numbers.</p>
</section>
<section id="polynomials" class="level2">
<h2 class="anchored" data-anchor-id="polynomials">Polynomials</h2>
<p>Let’s consider a simple polynomial function over the real numbers <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20x%5E2">. We can extend this function to accept dual numbers as input by replacing the real number <img src="https://latex.codecogs.com/png.latex?x%20=%20a"> with the dual number <img src="https://latex.codecogs.com/png.latex?x%20=%20a%20+%20%5Cepsilon">. <img src="https://latex.codecogs.com/png.latex?%0Af((a%20+%20%5Cepsilon))%20=%20(a+%5Cepsilon)%5Ccdot%20(a%20+%20%5Cepsilon)%20=%20a%5E2%20+%20(a%20+%20a)%20%5Cepsilon%20=%20a%5E2%20+%202%20a%20%5Cepsilon%0A"> Notice what just happened. The term <img src="https://latex.codecogs.com/png.latex?2%20a%20%5Cepsilon"> is the derivative of the function <img src="https://latex.codecogs.com/png.latex?f"> evaluated at <img src="https://latex.codecogs.com/png.latex?x%20=%20a">. By just computing the function <img src="https://latex.codecogs.com/png.latex?f"> at the modified dual number, we were able to get the derivative as well in one computation.</p>
<p>Let’s use the Binomial Theorem to see if this generalizes to <img src="https://latex.codecogs.com/png.latex?f(a%20+%20%5Cepsilon)%20=%20(a+%5Cepsilon)%5En"> for an arbitrary <img src="https://latex.codecogs.com/png.latex?n">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Af(a%20+%20%5Cepsilon)%20&amp;=%20(a%20+%20%5Cepsilon)%5En%20%5C%5C%0A&amp;=%20%5Csum_%7Bi%20=%200%7D%5En%20%7Bn%20%5Cchoose%20i%7D%20a%5E%7Bn%20-%20i%7D%20%5Cepsilon%5Ei%20%5C%5C%0A&amp;=%20%7Bn%20%5Cchoose%200%7D%20a%5En%20%5Cepsilon%5E0%20+%20%7Bn%20%5Cchoose%201%7D%20a%5E%7Bn-1%7D%20%5Cepsilon%5E1%20+%20%5Csum_%7Bi%20=%202%7D%5En%20%7Bn%20%5Cchoose%20i%7D%20a%5E%7Bn%20-%20i%7D%20%5Cepsilon%5Ei%20%5C%5C%0A&amp;=%20a%5En%20+%20n%20%5Ccdot%20a%5E%7Bn-1%7D%20%5Cepsilon%0A%5Cend%7Balign*%7D"> Where the last step follows because <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5E2%20=%200"> (by definition) and so all the other terms with <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%5Ei"> for <img src="https://latex.codecogs.com/png.latex?i%20%5Cgeq%202"> cancels out. Notice that this is exactly what we wanted to see: <img src="https://latex.codecogs.com/png.latex?f(a+%5Cepsilon)%20=%20f(a)%20+%20f'(a)%20%5Cepsilon"></p>
<p>Now, lets go even futher and see if this notion generalizes to an arbitrary polynomial <img src="https://latex.codecogs.com/png.latex?f(a%20+%20%5Cepsilon)%20=%20p_n%20(a%20+%20%5Cepsilon)%5En%20+%20p_%7Bn-1%7D%20(a%20+%20%5Cepsilon)%5E%7Bn-1%7D%20+%20%5Ccdots%20+%20p_1%20(a%20+%20%5Cepsilon)%20+%20p_0">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Af(a%20+%20%5Cepsilon)%20&amp;=%20p_n%20(a%20+%20%5Cepsilon)%5En%20+%20p_%7Bn-1%7D%20(a%20+%20%5Cepsilon)%5E%7Bn-1%7D%20+%20%5Ccdots%20+%20p_1%20(a%20+%20%5Cepsilon)%20+%20p_0%20%5C%5C%0A&amp;=%20p_n%20%5Csum_%7Bi%20=%200%7D%5En%20%7Bn%20%5Cchoose%20i%7D%20a%5E%7Bn%20-%20i%7D%20%5Cepsilon%5Ei%20+%20p_%7Bn-1%7D%20%5Csum_%7Bi%20=%200%7D%5E%7Bn-1%7D%20%7Bn-1%20%5Cchoose%20i%7D%20a%5E%7Bn%20-%201-%20i%7D%20%5Cepsilon%5Ei%20+p_1%20(a%20+%20%5Cepsilon)%20+%20p_0%20%20%5C%5C%0A&amp;=%20p_n%20(a%5En%20+%20n%20%5E%7Bn-1%7D%20%5Cepsilon)%20+%20p_%7Bn-1%7D%20(a%5E%7Bn-1%7D%20+%20(n-1)%20a%5E%7Bn-2%7D%20%5Cepsilon)%20+%20%5Ccdots%20+%20p_1%20(a%20+%20%5Cepsilon)%20+%20p_0%20%5C%5C%0A&amp;=%20p_n%20a%5En%20+%20p_n%20n%20a%5E%7Bn-1%7D%5Cepsilon%20+%20p_%7Bn-1%7Da%5E%7Bn-1%7D%20+%20p_%7Bn-1%7D%20(n-1)a%5E%7Bn-2%7D%5Cepsilon%20+%20%5Ccdots%20+%20p_1%20a%20+%20p_1%20%5Cepsilon%20+%20p_0%20%5C%5C%0A&amp;=%20(p_n%20a%5En%20+%20p_%7Bn-1%7D%20a%5E%7Bn-1%7D%20+%20%5Ccdots%20+%20p_1%20a%20+%20p_0)%20+(p_n%20n%20a%5E%7Bn-1%7D%20+%20p_%7Bn-1%7D%20(n-1)%20a%5E%7Bn-2%7D%20+%20%5Ccdots%20+%20p_1)%20%5Cepsilon%20%5C%5C%0A&amp;=%20f(a)%20+%20f'(a)%20%5Cepsilon%0A%5Cend%7Balign*%7D"> and indeed it does.</p>
</section>
<section id="other-functions" class="level2">
<h2 class="anchored" data-anchor-id="other-functions">Other functions</h2>
<p>Maybe we can actually make a stronger claim: does this property holds for all infinitely differentiable functions? So, let <img src="https://latex.codecogs.com/png.latex?f"> be such a function evaluated at the dual number <img src="https://latex.codecogs.com/png.latex?a%20+%20c%20%5Cepsilon"> (note that the coefficient for the dual part need not be 1 anymore). Then we can use the taylor series expansion of <img src="https://latex.codecogs.com/png.latex?f"> to write <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Af(a%20+%20c%5Cepsilon)%20&amp;=%20%5Csum_%7Bn%20=%200%7D%5E%5Cinfty%20%5Cfrac%7Bf%5E%7B(n)%7D%20(a)%7D%7Bn!%7D%20((a%20+%20c%5Cepsilon)%20-%20a)%5En%20%5C%5C%0A&amp;=%20%5Csum_%7Bn%20=%200%7D%5E%5Cinfty%20%5Cfrac%7Bf%5E%7B(n)%7D%20(a)%7D%7Bn!%7D%20(c%5Cepsilon)%5En%20%5C%5C%0A&amp;=%20%5Cfrac%7Bf%5E%7B(0)%7D%20(a)%7D%7B0!%7D%20(c%5Cepsilon)%5E0%20+%20%5Cfrac%7Bf%5E%7B(1)%7D%20(a)%7D%7B1!%7D%20(c%5Cepsilon)%5E1%20+%20%5Csum_%7Bn%20=%202%7D%5E%5Cinfty%20%5Cfrac%7Bf%5E%7B(n)%7D%20(a)%7D%7Bn!%7D%20(c%5Cepsilon)%5En%20%5C%5C%0A&amp;%20f(a)%20+%20f'(a)%20c%20%5Cepsilon%0A%5Cend%7Balign*%7D"></p>
<p>and again, it does!</p>
<p>But what about composition of functions? Let <img src="https://latex.codecogs.com/png.latex?f,%20g"> be functions extended to dual numbers such that <img src="https://latex.codecogs.com/png.latex?f(a%20+%20c%20%5Cepsilon)%20=%20f(a)%20+%20f'(a)%20c%20%5Cepsilon"> and <img src="https://latex.codecogs.com/png.latex?g(a%20+%20c%20%5Cepsilon)%20=%20g(a)%20+%20g'(a)%20c%20%5Cepsilon">. Then, note that: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0Af(g(a+%5Cepsilon))%20&amp;=%20f(g(a)%20+%20g'(a)%5Cepsilon)%20%5C%5C%0A&amp;=%20f(g(a))%20+%20f'(g(a))%20%5Ccdot%20g'(a)%20%5Cepsilon%20%5C%5C%0A%5Cend%7Balign*%7D"> and notice that the dual component <img src="https://latex.codecogs.com/png.latex?f'(g(a))%20%5Ccdot%20g'(a)"> is exactly the chain rule of derivatives! So, now we can imagine all sorts of complicated functions like <img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%20%5Csin(e%5E%7Bx%5E2%20+%201%7D)%0A"> and by using the dual number representation, we can compute both the function value and its derivative in a single forward pass through the function.</p>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing thoughts</h2>
<p>What I find most beautiful about dual numbers is how they transform the problem of differentiation from a difficult limiting process <img src="https://latex.codecogs.com/png.latex?%0Af'(x)%20=%20%5Clim_%7Bh%20%5Cto%200%7D%20%5Cfrac%7Bf(x+h)%20-%20f(x)%7D%7Bh%7D%0A"> into a problem of pure algebraic manipulation. This is incredibly beautiful because limits are (arguably) one of the ugliest tools in mathematics, and turning it into a formulation of abstract algebra is extremely satisfying.</p>
<p>While we gained zero divisors in dual numbers, what we got in return makes up for it: a computational method for computing derivatives that is exact (no truncation error like finite differences) and efficient (requires roughly the same number of operations as computing <img src="https://latex.codecogs.com/png.latex?f"> itself). This is why dual numbers, despite being a somewhat obscure mathematical structure, power automatic differentiation systems used in training neural networks.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Penn2022" class="csl-entry">
Penn, Michael. 2022. <span>“The Strange Cousin of the Complex Numbers – the Dual Numbers.”</span>
</div>
</div></section></div> ]]></description>
  <category>math</category>
  <guid>https://csuraparaju.github.io/posts/dual-numbers/</guid>
  <pubDate>Tue, 08 Apr 2025 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Sending Messages over Noisy Channels</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/noisy-channels/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>A fundamental problem in electrical engineering is sending messages over noisy channels. For example, all messages sent over the internet have to eventually be transmitted over physical channels (copper wires or fiber optic cables), and these channels can be damaged by the elements (eg. <a href="https://www.business-reporter.co.uk/management/sharks-ate-our-data">sharks</a>)</p>
<p>I recently came across a method to reduce this problem to a purely mathematical one: the sphere packing problem, which aims to find the densest packing of non-overlapping <img src="https://latex.codecogs.com/png.latex?n"> dimensional spheres in a given space.</p>
</section>
<section id="signals-and-code" class="level2">
<h2 class="anchored" data-anchor-id="signals-and-code">Signals and Code</h2>
<p>But first, let’s define the noisy channel problem more formally. Let <img src="https://latex.codecogs.com/png.latex?T%20%3E%200"> be a fixed length of time corresponding to the length of a signal transmission.</p>
<p>A signal is a continuous function <img src="https://latex.codecogs.com/png.latex?s%20:%20%5B0,%20T%5D%20%5Cto%20%5Cmathbb%7BR%7D,"> where <img src="https://latex.codecogs.com/png.latex?s(t)"> is the amplitude of the signal at time <img src="https://latex.codecogs.com/png.latex?t">, and the frequencies do not surpass some fixed limit <img src="https://latex.codecogs.com/png.latex?W">. Think of <img src="https://latex.codecogs.com/png.latex?s(t)"> as the voltage of the signal at time <img src="https://latex.codecogs.com/png.latex?t"> (for copper wires), or the intensity of a light signal at time <img src="https://latex.codecogs.com/png.latex?t"> (for fiber optic cables).</p>
<p>A code is a finite set of signals <img src="https://latex.codecogs.com/png.latex?%5C%7Bs_1,%20s_2,%20%5Cldots,%20s_n%5C%7D."> This can be thought of as a symbolic alphabet for two computers to communicate over a channel. A simple example of a code is <img src="https://latex.codecogs.com/png.latex?%0A%5C%7B%20s_1(t)%20=%201,%5C;%20s_2(t)%20=%20-1%20%5C%7D,%0A"> which can be used to send binary messages over a channel.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Shannon–Nyquist Sampling Theorem</a> states that any signal <img src="https://latex.codecogs.com/png.latex?s(t)"> with frequencies less than <img src="https://latex.codecogs.com/png.latex?W"> can be uniquely represented by a finite set of samples <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5C%7B%20s(0),%20s%5C!%5Cleft(%5Ctfrac%7B1%7D%7BW%7D%5Cright),%20s%5C!%5Cleft(%5Ctfrac%7B1%7D%7B2W%7D%5Cright),%20%5Cldots,%20s%5C!%5Cleft(%5Ctfrac%7Bn-1%7D%7B2W%7D%5Cright)%20%5Cright%5C%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?n%20=%202WT">. This means that we can represent any signal <img src="https://latex.codecogs.com/png.latex?s(t)"> as a vector <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D%20%5Cin%20%5Cmathbb%7BR%7D%5En,"> and any code as a finite subset <img src="https://latex.codecogs.com/png.latex?C%20%5Csubseteq%20%5Cmathbb%7BR%7D%5En">, where each element of <img src="https://latex.codecogs.com/png.latex?C"> represents a signal.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="sampling.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="https://csuraparaju.github.io/posts/noisy-channels/sampling.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></a></p>
</figure>
</div>
<p>The continuous signal above <img src="https://latex.codecogs.com/png.latex?S(t)"> represented by the discrete samples <img src="https://latex.codecogs.com/png.latex?S_i">. Therefore, we will represent a signal as a vector in the remaining discussion.</p>
</section>
<section id="the-noisy-channel" class="level2">
<h2 class="anchored" data-anchor-id="the-noisy-channel">The Noisy Channel</h2>
<p>In the real world, the sent signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D"> is almost never the same as the received signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D">. This is because the channel introduces noise, which we model as a random perturbation in the input.</p>
<p>Formally, we assume that the received signal is <img src="https://latex.codecogs.com/png.latex?%0A%5Cvec%7Br%7D%20=%20%5Cvec%7Bs%7D%20+%20%5Cvec%7Bz%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bz%7D"> is a random vector with Gaussian entries (each <img src="https://latex.codecogs.com/png.latex?z_i%20%5Csim%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2)"> and is i.i.d). If the receiver wants to determine which signal was sent, a natural decoding strategy is nearest-neighbor decoding: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cvec%7Bs%7D%7D%20=%20%5Cunderset%7B%5Cvec%7Bs_i%7D%20%5Cin%20C%7D%7B%5Ctext%7Bargmin%7D%7D%20%5C%7C%20%5Cvec%7Br%7D%20-%20%5Cvec%7Bs_i%7D%20%5C%7C_2.%0A"></p>
<p>However, if two signals in the code are too close together, noise may make decoding ambiguous. For example, in the following situation:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="close_signals.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="https://csuraparaju.github.io/posts/noisy-channels/close_signals.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
<p>which signal does <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D"> correspond to? Both <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs_1%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs_2%7D"> are equally likely.</p>
<p><br>
</p>
<p>To solve this problem, remember that we assumed <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D%20=%20%5Cvec%7Bs%7D%20+%20%5Cvec%7Bz%7D"> and that each <img src="https://latex.codecogs.com/png.latex?z_i%20%5Csim%20%5Cmathcal%7BN%7D(0,%20%5Csigma%5E2)">. A fundamental property of the Gaussian distribution is that <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5B-3%20%5Csigma%20%5Cleq%20z_i%20%5Cleq%203%20%5Csigma%5D%20%5Capprox%2099.7%5C%25%0A"> So, we can use this to figure out a bound on the distance between <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D"> with high probability. In the worst-case, each component is such that <img src="https://latex.codecogs.com/png.latex?%7Cz_i%7C%20=%203%20%5Csigma">. Since we have <img src="https://latex.codecogs.com/png.latex?n"> components, this means that <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5C%7C%20%5Cvec%7Br%7D%20-%20%5Cvec%7Bs%7D%20%5C%7C_2%20=%20%20%5C%7C%20%5Cvec%7Bz%7D%5C%7C_2%20&amp;=%20%5Csqrt%7B%5Csum_%7Bi%20=%201%7D%5E%7Bn%7D%20z_i%5E2%7D%20%5C%5C%20&amp;%5Cleq%5Csqrt%7B%5Csum_%7Bi%20=%201%7D%5E%7Bn%7D%20(3%20%5Csigma)%5E2%7D%20%5C%5C%20&amp;%5Cleq%20%5Csqrt%7Bn%20%5Ccdot%209%20%5Csigma%5E2%7D%20%5C%5C%20&amp;%5Cleq%203%20%5Csigma%20%5Csqrt%7Bn%7D%0A%5Cend%7Balign*%7D"> with probability <img src="https://latex.codecogs.com/png.latex?99.7%5C%25">. This means that the worst-case noise can push the received signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D"> up to a distance of <img src="https://latex.codecogs.com/png.latex?3%20%5Csigma%20%5Csqrt%7Bn%7D"> away from the true signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D">. This creates a sphere of radius <img src="https://latex.codecogs.com/png.latex?3%20%5Csigma%20%5Csqrt%7Bn%7D"> around <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs_1%7D">. To make our decoding as unambiguous as possible, we need to ensure that even when noise pushes <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Br%7D"> to the edge of this sphere, it is still closer to <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D"> than any other signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs_2%7D%20%5Cin%20C">. But signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs_2%7D"> also has a sphere of radius <img src="https://latex.codecogs.com/png.latex?3%20%5Csigma%20%5Csqrt%7Bn%7D"> around it. Therefore, to guarantee that two spheres don’t overlap, the minimum distance between any two signals must be at least <img src="https://latex.codecogs.com/png.latex?6%20%5Csigma%20%5Csqrt%7Bn%7D"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="packed_signal.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="https://csuraparaju.github.io/posts/noisy-channels/packed_signal.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%"></a></p>
</figure>
</div>
</section>
<section id="dense-sphere-packing" class="level2">
<h2 class="anchored" data-anchor-id="dense-sphere-packing">Dense Sphere packing</h2>
<p>Now, you might ask: why don’t we just pick signals that are as far apart as possible? For example, we could place just two signals at opposite ends of our signal space.</p>
<p>But recall that a signal <img src="https://latex.codecogs.com/png.latex?%5Cvec%7Bs%7D"> represents the amplitude or voltage of a message over time. Physicists have told us the power of a signal is proportional to the square of its amplitude: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BPower%7D%20%5Cpropto%20%5C%7C%5Cvec%7Bs%7D%5C%7C_2%5E2%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20s_i%5E2%0A"></p>
<p>In practice, we cannot transmit signals with arbitrarily large power. There are physical limits on how much voltage we can apply to a copper wire and how much light we can send through a fiber optic cable. If we set a maximum power budget <img src="https://latex.codecogs.com/png.latex?P">, then all our signals must satisfy <img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cvec%7Bs%7D%5C%7C_2%5E2%20%5Cleq%20nP%20%5Cquad%20%5CLongrightarrow%20%5Cquad%20%5C%7C%5Cvec%7Bs%7D%5C%7C_2%20%5Cleq%20%5Csqrt%7BnP%7D%0A"></p>
<p>This means all our signals must lie within a sphere of radius <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7BnP%7D"> in <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">, which is a compact region. The more signals we can fit in this region, the more information we can transmit per signal. For example, a code with 2 signals transmits only 1 bit per transmission, but a code with 256 signals transmits 8 bits per transmission.</p>
<p>At the same time, each signal needs a sphere of radius <img src="https://latex.codecogs.com/png.latex?3%5Csigma%5Csqrt%7Bn%7D"> around it to ensure correct decoding with probability <img src="https://latex.codecogs.com/png.latex?99.7%5C%25">. This is exactly the sphere packing problem: what is the maximum number of non-overlapping spheres of radius <img src="https://latex.codecogs.com/png.latex?3%5Csigma%5Csqrt%7Bn%7D"> that we can pack inside a sphere of radius <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7BnP%7D">? Each sphere center represents a signal in our code, and finding the densest packing directly gives us the code that can transmit the most information reliably. Therefore, the problem of designing optimal codes for noisy channels reduces to an entirely abstract problem in math.</p>


</section>

 ]]></description>
  <category>math</category>
  <category>ece</category>
  <guid>https://csuraparaju.github.io/posts/noisy-channels/</guid>
  <pubDate>Wed, 25 Dec 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Metric Spaces, dimension reduction, and Bourgain Embeddings</title>
  <dc:creator>Krish Suraparaju</dc:creator>
  <link>https://csuraparaju.github.io/posts/bourgain-embedding/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Metric embeddings and dimension reduction are important tools in computer science and mathematics for solving the curse of dimensionality. In this post, I want to examine the Bourgain embedding, which shows that any finite metric space can be embedded into an Euclidean metric space space with <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20%E2%81%A1n)"> distortion. While Bourgain embeddings are very impractical (the constant factors in the big - O are huge and the “reduced” dimensions is still high), it is an important result that shows us that no matter how “horrible” the initial metric space is, we can be achieve a distortion factor of <img src="https://latex.codecogs.com/png.latex?O(%5Clog%20n)">.</p>
<p><em>Note</em>: I’ve written this post assuming that the reader has at least taken an undergraduate discrete math course. However, I will still define the important objects we will be using today.</p>
<section id="metric-spaces" class="level3">
<h3 class="anchored" data-anchor-id="metric-spaces">Metric Spaces</h3>
<p>A metric space <img src="https://latex.codecogs.com/png.latex?(X,%20d)"> is a set <img src="https://latex.codecogs.com/png.latex?X"> with a distance function <img src="https://latex.codecogs.com/png.latex?d:%20X%20%5Ctimes%20X%20%5Cto%20%5Cmathbb%7BR%7D">. The distance function satisfies the following properties:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20%5Cgeq%200"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20X"></li>
<li><img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20=%200"> if and only if <img src="https://latex.codecogs.com/png.latex?x%20=%20y"></li>
<li><img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20=%20d(y,%20x)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20X"></li>
<li><img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20%5Cleq%20d(x,%20z)%20+%20d(z,%20y)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y,%20z%20%5Cin%20X"></li>
</ol>
<p>Some examples of metric spaces:</p>
<ol type="1">
<li>A weighted graph <img src="https://latex.codecogs.com/png.latex?G%20=%20(V,%20E)"> with <img src="https://latex.codecogs.com/png.latex?X%20=%20V"> and <img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20="> length of the shortest path between <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">.</li>
<li>The DNA space with <img src="https://latex.codecogs.com/png.latex?X%20=%20%5C%7BA,%20C,%20G,%20T%5C%7D%5En"> and <img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20="> number of positions where <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> differ.</li>
<li>The Euclidean space <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> with <img src="https://latex.codecogs.com/png.latex?X%20=%20%5Cmathbb%7BR%7D%5En"> and <img src="https://latex.codecogs.com/png.latex?d(x,%20y)%20=%20%5C%7Cx%20-%20y%5C%7C_2%20=%20%5Csqrt%7B(x_1%20-%20y_1)%5E2%20+%20%5Ccdots%20+%20(x_n%20-%20y_n)%5E2%7D">.</li>
</ol>
</section>
<section id="maps-embeddings-and-distortions" class="level3">
<h3 class="anchored" data-anchor-id="maps-embeddings-and-distortions">Maps, Embeddings and Distortions</h3>
<p>A map <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Cto%20Y"> between two metric spaces <img src="https://latex.codecogs.com/png.latex?(X,%20d_X)"> and <img src="https://latex.codecogs.com/png.latex?(Y,%20d_Y)"> is called an embedding that maps elements of <img src="https://latex.codecogs.com/png.latex?X"> to elements of <img src="https://latex.codecogs.com/png.latex?Y">.</p>
<p>The embedding is said to be distance-preserving (isometric) if <img src="https://latex.codecogs.com/png.latex?d_Y(f(x),%20f(y))%20=%20d_X(x,%20y)"> for all <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20X">. However, very rarely do we have distance-preserving embeddings between metric spaces. Instead, we often consider embeddings that are “almost” distance-preserving.</p>
<p>An embedding with distortion of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> of a metric space <img src="https://latex.codecogs.com/png.latex?(X,%20d_X)"> into another metric space <img src="https://latex.codecogs.com/png.latex?(Y,%20d_Y)"> is a map <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Cto%20Y"> such that there exists constant <img src="https://latex.codecogs.com/png.latex?r%20%3E%200"> for which <img src="https://latex.codecogs.com/png.latex?r%20%5Ccdot%20d_X(x,%20y)%20%5Cleq%20d_Y(f(x),%20f(y))%20%5Cleq%20%5Calpha%20r%20%5Ccdot%20d_X(x,%20y)%20%5Ctext%7B%20for%20all%20%7D%20x,%20y%20%5Cin%20X"> The distortion of an embedding is the smallest <img src="https://latex.codecogs.com/png.latex?%5Calpha"> for which such a map exists.</p>
<p>Because we are working in a finite set for now, we can equivalently define the distortion in terms of the contraction and expansion. Given a map <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Cto%20Y">, let:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BContraction%7D(f)%20=%20%5Cmax_%7Bx,%20y%20%5Cin%20X%7D%20%5Cfrac%7Bd_Y(f(x),%20f(y))%7D%7Bd_X(x,%20y)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctext%7BExpansion%7D(f)%20=%20%5Cmax_%7Bx,%20y%20%5Cin%20X%7D%20%5Cfrac%7Bd_X(x,%20y)%7D%7Bd_Y(f(x),%20f(y))%7D"></p>
<p>Define the distortion of <img src="https://latex.codecogs.com/png.latex?f"> as <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20%5Ctext%7BExpansion%7D(f)%5Ccdot%5Ctext%7BContraction%7D(f)">.</p>
</section>
</section>
<section id="bourgain-embedding" class="level2">
<h2 class="anchored" data-anchor-id="bourgain-embedding">Bourgain Embedding</h2>
<p>Given an arbitrary finite metric space <img src="https://latex.codecogs.com/png.latex?(X,%20d)"> with <img src="https://latex.codecogs.com/png.latex?n"> points, Bourgain’s theorem says that there exists a map <img src="https://latex.codecogs.com/png.latex?f:%20X%20%5Cto%20%5Cmathbb%7BR%7D%5Ek"> such that the distortion of <img src="https://latex.codecogs.com/png.latex?f"> is <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin%20O(%5Clog%20n)">, and <img src="https://latex.codecogs.com/png.latex?k%20%5Cin%20O(%5Clog%5E2%20n)">. The proof of this theorem is beyond the scope of this post, but it is a constructive proof so there is a natural algorithm that arises from the proof which we can implement.</p>
<p>The Bourgain embedding algorithm is as follows <span class="citation" data-cites="Ye2023">(Ye 2023)</span>:</p>
<ol type="1">
<li><p>Let <img src="https://latex.codecogs.com/png.latex?c"> be a sufficiently large constant, and let <img src="https://latex.codecogs.com/png.latex?%5Clog(n)"> denote the base-2 logarithm of <img src="https://latex.codecogs.com/png.latex?n">.</p></li>
<li><p>For each point <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20X">, define its embedding vector <img src="https://latex.codecogs.com/png.latex?f(x)"> in the following steps:</p>
<ul>
<li>For <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5C%7B1,%202,%20%5Cdots,%20%5Clceil%20%5Clog_2(n)%20%5Crceil%20%5C%7D">:
<ul>
<li>For <img src="https://latex.codecogs.com/png.latex?j%20%5Cin%20%5C%7B1,%202,%20%5Cdots,%20c%20%5Ccdot%20%5Clceil%20%5Clog_2(n)%20%5Crceil%20%5C%7D">:
<ol type="1">
<li>Choose a random subset <img src="https://latex.codecogs.com/png.latex?S_%7Bij%7D%20%5Csubseteq%20X">, where each <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20X"> is included in <img src="https://latex.codecogs.com/png.latex?S_%7Bij%7D"> with probability <img src="https://latex.codecogs.com/png.latex?2%5E%7B-i%7D">.</li>
<li>Compute <img src="https://latex.codecogs.com/png.latex?d(x,%20S_%7Bij%7D)">, the minimum distance from <img src="https://latex.codecogs.com/png.latex?x"> to any point in <img src="https://latex.codecogs.com/png.latex?S_%7Bij%7D">.</li>
</ol></li>
<li>Construct the embedding vector: <img src="https://latex.codecogs.com/png.latex?f(x)%20=%20%5Clangle%20d(x,%20S_%7B11%7D),%20d(x,%20S_%7B12%7D),%20%5Cdots,%20d(x,%20S_%7B%5Clceil%20%5Clog_2(n)%20%5Crceil%20%5Ccdot%20c%20%5Ccdot%20%5Clceil%20%5Clog_2(n)%20%5Crceil%7D)%5Crangle."></li>
</ul></li>
</ul></li>
</ol>
<p>An intuition for the algorithm is that it creates a “fingerprint” for each point by measuring its distance to random subsets of the space at multiple scales. So, for each point, we answer the question:</p>
<ul>
<li>“How far am I from a randomly chosen half of all points?”</li>
<li>“How far am I from a randomly chosen quarter of all points?”</li>
<li>“How far am I from a randomly chosen eighth of all points?”</li>
<li>…</li>
</ul>
<p>Why does this capture distance information? Imagine two points <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> in our original metric space. If they are close, they’ll have similar distances to most random subsets. When we sample a random subset <img src="https://latex.codecogs.com/png.latex?S">, chances are the nearest point in <img src="https://latex.codecogs.com/png.latex?S"> to <img src="https://latex.codecogs.com/png.latex?x"> will also be close to <img src="https://latex.codecogs.com/png.latex?y"> so, the embedded distances will also be similar.</p>
<p>However, if <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> are far apart, then at some scale, we’ll sample points that “separate” them. That is, there will be random subsets where <img src="https://latex.codecogs.com/png.latex?x"> is close to some sampled point but <img src="https://latex.codecogs.com/png.latex?y"> is far from all sampled points (or vice versa). This creates a difference in their fingerprints that the algorithm can capture when construction the embedding.</p>
<p>Note that in practice, this algorithm can be really bad because of the big-O constants. For example, if <img src="https://latex.codecogs.com/png.latex?X%20=%20%5Cmathbb%7BR%7D%5E%7B1000%7D">, and we chose <img src="https://latex.codecogs.com/png.latex?c%20=%20100"> (for less distortion), then the reduced dimension <img src="https://latex.codecogs.com/png.latex?k%20=%20100%20%5Ccdot%20%5Clog%5E2%7B1000%7D%20=%20900">. This is an improvement over the <img src="https://latex.codecogs.com/png.latex?1000"> dimensional space, but just barely.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>Let’s implement this in python using numpy so we can vectorize parts of the code.</p>
<div id="5e84faad" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> bourgain_embedding(dist_mat, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb1-4">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(dist_mat)</span>
<span id="cb1-5"></span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-7">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.zeros((n, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-8"></span>
<span id="cb1-9">    log_n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(np.ceil(np.log2(n)))</span>
<span id="cb1-10">    k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> log_n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> log_n</span>
<span id="cb1-11">    max_dist <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist_mat.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()</span>
<span id="cb1-12"></span>
<span id="cb1-13">    f_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((n, k))</span>
<span id="cb1-14"></span>
<span id="cb1-15">    j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, log_n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb1-17">        p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>i)</span>
<span id="cb1-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> log_n):</span>
<span id="cb1-19">            subset_mask <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.rand(n) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> p</span>
<span id="cb1-20"></span>
<span id="cb1-21">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">any</span>(subset_mask):</span>
<span id="cb1-22">                f_x[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> max_dist</span>
<span id="cb1-23">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-24">                f_x[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist_mat[:, subset_mask].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb1-25">            j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-26"></span>
<span id="cb1-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> f_x</span></code></pre></div></div>
</div>
<p>We can plot the distortion of this algorithm on a randomly generated metric space, as a function of <img src="https://latex.codecogs.com/png.latex?n"> to see the logarithmic curve.</p>
<div id="047ac06d" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> distortion(dist_mat, emb):</span>
<span id="cb2-4">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist_mat.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-5">    max_expand <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb2-6">    max_contract <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb2-7"></span>
<span id="cb2-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb2-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n):</span>
<span id="cb2-10">            d0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dist_mat[i, j]</span>
<span id="cb2-11">            d1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linalg.norm(emb[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> emb[j], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">ord</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-12"></span>
<span id="cb2-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> d0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> d1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb2-14">                max_expand <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(max_expand, d1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> d0)</span>
<span id="cb2-15">                max_contract <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(max_contract, d0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> d1)</span>
<span id="cb2-16"></span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> max_expand <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> max_contract</span>
<span id="cb2-18"></span>
<span id="cb2-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> gen_random_space(n, d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb2-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> seed <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-21">        np.random.seed(seed)</span>
<span id="cb2-22">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(n, d)</span>
<span id="cb2-23">    diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[:, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> X[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, :, :]</span>
<span id="cb2-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.linalg.norm(diff, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-25"></span>
<span id="cb2-26">c_fixed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb2-27">ns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">350</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">750</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>]</span>
<span id="cb2-28"></span>
<span id="cb2-29">distortions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-30"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> ns:</span>
<span id="cb2-31">    dist_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_space(n, d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-32">    emb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bourgain_embedding(dist_mat, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>c_fixed)</span>
<span id="cb2-33">    distortions.append(distortion(dist_mat, emb))</span>
<span id="cb2-34"></span>
<span id="cb2-35">log_ns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log(ns)</span>
<span id="cb2-36">log_ns_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_ns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> log_ns[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> distortions[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-37"></span>
<span id="cb2-38">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb2-39">plt.plot(ns, distortions, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bourgain distortion"</span>)</span>
<span id="cb2-40">plt.plot(ns, log_ns_scaled, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r"scaled </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">\</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">log n</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-41"></span>
<span id="cb2-42">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r"</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">n</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-43">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distortion"</span>)</span>
<span id="cb2-44">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bourgain embedding distortion vs log n"</span>)</span>
<span id="cb2-45">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-46">plt.legend()</span>
<span id="cb2-47">plt.tight_layout()</span>
<span id="cb2-48">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://csuraparaju.github.io/posts/bourgain-embedding/index_files/figure-html/cell-3-output-1.png" width="662" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note that the variable <img src="https://latex.codecogs.com/png.latex?c"> is a hyper-parameter to this algorithm, and we can observe distortion as a function of <img src="https://latex.codecogs.com/png.latex?c"> as well.</p>
<div id="1017dff4" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">n_fixed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb3-2">cs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">400</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">600</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">700</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">800</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">900</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1100</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1200</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1300</span>]</span>
<span id="cb3-3"></span>
<span id="cb3-4">distortions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> cs:</span>
<span id="cb3-6">    dist_mat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> gen_random_space(n_fixed, d<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb3-7">    emb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bourgain_embedding(dist_mat, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>c)</span>
<span id="cb3-8">    distortions.append(distortion(dist_mat, emb))</span>
<span id="cb3-9"></span>
<span id="cb3-10"></span>
<span id="cb3-11">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb3-12">plt.plot(cs, distortions, marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bourgain distortion"</span>)</span>
<span id="cb3-13"></span>
<span id="cb3-14">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r"</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">c</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-15">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distortion"</span>)</span>
<span id="cb3-16">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Bourgain embedding distortion vs c"</span>)</span>
<span id="cb3-17">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb3-18">plt.legend()</span>
<span id="cb3-19">plt.tight_layout()</span>
<span id="cb3-20">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://csuraparaju.github.io/posts/bourgain-embedding/index_files/figure-html/cell-4-output-1.png" width="662" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Ye2023" class="csl-entry">
Ye, Richard. 2023. <span>“Bourgain’s Embedding Theorem, Johnson-Lindenstrauss Lemma, and the Sparsest Cut Problem.”</span>
</div>
</div></section></div> ]]></description>
  <category>math</category>
  <guid>https://csuraparaju.github.io/posts/bourgain-embedding/</guid>
  <pubDate>Fri, 20 Dec 2024 05:00:00 GMT</pubDate>
</item>
</channel>
</rss>
