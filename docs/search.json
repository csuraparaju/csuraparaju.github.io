[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "I’m an undergraduate at Carnegie Mellon University studying mathematics and computer science. I blog here about things I find interesting.\nMy favorite classes at CMU have been Theoretical Computer Science (15-251), Markov Chains (21-326) and Computational Biology (02-251).\n\n\nCarnegie Mellon University | Pittsburgh, PA | Aug 2022 - May 2026\nB.S. in Mathematics, Additional Major in Computer Science\n\n\n\nMeta | SWE Intern | May 2025 - Aug 2025\nAmazon | SDE Intern | May 2024 - Aug 2024\nCMU | Head TA for 15-122 | Aug 2023 - present\nMohimani Lab @ CMU | Undergraduate RA | Jan 2023 - Aug 2024"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "Carnegie Mellon University | Pittsburgh, PA | Aug 2022 - May 2026\nB.S. in Mathematics, Additional Major in Computer Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "Meta | SWE Intern | May 2025 - Aug 2025\nAmazon | SDE Intern | May 2024 - Aug 2024\nCMU | Head TA for 15-122 | Aug 2023 - present\nMohimani Lab @ CMU | Undergraduate RA | Jan 2023 - Aug 2024"
  },
  {
    "objectID": "posts/dual-numbers/index.html",
    "href": "posts/dual-numbers/index.html",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "",
    "text": "I recently came across a curious algebraic structure called the Dual Number system, denoted as \\(\\mathbb{R} (\\epsilon)\\). The dual numbers are an extension of the real numbers, with a very interesting property: evaluating a differentiable function \\(f\\) at a dual number \\(x\\) will give us both \\(f(x)\\) and \\(f'(x)\\) at the same time. This is a powerful idea in numerical analysis and machine learning, as it allows for efficient computation of derivatives."
  },
  {
    "objectID": "posts/dual-numbers/index.html#introduction",
    "href": "posts/dual-numbers/index.html#introduction",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "",
    "text": "I recently came across a curious algebraic structure called the Dual Number system, denoted as \\(\\mathbb{R} (\\epsilon)\\). The dual numbers are an extension of the real numbers, with a very interesting property: evaluating a differentiable function \\(f\\) at a dual number \\(x\\) will give us both \\(f(x)\\) and \\(f'(x)\\) at the same time. This is a powerful idea in numerical analysis and machine learning, as it allows for efficient computation of derivatives."
  },
  {
    "objectID": "posts/dual-numbers/index.html#dual-numbers",
    "href": "posts/dual-numbers/index.html#dual-numbers",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Dual Numbers",
    "text": "Dual Numbers\nLet \\(\\epsilon \\neq 0\\) be a new “number” such that \\(\\epsilon^2 = 0\\) (Penn 2022). Before we go any further, we must first ask ourselves if such a number can even exist. The answer is of course not if you want to work in complete fields like \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\). No matter how small a real (or complex) number is, as long as it is non-zero, its square will also be non-zero. So why do we care about this number then?\nOne reason is that it can help us model the notion of an “infinitesimal” from calculus, which can be thought of as a non-zero number that is “smaller” than any other number. Using the definition above then, we note that even though \\(\\epsilon\\) is non-zero, its square is zero, and so the number is somehow “smaller” than any other real (or complex) number. In order to study this more, let’s assume that such a number actually exists and try to see if any laws we know about \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\) does (or does not) break down. Then, let’s see how it can be used to compute derivatives of a function.\nThe structure we want to study is defined as follows \\[\n\\mathbb{R} (\\epsilon) = \\{ a + b \\epsilon \\mid a, b \\in \\mathbb{R} \\}\n\\]"
  },
  {
    "objectID": "posts/dual-numbers/index.html#basic-arithmetic",
    "href": "posts/dual-numbers/index.html#basic-arithmetic",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Basic Arithmetic",
    "text": "Basic Arithmetic\nFirst, let’s define how to add, subtract, and multiply. Given a dual number \\(x = a + b \\epsilon\\) and \\(y = c + d \\epsilon\\), define: \\[\\begin{align*}\nx + y &:= (a + c) + (b + d) \\epsilon \\\\\nx - y &:= (a - c) + (b - d) \\epsilon \\\\\nx \\cdot y &:= (a \\cdot c) + (a \\cdot d + d \\cdot c) \\epsilon \\\\\n\\end{align*}\\]\nWhenever we create new objects and define operations on the objects, we should always check that they are well defined. That means, for each operation above we need to check that if \\(x = y\\) and \\(z\\) is any other dual number then \\(x + z = y + z\\) and \\(x - z = y - z\\) and \\(x \\cdot z = y \\cdot z\\). In this case, checking this is very easy so I won’t do that. We can also divide \\(x / y\\), given that \\(c \\neq 0\\): \\[\n\\frac{x}{y} := \\frac{a}{c} + \\frac{b \\cdot c - a \\cdot d}{c^2}\n\\] Hopefully it is not hard to see that distributivity, associativity, and commutativity follow because we are simply using \\(+, -, \\cdot\\) from \\(\\mathbb{R}\\). This means all the nice properties we learned in high school about algebra on real numbers (ex. FOIL) still hold. So far so good.\nHowever, things get interesting when we starting playing with multiplication and division operator a bit more. Specifically, what happens when we multiply two numbers \\(0 + b \\epsilon\\) and \\(0 + d \\epsilon\\) where \\(b, d \\neq 0\\)? Clearly these are non-zero numbers. However, note that \\[\n(0 + b\\epsilon) + (0 + d\\epsilon) = (0 \\cdot 0) + (0 \\cdot d + b \\cdot 0)\\epsilon = 0\n\\] This s strange! Recall that in \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\) there are no zero divisors. That is, if \\(a, b \\in \\mathbb{R} (\\text{or } \\mathbb{C})\\), and \\(a \\cdot b = 0\\) then either \\(a = 0\\) or \\(b = 0\\). We just saw that this need not be the case for dual numbers anymore! Finally, a place where dual number arithmetic breaks down. Nonetheless, it is still remarkable that most of the basic arithmetic operations and laws from \\(\\mathbb{R}\\) still hold, so let’s see what kind of algebra we can do on these numbers."
  },
  {
    "objectID": "posts/dual-numbers/index.html#polynomials",
    "href": "posts/dual-numbers/index.html#polynomials",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Polynomials",
    "text": "Polynomials\nLet’s consider a simple polynomial function over the real numbers \\(f(x) = x^2\\). We can extend this function to accept dual numbers as input by replacing the real number \\(x = a\\) with the dual number \\(x = a + \\epsilon\\). \\[\nf((a + \\epsilon)) = (a+\\epsilon)\\cdot (a + \\epsilon) = a^2 + (a + a) \\epsilon = a^2 + 2 a \\epsilon\n\\] Notice what just happened. The term \\(2 a \\epsilon\\) is the derivative of the function \\(f\\) evaluated at \\(x = a\\). By just computing the function \\(f\\) at the modified dual number, we were able to get the derivative as well in one computation.\nLet’s use the Binomial Theorem to see if this generalizes to \\(f(a + \\epsilon) = (a+\\epsilon)^n\\) for an arbitrary \\(n\\): \\[\\begin{align*}\nf(a + \\epsilon) &= (a + \\epsilon)^n \\\\\n&= \\sum_{i = 0}^n {n \\choose i} a^{n - i} \\epsilon^i \\\\\n&= {n \\choose 0} a^n \\epsilon^0 + {n \\choose 1} a^{n-1} \\epsilon^1 + \\sum_{i = 2}^n {n \\choose i} a^{n - i} \\epsilon^i \\\\\n&= a^n + n \\cdot a^{n-1} \\epsilon\n\\end{align*}\\] Where the last step follows because \\(\\epsilon^2 = 0\\) (by definition) and so all the other terms with \\(\\epsilon^i\\) for \\(i \\geq 2\\) cancels out. Notice that this is exactly what we wanted to see: \\(f(a+\\epsilon) = f(a) + f'(a) \\epsilon\\)\nNow, lets go even futher and see if this notion generalizes to an arbitrary polynomial \\(f(a + \\epsilon) = p_n (a + \\epsilon)^n + p_{n-1} (a + \\epsilon)^{n-1} + \\cdots + p_1 (a + \\epsilon) + p_0\\): \\[\\begin{align*}\nf(a + \\epsilon) &= p_n (a + \\epsilon)^n + p_{n-1} (a + \\epsilon)^{n-1} + \\cdots + p_1 (a + \\epsilon) + p_0 \\\\\n&= p_n \\sum_{i = 0}^n {n \\choose i} a^{n - i} \\epsilon^i + p_{n-1} \\sum_{i = 0}^{n-1} {n-1 \\choose i} a^{n - 1- i} \\epsilon^i +p_1 (a + \\epsilon) + p_0  \\\\\n&= p_n (a^n + n ^{n-1} \\epsilon) + p_{n-1} (a^{n-1} + (n-1) a^{n-2} \\epsilon) + \\cdots + p_1 (a + \\epsilon) + p_0 \\\\\n&= p_n a^n + p_n n a^{n-1}\\epsilon + p_{n-1}a^{n-1} + p_{n-1} (n-1)a^{n-2}\\epsilon + \\cdots + p_1 a + p_1 \\epsilon + p_0 \\\\\n&= (p_n a^n + p_{n-1} a^{n-1} + \\cdots + p_1 a + p_0) +(p_n n a^{n-1} + p_{n-1} (n-1) a^{n-2} + \\cdots + p_1) \\epsilon \\\\\n&= f(a) + f'(a) \\epsilon\n\\end{align*}\\] and indeed it does."
  },
  {
    "objectID": "posts/dual-numbers/index.html#other-functions",
    "href": "posts/dual-numbers/index.html#other-functions",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Other functions",
    "text": "Other functions\nMaybe we can actually make a stronger claim: does this property holds for all infinitely differentiable functions? So, let \\(f\\) be such a function evaluated at the dual number \\(a + c \\epsilon\\) (note that the coefficient for the dual part need not be 1 anymore). Then we can use the taylor series expansion of \\(f\\) to write \\[\\begin{align*}\nf(a + c\\epsilon) &= \\sum_{n = 0}^\\infty \\frac{f^{(n)} (a)}{n!} ((a + c\\epsilon) - a)^n \\\\\n&= \\sum_{n = 0}^\\infty \\frac{f^{(n)} (a)}{n!} (c\\epsilon)^n \\\\\n&= \\frac{f^{(0)} (a)}{0!} (c\\epsilon)^0 + \\frac{f^{(1)} (a)}{1!} (c\\epsilon)^1 + \\sum_{n = 2}^\\infty \\frac{f^{(n)} (a)}{n!} (c\\epsilon)^n \\\\\n& f(a) + f'(a) c \\epsilon\n\\end{align*}\\]\nand again, it does!\nBut what about composition of functions? Let \\(f, g\\) be functions extended to dual numbers such that \\(f(a + c \\epsilon) = f(a) + f'(a) c \\epsilon\\) and \\(g(a + c \\epsilon) = g(a) + g'(a) c \\epsilon\\). Then, note that: \\[\\begin{align*}\nf(g(a+\\epsilon)) &= f(g(a) + g'(a)\\epsilon) \\\\\n&= f(g(a)) + f'(g(a)) \\cdot g'(a) \\epsilon \\\\\n\\end{align*}\\] and notice that the dual component \\(f'(g(a)) \\cdot g'(a)\\) is exactly the chain rule of derivatives! So, now we can imagine all sorts of complicated functions like \\[\nf(x) = \\sin(e^{x^2 + 1})\n\\] and by using the dual number representation, we can compute both the function value and its derivative in a single forward pass through the function."
  },
  {
    "objectID": "posts/dual-numbers/index.html#closing-thoughts",
    "href": "posts/dual-numbers/index.html#closing-thoughts",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nWhat I find most beautiful about dual numbers is how they transform the problem of differentiation from a difficult limiting process \\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\] into a problem of pure algebraic manipulation. This is incredibly beautiful because limits are (arguably) one of the ugliest tools in mathematics, and turning it into a formulation of abstract algebra is extremely satisfying.\nWhile we gained zero divisors in dual numbers, what we got in return makes up for it: a computational method for computing derivatives that is exact (no truncation error like finite differences) and efficient (requires roughly the same number of operations as computing \\(f\\) itself). This is why dual numbers, despite being a somewhat obscure mathematical structure, power automatic differentiation systems used in training neural networks."
  },
  {
    "objectID": "posts/noisy-channels/index.html",
    "href": "posts/noisy-channels/index.html",
    "title": "Sending Messages over Noisy Channels",
    "section": "",
    "text": "A fundamental problem in electrical engineering is sending messages over noisy channels. For example, all messages sent over the internet have to eventually be transmitted over physical channels (copper wires or fiber optic cables), and these channels can be damaged by the elements (eg. sharks)\nI recently came across a method to reduce this problem to a purely mathematical one: the sphere packing problem, which aims to find the densest packing of non-overlapping \\(n\\) dimensional spheres in a given space."
  },
  {
    "objectID": "posts/noisy-channels/index.html#introduction",
    "href": "posts/noisy-channels/index.html#introduction",
    "title": "Sending Messages over Noisy Channels",
    "section": "",
    "text": "A fundamental problem in electrical engineering is sending messages over noisy channels. For example, all messages sent over the internet have to eventually be transmitted over physical channels (copper wires or fiber optic cables), and these channels can be damaged by the elements (eg. sharks)\nI recently came across a method to reduce this problem to a purely mathematical one: the sphere packing problem, which aims to find the densest packing of non-overlapping \\(n\\) dimensional spheres in a given space."
  },
  {
    "objectID": "posts/noisy-channels/index.html#signals-and-code",
    "href": "posts/noisy-channels/index.html#signals-and-code",
    "title": "Sending Messages over Noisy Channels",
    "section": "Signals and Code",
    "text": "Signals and Code\nBut first, let’s define the noisy channel problem more formally. Let \\(T &gt; 0\\) be a fixed length of time corresponding to the length of a signal transmission.\nA signal is a continuous function \\(s : [0, T] \\to \\mathbb{R},\\) where \\(s(t)\\) is the amplitude of the signal at time \\(t\\), and the frequencies do not surpass some fixed limit \\(W\\). Think of \\(s(t)\\) as the voltage of the signal at time \\(t\\) (for copper wires), or the intensity of a light signal at time \\(t\\) (for fiber optic cables).\nA code is a finite set of signals \\(\\{s_1, s_2, \\ldots, s_n\\}.\\) This can be thought of as a symbolic alphabet for two computers to communicate over a channel. A simple example of a code is \\[\n\\{ s_1(t) = 1,\\; s_2(t) = -1 \\},\n\\] which can be used to send binary messages over a channel.\nThe Shannon–Nyquist Sampling Theorem states that any signal \\(s(t)\\) with frequencies less than \\(W\\) can be uniquely represented by a finite set of samples \\[\n\\left\\{ s(0), s\\!\\left(\\tfrac{1}{W}\\right), s\\!\\left(\\tfrac{1}{2W}\\right), \\ldots, s\\!\\left(\\tfrac{n-1}{2W}\\right) \\right\\},\n\\] where \\(n = 2WT\\). This means that we can represent any signal \\(s(t)\\) as a vector \\(\\vec{s} \\in \\mathbb{R}^n,\\) and any code as a finite subset \\(C \\subseteq \\mathbb{R}^n\\), where each element of \\(C\\) represents a signal.\n\n\n\n\n\nThe continuous signal above \\(S(t)\\) represented by the discrete samples \\(S_i\\). Therefore, we will represent a signal as a vector in the remaining discussion."
  },
  {
    "objectID": "posts/noisy-channels/index.html#the-noisy-channel",
    "href": "posts/noisy-channels/index.html#the-noisy-channel",
    "title": "Sending Messages over Noisy Channels",
    "section": "The Noisy Channel",
    "text": "The Noisy Channel\nIn the real world, the sent signal \\(\\vec{s}\\) is almost never the same as the received signal \\(\\vec{r}\\). This is because the channel introduces noise, which we model as a random perturbation in the input.\nFormally, we assume that the received signal is \\[\n\\vec{r} = \\vec{s} + \\vec{z},\n\\] where \\(\\vec{z}\\) is a random vector with Gaussian entries (each \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\) and is i.i.d). If the receiver wants to determine which signal was sent, a natural decoding strategy is nearest-neighbor decoding: \\[\n\\hat{\\vec{s}} = \\underset{\\vec{s_i} \\in C}{\\text{argmin}} \\| \\vec{r} - \\vec{s_i} \\|_2.\n\\]\nHowever, if two signals in the code are too close together, noise may make decoding ambiguous. For example, in the following situation:\n\n\n\n\n\nwhich signal does \\(\\vec{r}\\) correspond to? Both \\(\\vec{s_1}\\) and \\(\\vec{s_2}\\) are equally likely.\n\n\nTo solve this problem, remember that we assumed \\(\\vec{r} = \\vec{s} + \\vec{z}\\) and that each \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\). A fundamental property of the Gaussian distribution is that \\[\n\\mathbb{P}[-3 \\sigma \\leq z_i \\leq 3 \\sigma] \\approx 99.7\\%\n\\] So, we can use this to figure out a bound on the distance between \\(\\vec{r}\\) and \\(\\vec{s}\\) with high probability. In the worst-case, each component is such that \\(|z_i| = 3 \\sigma\\). Since we have \\(n\\) components, this means that \\[\\begin{align*}\n\\| \\vec{r} - \\vec{s} \\|_2 =  \\| \\vec{z}\\|_2 &= \\sqrt{\\sum_{i = 1}^{n} z_i^2} \\\\ &\\leq\\sqrt{\\sum_{i = 1}^{n} (3 \\sigma)^2} \\\\ &\\leq \\sqrt{n \\cdot 9 \\sigma^2} \\\\ &\\leq 3 \\sigma \\sqrt{n}\n\\end{align*}\\] with probability \\(99.7\\%\\). This means that the worst-case noise can push the received signal \\(\\vec{r}\\) up to a distance of \\(3 \\sigma \\sqrt{n}\\) away from the true signal \\(\\vec{s}\\). This creates a sphere of radius \\(3 \\sigma \\sqrt{n}\\) around \\(\\vec{s_1}\\). To make our decoding as unambiguous as possible, we need to ensure that even when noise pushes \\(\\vec{r}\\) to the edge of this sphere, it is still closer to \\(\\vec{s}\\) than any other signal \\(\\vec{s_2} \\in C\\). But signal \\(\\vec{s_2}\\) also has a sphere of radius \\(3 \\sigma \\sqrt{n}\\) around it. Therefore, to guarantee that two spheres don’t overlap, the minimum distance between any two signals must be at least \\(6 \\sigma \\sqrt{n}\\)"
  },
  {
    "objectID": "posts/noisy-channels/index.html#dense-sphere-packing",
    "href": "posts/noisy-channels/index.html#dense-sphere-packing",
    "title": "Sending Messages over Noisy Channels",
    "section": "Dense Sphere packing",
    "text": "Dense Sphere packing\nNow, you might ask: why don’t we just pick signals that are as far apart as possible? For example, we could place just two signals at opposite ends of our signal space.\nBut recall that a signal \\(\\vec{s}\\) represents the amplitude or voltage of a message over time. Physicists have told us the power of a signal is proportional to the square of its amplitude: \\[\n\\text{Power} \\propto \\|\\vec{s}\\|_2^2 = \\sum_{i=1}^{n} s_i^2\n\\]\nIn practice, we cannot transmit signals with arbitrarily large power. There are physical limits on how much voltage we can apply to a copper wire and how much light we can send through a fiber optic cable. If we set a maximum power budget \\(P\\), then all our signals must satisfy \\[\n\\|\\vec{s}\\|_2^2 \\leq nP \\quad \\Longrightarrow \\quad \\|\\vec{s}\\|_2 \\leq \\sqrt{nP}\n\\]\nThis means all our signals must lie within a sphere of radius \\(\\sqrt{nP}\\) in \\(\\mathbb{R}^n\\), which is a compact region. The more signals we can fit in this region, the more information we can transmit per signal. For example, a code with 2 signals transmits only 1 bit per transmission, but a code with 256 signals transmits 8 bits per transmission.\nAt the same time, each signal needs a sphere of radius \\(3\\sigma\\sqrt{n}\\) around it to ensure correct decoding with probability \\(99.7\\%\\). This is exactly the sphere packing problem: what is the maximum number of non-overlapping spheres of radius \\(3\\sigma\\sqrt{n}\\) that we can pack inside a sphere of radius \\(\\sqrt{nP}\\)? Each sphere center represents a signal in our code, and finding the densest packing directly gives us the code that can transmit the most information reliably. Therefore, the problem of designing optimal codes for noisy channels reduces to an entirely abstract problem in math."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html",
    "href": "posts/bourgain-embedding/index.html",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "",
    "text": "Metric embeddings and dimension reduction are important tools in computer science and mathematics for solving the curse of dimensionality. In this post, I want to examine the Bourgain embedding, which shows that any finite metric space can be embedded into an Euclidean metric space space with \\(O(\\log ⁡n)\\) distortion. While Bourgain embeddings are very impractical (the constant factors in the big - O are huge and the “reduced” dimensions is still high), it is an important result that shows us that no matter how “horrible” the initial metric space is, we can be achieve a distortion factor of \\(O(\\log n)\\).\nNote: I’ve written this post assuming that the reader has at least taken an undergraduate discrete math course. However, I will still define the important objects we will be using today.\n\n\nA metric space \\((X, d)\\) is a set \\(X\\) with a distance function \\(d: X \\times X \\to \\mathbb{R}\\). The distance function satisfies the following properties:\n\n\\(d(x, y) \\geq 0\\) for all \\(x, y \\in X\\)\n\\(d(x, y) = 0\\) if and only if \\(x = y\\)\n\\(d(x, y) = d(y, x)\\) for all \\(x, y \\in X\\)\n\\(d(x, y) \\leq d(x, z) + d(z, y)\\) for all \\(x, y, z \\in X\\)\n\nSome examples of metric spaces:\n\nA weighted graph \\(G = (V, E)\\) with \\(X = V\\) and \\(d(x, y) =\\) length of the shortest path between \\(x\\) and \\(y\\).\nThe DNA space with \\(X = \\{A, C, G, T\\}^n\\) and \\(d(x, y) =\\) number of positions where \\(x\\) and \\(y\\) differ.\nThe Euclidean space \\(\\mathbb{R}^n\\) with \\(X = \\mathbb{R}^n\\) and \\(d(x, y) = \\|x - y\\|_2 = \\sqrt{(x_1 - y_1)^2 + \\cdots + (x_n - y_n)^2}\\).\n\n\n\n\nA map \\(f: X \\to Y\\) between two metric spaces \\((X, d_X)\\) and \\((Y, d_Y)\\) is called an embedding that maps elements of \\(X\\) to elements of \\(Y\\).\nThe embedding is said to be distance-preserving (isometric) if \\(d_Y(f(x), f(y)) = d_X(x, y)\\) for all \\(x, y \\in X\\). However, very rarely do we have distance-preserving embeddings between metric spaces. Instead, we often consider embeddings that are “almost” distance-preserving.\nAn embedding with distortion of \\(\\alpha\\) of a metric space \\((X, d_X)\\) into another metric space \\((Y, d_Y)\\) is a map \\(f: X \\to Y\\) such that there exists constant \\(r &gt; 0\\) for which \\[r \\cdot d_X(x, y) \\leq d_Y(f(x), f(y)) \\leq \\alpha r \\cdot d_X(x, y) \\text{ for all } x, y \\in X\\] The distortion of an embedding is the smallest \\(\\alpha\\) for which such a map exists.\nBecause we are working in a finite set for now, we can equivalently define the distortion in terms of the contraction and expansion. Given a map \\(f: X \\to Y\\), let:\n\\[\\text{Contraction}(f) = \\max_{x, y \\in X} \\frac{d_Y(f(x), f(y))}{d_X(x, y)}\\]\n\\[ \\text{Expansion}(f) = \\max_{x, y \\in X} \\frac{d_X(x, y)}{d_Y(f(x), f(y))}\\]\nDefine the distortion of \\(f\\) as \\(\\alpha = \\text{Expansion}(f)\\cdot\\text{Contraction}(f)\\)."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#introduction",
    "href": "posts/bourgain-embedding/index.html#introduction",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "",
    "text": "Metric embeddings and dimension reduction are important tools in computer science and mathematics for solving the curse of dimensionality. In this post, I want to examine the Bourgain embedding, which shows that any finite metric space can be embedded into an Euclidean metric space space with \\(O(\\log ⁡n)\\) distortion. While Bourgain embeddings are very impractical (the constant factors in the big - O are huge and the “reduced” dimensions is still high), it is an important result that shows us that no matter how “horrible” the initial metric space is, we can be achieve a distortion factor of \\(O(\\log n)\\).\nNote: I’ve written this post assuming that the reader has at least taken an undergraduate discrete math course. However, I will still define the important objects we will be using today.\n\n\nA metric space \\((X, d)\\) is a set \\(X\\) with a distance function \\(d: X \\times X \\to \\mathbb{R}\\). The distance function satisfies the following properties:\n\n\\(d(x, y) \\geq 0\\) for all \\(x, y \\in X\\)\n\\(d(x, y) = 0\\) if and only if \\(x = y\\)\n\\(d(x, y) = d(y, x)\\) for all \\(x, y \\in X\\)\n\\(d(x, y) \\leq d(x, z) + d(z, y)\\) for all \\(x, y, z \\in X\\)\n\nSome examples of metric spaces:\n\nA weighted graph \\(G = (V, E)\\) with \\(X = V\\) and \\(d(x, y) =\\) length of the shortest path between \\(x\\) and \\(y\\).\nThe DNA space with \\(X = \\{A, C, G, T\\}^n\\) and \\(d(x, y) =\\) number of positions where \\(x\\) and \\(y\\) differ.\nThe Euclidean space \\(\\mathbb{R}^n\\) with \\(X = \\mathbb{R}^n\\) and \\(d(x, y) = \\|x - y\\|_2 = \\sqrt{(x_1 - y_1)^2 + \\cdots + (x_n - y_n)^2}\\).\n\n\n\n\nA map \\(f: X \\to Y\\) between two metric spaces \\((X, d_X)\\) and \\((Y, d_Y)\\) is called an embedding that maps elements of \\(X\\) to elements of \\(Y\\).\nThe embedding is said to be distance-preserving (isometric) if \\(d_Y(f(x), f(y)) = d_X(x, y)\\) for all \\(x, y \\in X\\). However, very rarely do we have distance-preserving embeddings between metric spaces. Instead, we often consider embeddings that are “almost” distance-preserving.\nAn embedding with distortion of \\(\\alpha\\) of a metric space \\((X, d_X)\\) into another metric space \\((Y, d_Y)\\) is a map \\(f: X \\to Y\\) such that there exists constant \\(r &gt; 0\\) for which \\[r \\cdot d_X(x, y) \\leq d_Y(f(x), f(y)) \\leq \\alpha r \\cdot d_X(x, y) \\text{ for all } x, y \\in X\\] The distortion of an embedding is the smallest \\(\\alpha\\) for which such a map exists.\nBecause we are working in a finite set for now, we can equivalently define the distortion in terms of the contraction and expansion. Given a map \\(f: X \\to Y\\), let:\n\\[\\text{Contraction}(f) = \\max_{x, y \\in X} \\frac{d_Y(f(x), f(y))}{d_X(x, y)}\\]\n\\[ \\text{Expansion}(f) = \\max_{x, y \\in X} \\frac{d_X(x, y)}{d_Y(f(x), f(y))}\\]\nDefine the distortion of \\(f\\) as \\(\\alpha = \\text{Expansion}(f)\\cdot\\text{Contraction}(f)\\)."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#bourgain-embedding",
    "href": "posts/bourgain-embedding/index.html#bourgain-embedding",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "Bourgain Embedding",
    "text": "Bourgain Embedding\nGiven an arbitrary finite metric space \\((X, d)\\) with \\(n\\) points, Bourgain’s theorem says that there exists a map \\(f: X \\to \\mathbb{R}^k\\) such that the distortion of \\(f\\) is \\(\\alpha \\in O(\\log n)\\), and \\(k \\in O(\\log^2 n)\\). The proof of this theorem is beyond the scope of this post, but it is a constructive proof so there is a natural algorithm that arises from the proof which we can implement.\nThe Bourgain embedding algorithm is as follows (Ye 2023):\n\nLet \\(c\\) be a sufficiently large constant, and let \\(\\log(n)\\) denote the base-2 logarithm of \\(n\\).\nFor each point \\(x \\in X\\), define its embedding vector \\(f(x)\\) in the following steps:\n\nFor \\(i \\in \\{1, 2, \\dots, \\lceil \\log_2(n) \\rceil \\}\\):\n\nFor \\(j \\in \\{1, 2, \\dots, c \\cdot \\lceil \\log_2(n) \\rceil \\}\\):\n\nChoose a random subset \\(S_{ij} \\subseteq X\\), where each \\(y \\in X\\) is included in \\(S_{ij}\\) with probability \\(2^{-i}\\).\nCompute \\(d(x, S_{ij})\\), the minimum distance from \\(x\\) to any point in \\(S_{ij}\\).\n\nConstruct the embedding vector: \\(f(x) = \\langle d(x, S_{11}), d(x, S_{12}), \\dots, d(x, S_{\\lceil \\log_2(n) \\rceil \\cdot c \\cdot \\lceil \\log_2(n) \\rceil})\\rangle.\\)\n\n\n\nAn intuition for the algorithm is that it creates a “fingerprint” for each point by measuring its distance to random subsets of the space at multiple scales. So, for each point, we answer the question:\n\n“How far am I from a randomly chosen half of all points?”\n“How far am I from a randomly chosen quarter of all points?”\n“How far am I from a randomly chosen eighth of all points?”\n…\n\nWhy does this capture distance information? Imagine two points \\(x\\) and \\(y\\) in our original metric space. If they are close, they’ll have similar distances to most random subsets. When we sample a random subset \\(S\\), chances are the nearest point in \\(S\\) to \\(x\\) will also be close to \\(y\\) so, the embedded distances will also be similar.\nHowever, if \\(x\\) and \\(y\\) are far apart, then at some scale, we’ll sample points that “separate” them. That is, there will be random subsets where \\(x\\) is close to some sampled point but \\(y\\) is far from all sampled points (or vice versa). This creates a difference in their fingerprints that the algorithm can capture when construction the embedding.\nNote that in practice, this algorithm can be really bad because of the big-O constants. For example, if \\(X = \\mathbb{R}^{1000}\\), and we chose \\(c = 100\\) (for less distortion), then the reduced dimension \\(k = 100 \\cdot \\log^2{1000} = 900\\). This is an improvement over the \\(1000\\) dimensional space, but just barely."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#implementation",
    "href": "posts/bourgain-embedding/index.html#implementation",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "Implementation",
    "text": "Implementation\nLet’s implement this in python using numpy so we can vectorize parts of the code.\n\nimport numpy as np\n\ndef bourgain_embedding(dist_mat, c=10):\n    n = len(dist_mat)\n\n    if n &lt;= 1:\n        return np.zeros((n, 1))\n\n    log_n = int(np.ceil(np.log2(n)))\n    k = c * log_n * log_n\n    max_dist = dist_mat.max()\n\n    f_x = np.zeros((n, k))\n\n    j = 0\n    for i in range(1, log_n + 1):\n        p = 2 ** (-i)\n        for _ in range(c * log_n):\n            subset_mask = np.random.rand(n) &lt; p\n\n            if not np.any(subset_mask):\n                f_x[:, j] = max_dist\n            else:\n                f_x[:, j] = dist_mat[:, subset_mask].min(axis=1)\n            j += 1\n\n    return f_x\n\nWe can plot the distortion of this algorithm on a randomly generated metric space, as a function of \\(n\\) to see the logarithmic curve.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef distortion(dist_mat, emb):\n    n = dist_mat.shape[0]\n    max_expand = 0.0\n    max_contract = 0.0\n\n    for i in range(n):\n        for j in range(i + 1, n):\n            d0 = dist_mat[i, j]\n            d1 = np.linalg.norm(emb[i] - emb[j], ord=1)\n\n            if d0 &gt; 0 and d1 &gt; 0:\n                max_expand = max(max_expand, d1 / d0)\n                max_contract = max(max_contract, d0 / d1)\n\n    return max_expand * max_contract\n\ndef gen_random_space(n, d=5, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    X = np.random.randn(n, d)\n    diff = X[:, None, :] - X[None, :, :]\n    return np.linalg.norm(diff, axis=2)\n\nc_fixed = 8\nns = [10, 50, 100, 350, 500, 750, 1000]\n\ndistortions = []\nfor n in ns:\n    dist_mat = gen_random_space(n, d=5, seed=0)\n    emb = bourgain_embedding(dist_mat, c=c_fixed)\n    distortions.append(distortion(dist_mat, emb))\n\nlog_ns = np.log(ns)\nlog_ns_scaled = log_ns / log_ns[0] * distortions[0]\n\nplt.figure(figsize=(7, 5))\nplt.plot(ns, distortions, marker='o', label=\"Bourgain distortion\")\nplt.plot(ns, log_ns_scaled, linestyle='--', label=r\"scaled $\\log n$\")\n\nplt.xlabel(r\"$n$\")\nplt.ylabel(\"Distortion\")\nplt.title(\"Bourgain embedding distortion vs log n\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNote that the variable \\(c\\) is a hyper-parameter to this algorithm, and we can observe distortion as a function of \\(c\\) as well.\n\n\nCode\nn_fixed = 100\ncs = [10, 50, 100, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300]\n\ndistortions = []\nfor c in cs:\n    dist_mat = gen_random_space(n_fixed, d=5, seed=0)\n    emb = bourgain_embedding(dist_mat, c=c)\n    distortions.append(distortion(dist_mat, emb))\n\n\nplt.figure(figsize=(7, 5))\nplt.plot(cs, distortions, marker='o', label=\"Bourgain distortion\")\n\nplt.xlabel(r\"$c$\")\nplt.ylabel(\"Distortion\")\nplt.title(\"Bourgain embedding distortion vs c\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/sampling/index.html",
    "href": "posts/sampling/index.html",
    "title": "Algorithms for Sampling from a probability distribution",
    "section": "",
    "text": "Drawing samples from a probability distribution \\(\\pi\\) is a problem that arises often in computer science. For example in generative AI, language models sample tokens from learned probability distributions to generate new text. In computer graphics, many graphical engines randomly sample light paths bouncing around to simulate realistic lighting. But what exactly does it mean for a computer or an algorithm to sample from a probability distribution? How can a deterministic machine like a modern computer sample “randomly” from a distribution?"
  },
  {
    "objectID": "posts/sampling/index.html#introduction",
    "href": "posts/sampling/index.html#introduction",
    "title": "Algorithms for Sampling from a probability distribution",
    "section": "",
    "text": "Drawing samples from a probability distribution \\(\\pi\\) is a problem that arises often in computer science. For example in generative AI, language models sample tokens from learned probability distributions to generate new text. In computer graphics, many graphical engines randomly sample light paths bouncing around to simulate realistic lighting. But what exactly does it mean for a computer or an algorithm to sample from a probability distribution? How can a deterministic machine like a modern computer sample “randomly” from a distribution?"
  },
  {
    "objectID": "posts/sampling/index.html#hardware-assumptions",
    "href": "posts/sampling/index.html#hardware-assumptions",
    "title": "Algorithms for Sampling from a probability distribution",
    "section": "Hardware Assumptions",
    "text": "Hardware Assumptions\nOn most modern computers, one bit represents an atom of data or computation. So, if we want our computer to generate random samples, we would probably need it to be able to generate uniformly random bits. Let’s assume we have a black box machine that does exactly that.\n\n\n\n\n\n\nNoteNote\n\n\n\n\n\nAbove assumption is not an unrealistic one since we have hardware random bit generators which generate random bits from physical process that produce entropy.\n\n\n\nMore formally, let \\(B \\in \\{0, 1\\}\\) be a uniform random bit with \\[\\mathbb{P}[B = 0] = \\mathbb{P}[B = 1] = \\frac{1}{2}\\] and assume that we can sample from \\(B\\)."
  },
  {
    "objectID": "posts/sampling/index.html#the-uniform-distribution",
    "href": "posts/sampling/index.html#the-uniform-distribution",
    "title": "Algorithms for Sampling from a probability distribution",
    "section": "The Uniform Distribution",
    "text": "The Uniform Distribution\nLet’s start the discussion by coming up with algorithms for sampling integers uniformly randomly in a fixed range.\n\nInteger in range \\([0, 2^N]\\)\nFirst, let’s see if we can generate a uniformly random integer \\(M \\in \\{0, \\cdots, 2^N - 1\\}\\). That is, we want to come up with an algorithm to produce \\(M\\) such that \\[\\mathbb{P}[M = m] = \\frac{1}{2^N}\\] for all \\(m \\in \\{0, \\cdots, 2^N - 1\\}\\).\nWell, given that we only have a random bit generator, there is really only one thing we can do here. We should generate \\(N\\) independent bits using our random bit generator and concatenate them together to get the base 2 representation of the integer we return.\n\nAlgorithm 1\n\\(M \\gets 0\\)\nfor \\(i = 1\\) to \\(N\\) do\n\\(\\quad M \\gets M + B_i \\cdot 2^i\\)\nreturn \\(M\\)\n\nLet’s check that this gives us the desired result. Let \\(B_1, B_2, \\dots, B_N\\) be the bits we generated. Now, note that for any integer \\(k \\in \\{0, \\cdots, 2^N - 1\\}\\), there exists a unique binary representation: \\[\nk = \\sum_{i = 1}^{N} b_i \\cdot 2^i\n\\] where each \\(b_i \\in \\{0, 1\\}\\) is the \\(i\\)-th bit of \\(k\\). The probability that \\(M = k\\) is given as: \\[\\begin{align*}\n\\mathbb{P}[M = k] &= \\mathbb{P}[B_0 = b_0, B_1 = b_1, \\cdots, B_n = b_n] \\\\\n&= \\mathbb{P}[B_0 = b_0] \\cdot \\mathbb{P}[B_1 = b_1] \\cdots \\mathbb{P}[B_N = b_n] \\tag{By independence} \\\\\n&= \\frac{1}{2} \\cdot \\frac{1}{2} \\cdots \\frac{1}{2} \\tag{$N$ times} \\\\\n&= \\frac{1}{2^N}\n\\end{align*}\\] as desired.\n\n\nInteger in Arbitrary Range\nGreat, now lets move onto a slightly more difficult problem: generating a uniformly random integer in \\(\\{0, \\dots, M\\}\\), where \\(M\\) is not necessarily a power of \\(2\\).\nA first idea would be to use the same approach as before and concatenate a string of \\(N\\) random bits. This, however, does not work. For example, if \\(N = 3\\) and \\(M = 4\\) then our random bit generator produces outcomes in the range \\(\\{0, \\dots, 2^3 - 1\\} = \\{0, 1, 2, 3, 4, 5, 6, 7\\}\\). Each of these \\(8\\) outcomes occurs with probability \\(\\frac{1}{8}\\). However, we wanted an integer in the range \\(\\{0, 1, 2, 3, 4\\}\\), and the probability of each outcome to be exactly \\(\\frac{1}{5}\\).\nAnother approach would be to generate \\(N\\) random bits as before, and then return \\(N \\mod (M + 1)\\). I leave it as an exercise to figure out why this approach also does not work (Hint: use the same counter example from above).\nNotice that the issue in the above two approach is that our algorithm considers values that are outside the given range. To fix this problem, one idea would be to simply ignore an integers that are outside the range. More formally, define the algorithm as\n\nAlgorithm 2\n\\(P \\gets \\lceil \\log_2 (M + 1) \\rceil\\)\nrepeat\n\\(\\quad\\) Generate \\(N \\in \\{0, \\cdots, 2^P - 1\\}\\) using Algorithm 1\nuntil \\(N \\leq M\\)\nreturn \\(N\\)\n\nThis algorithm is called rejection sampling. A subtle difference between this algorithm and the previous algorithm is that its running time is non-deterministic. For example, we know that algorithm 1 terminates after generating \\(N\\) random bits. Algorithm 2, however, has no such guarantees. It is technically possible that we get really really really unlucky and always generate \\(N &gt; M\\) and so the algorithm would never return a number.\nSo, let’s compute probability that the algorithm terminates, which only happens when \\(N \\leq M\\) \\[\\begin{align*}\n\\mathbb{P}[N \\leq M] &= \\sum_{i \\leq M} \\mathbb{P}[N = i] \\tag{CDF of $N$} \\\\\n&= \\sum_{i = 0}^M \\frac{1}{2^N} \\tag{From Algorithm 1} \\\\\n&= \\frac{(M + 1)}{2^N}\n\\end{align*}\\]\nNote that if \\(M \\ll N\\) then the exponential in the denominator dominates, and the probability of terminating is near zero. When \\(M \\gg N\\) then the fraction is approximately linear, and so the algorithm terminates with higher probability.\nHowever, in the case that the algorithm does return, then we can be sure that it is correct. We prove this below. Note that for any \\(k \\in \\{0, \\cdots, M\\}\\), we have \\[\\begin{align*}\n\\mathbb{P}[N = k] &= \\mathbb{P}[N = k | N \\leq M] \\tag{Since $N \\leq M$ iff $N$ is returned} \\\\\n&= \\frac{\\mathbb{P}[N = k, N \\leq M]}{\\mathbb{P}[N \\leq M]} \\tag{Definition of conditional prob.} \\\\\n&= \\frac{\\mathbb{P}[N = k]}{\\mathbb{P}[N \\leq M]} \\tag{$k \\leq M$ so if $N = k$, we know $N \\leq M$} \\\\\n&= \\frac{1/2^N}{(M+1)/2^N} \\\\\n&= \\frac{1}{M + 1}\n\\end{align*}\\] as desired.\n\n\nReal number in range \\([0, 1]\\)\nNow let’s move onto the hardest problem yet: sampling a real number in the range \\([0, 1]\\). Note that every real number \\(r \\in [0, 1]\\) can be represented as a binary expansion: \\[\nr = \\sum_{i=1}^{\\infty} \\frac{B_i}{2^i} = 0.B_1B_2B_3\\dots\n\\] where each \\(B_i \\in \\{0, 1\\}\\). Again, our first instinct should be to ask if generating an infinite string of bits and concatenating them gives us the desired result. In this case, it turns out to be true. However, the proof of this fact is non-trivial and takes quite a lot of work.\n\n\n\n\n\n\nImportantMain Claim\n\n\n\nIf \\(B_1, B_2, B_3, \\dots\\) are independent uniform random bits, then \\[\nU = \\sum_{i=1}^{\\infty} \\frac{B_i}{2^i}\n\\] is uniformly distributed on \\([0, 1]\\).\n\n\nTo prove this, we need to show that for any interval \\([a, b) \\subseteq [0, 1]\\), we have \\[\n\\mathbb{P}[U \\in [a, b)] = \\text{length}([a, b)]) = b - a\n\\]\n\nPartitioning \\([0, 1]\\) and characterizing the partitions\nThe proof uses a clever partitioning argument. We’ll divide \\([0, 1]\\) into dyadic intervals, which are defined to be intervals whose endpoints are fractions with powers of 2 in the denominator.\nFor any arbitrary \\(k \\in \\mathbb{N}\\), we can partition \\([0, 1]\\) into \\(2^k\\) equal intervals: \\[\nI_j = \\left[\\frac{j}{2^k}, \\frac{j+1}{2^k}\\right) \\quad \\text{for } j = 0, 1, \\dots, 2^k - 1\n\\]\nEach interval has length \\(\\frac{1}{2^k}\\), and together they cover the entire unit interval. The larger \\(k\\), the better this approximation gets.\nNow, I claim that the first \\(k\\) bits \\((B_1, \\dots, B_k)\\) of \\(U\\) completely determine which dyadic interval \\(I_j\\) contains the value \\(U\\). Specifically: \\[\nU \\in I_j = \\left[\\frac{j}{2^k}, \\frac{j+1}{2^k}\\right) \\iff \\sum_{i=1}^{k} \\frac{B_i}{2^{i}} = \\frac{j}{2^k}\n\\]\nTo see why, we can decompose \\(U\\) into two parts:\n\\[\nU = \\underbrace{\\sum_{i=1}^{k} \\frac{B_i}{2^i}}_{:=U_k} + \\underbrace{\\sum_{i=k+1}^{\\infty}\\frac{B_i}{2^i}}_{:=R_k}\n\\] where \\(U_k\\) represents the first \\(k\\) bits and \\(R_k\\) represents the remaining bits. Now, note that The first \\(k\\) bits can only produce discrete values because there are \\(2^k\\) possible bit strings of length \\(k\\), and these values are \\[\\left\\{0, \\frac{1}{2^k}, \\frac{2}{2^k}, \\ldots, \\frac{2^k-1}{2^k}\\right\\}\\] Observe that these are exactly the left endpoints of our dyadic intervals \\(I_j\\).\nNow, The remaining bits \\(R_k\\) can be rewritten by factoring out \\(\\frac{1}{2^k}\\): \\[\\begin{align*}\nR_k &= \\sum_{i=k+1}^{\\infty} \\frac{B_i}{2^i} = \\frac{1}{2^k} \\sum_{i=1}^{\\infty} \\frac{B_{k+i}}{2^i}\n\\end{align*}\\] Since \\(\\sum_{i=1}^{\\infty} \\frac{B_{k+i}}{2^i}\\) is a binary expansion taking values in \\([0, 1)\\), we have \\[0 \\leq R_k &lt; \\frac{1}{2^k}\\] So, we have that \\(R_k\\) is strictly less than the width of a dyadic interval.\nTherefore, we can now claim that if \\(U_k = \\frac{j}{2^k}\\), then \\(U = \\frac{j}{2^k} + R_k\\) must satisfy \\[\\frac{j}{2^k} \\leq U &lt; \\frac{j+1}{2^k}\\] placing \\(U\\) in interval \\(I_j\\). Conversely, if \\(U \\in I_j\\), the constraint \\[\\frac{j}{2^k} \\leq U &lt; \\frac{j+1}{2^k}\\] combined with the fact that \\(U_k\\) is a discrete value in this range forces \\(U_k = \\frac{j}{2^k}\\). Therefore, we have shown that\n\n\n\n\n\n\n\\[\nU \\in I_j \\iff \\sum_{i = 1}^k \\frac{B_i}{2^i} = \\frac{j}{2^k}\n\\]\n\n\n\n\n\nProbability of landing in \\(I_j\\)\nNow we can calculate the probability that \\(U\\) lands in a specific dyadic interval \\(I_j\\):\n\\[\\begin{align*}\n\\mathbb{P}[U \\in I_j] &= \\mathbb{P}\\left[\\sum_{i=1}^{k} \\frac{B_i}{2^{i}} = \\frac{j}{2^k}\\right] \\\\\n&= \\mathbb{P}[B_1 = b_1, B_2 = b_2, \\dots, B_k = b_k]\n\\end{align*}\\]\nwhere \\((b_1, \\dots, b_k)\\) is the binary representation of \\(j\\).\nSince the bits are independent and each has probability \\(\\frac{1}{2}\\):\n\\[\\begin{align*}\n\\mathbb{P}[U \\in I_j] &= \\prod_{i=1}^{k} \\mathbb{P}[B_i = b_i] \\\\\n&= \\prod_{i=1}^{k} \\frac{1}{2} \\\\\n&= \\frac{1}{2^k}\n\\end{align*}\\]\nNote that this is exactly the length of the interval \\(I_j\\)! Therefore, we have show that at least for dyadic intervals, \\(U\\) is uniformly distributed!\n\n\nExtending to General Intervals\n\n\n\n\n\n\nNoteTechnical Note\n\n\n\n\n\nExtending this to the general interval \\([a, b]\\) is complicated and requires careful machinery taught usually in a course in real analysis. Therefore, will handwave a lot of the technicalities, but do keep in mind that there is more careful argument to be made here.\n\n\n\nFor a general interval \\([a, b) \\subseteq [0, 1]\\), we approximate it using dyadic intervals. Let \\(J_k\\) be the set of indices where \\(I_j \\subseteq [a, b)\\). Then:\n\\[\n[a, b) \\approx \\bigcup_{j \\in J_k} I_j\n\\]\nAs \\(k\\) increases, the dyadic intervals become finer, and this approximation improves.\nSince the dyadic intervals are disjoint:\n\\[\\begin{align*}\n\\mathbb{P}\\left[U \\in \\bigcup_{j \\in J_k} I_j\\right] &= \\sum_{j \\in J_k} \\mathbb{P}[U \\in I_j] \\\\\n&= \\sum_{j \\in J_k} \\frac{1}{2^k} \\\\\n&= \\frac{|J_k|}{2^k}\n\\end{align*}\\]\nwhere \\(|J_k|\\) is the number of dyadic intervals that fit inside \\([a, b)\\).\n\n\nTaking the Limit\nNotice that \\(\\frac{|J_k|}{2^k}\\) represents the total length of all dyadic intervals contained in \\([a, b)\\):\n\\[\n\\frac{|J_k|}{2^k} = \\sum_{j \\in J_k} \\text{length}(I_j)\n\\]\nAs \\(k \\to \\infty\\), these intervals cover \\([a, b)\\) with increasing precision, so we expect\n\\[\n\\lim_{k \\to \\infty} \\frac{|J_k|}{2^k} = b - a\n\\]\nTherefore:\n\\[\\begin{align*}\n\\mathbb{P}[U \\in [a, b)] &= \\lim_{k \\to \\infty} \\mathbb{P}\\left[U \\in \\bigcup_{j \\in J_k} I_j\\right] \\\\\n&= \\lim_{k \\to \\infty} \\frac{|J_k|}{2^k} \\\\\n&= b - a \\\\\n&= \\text{length}([a, b))\n\\end{align*}\\]\nas desired\n\n\nFrom Theory to Practice: Finite Bit Algorithms\nThe theoretical result above is beautiful, but it has a glaring practical issue: we cannot generate an infinite sequence of bits! In the real world, we need to work with a finite number of bits. Fortunately, our proof gives us a natural way to approximate the uniform distribution using only a finite number of bits.\nThe key insight is that after generating \\(N\\) bits, we’ve already determined which of the \\(2^N\\) dyadic intervals our value falls into. The remaining (ungenerated) bits would only refine our position within that interval, which has width \\(\\frac{1}{2^N}\\). For large \\(N\\), this interval is so small that the difference is negligible\nThis leads us to the following algorithm:\n\nAlgorithm 3\n\\(U \\gets 0\\)\nfor \\(i = 1\\) to \\(N\\) do\n\\(\\quad\\) Generate random bit \\(B_i\\)\n\\(\\quad\\) \\(U \\gets U + B_i \\cdot 2^{-i}\\)\nreturn \\(U\\)\n\nThis algorithm generates values of the form \\(\\frac{k}{2^N}\\) where \\(k \\in \\{0, 1, \\ldots, 2^N - 1\\}\\). In other words, it produces a discrete uniform distribution over \\(2^N\\) equally-spaced points in \\([0, 1]\\), rather than a truly continuous uniform distribution.\nLet \\(U_N\\) denote the output of Algorithm 3 and let \\(U_{\\text{true}}\\) denote a truly uniform random variable on \\([0, 1]\\). How far off is our approximation? Well the maximum distance between any generated value and its “true” counterpart is at most the granularity of our discretization: \\[\n\\max_{x \\in [0, 1]} |U_N - x| \\leq \\frac{1}{2^N}\n\\] So, if \\(N = 64\\), then we have: \\[\\begin{align*}\n\\frac{1}{2^{64}} &\\approx 5.42 \\times 10^{-20} \\\\\n\\end{align*}\\]\nThis means our approximation error is about \\(10^{-20}\\)! For all practical purposes, this is indistinguishable from a true uniform distribution. In fact, 64-bit floating point numbers (the standard in most programming languages) have a precision of about \\(10^{-16}\\), which is less precise than our sampling error.\n\n\n\n\n\n\nNotePractical Implementation\n\n\n\nIn practice, most programming languages use 64-bit floating point numbers for representing real numbers. Algorithm 3 with \\(N = 64\\) bits produces values that are uniformly distributed on the set of representable floating point numbers in \\([0, 1]\\), which is the best we can do given the finite precision of computer arithmetic.\n\n\nTherefore, Algorithm 3 with \\(N = 64\\) bits gives us an excellent practical algorithm for sampling (approximately) uniform real numbers in \\([0, 1]\\)."
  },
  {
    "objectID": "posts/sampling/index.html#arbitrary-probability-distribution",
    "href": "posts/sampling/index.html#arbitrary-probability-distribution",
    "title": "Algorithms for Sampling from a probability distribution",
    "section": "Arbitrary Probability Distribution",
    "text": "Arbitrary Probability Distribution\nThat was a lot of work! If it took so much effort just to sample from the uniform distribution, how can we possibly expect to come up with algorithms for complicated probability distributions? Well, it turns out that with not too much extra work we can sample from any arbitrary distribution \\(\\pi\\). More formally, lets say \\(\\pi\\) is a probability distribution on a finite state space \\(\\chi = \\{x_1, \\cdots x_n\\}\\). I want to generate an \\(X \\in \\chi\\) such that \\[\n\\mathbb{P}[X = x] = \\pi(x)\n\\] Now that we can sample from the uniform distribution on \\([0, 1]\\), let’s use that. We will use another clever partitioning technique, similar to what we did for proving the uniform distribution result.\nMore specifically, we divide the interval \\([0, 1]\\) into \\(N\\) consecutive segments. For each \\(i \\in \\{1, 2, \\ldots, N\\}\\), define segment \\(i\\) as: \\[\nS_i = \\left[\\sum_{j=1}^{i-1} \\pi(x_j), \\sum_{j=1}^{i} \\pi(x_j)\\right)\n\\] where by convention, \\(\\sum_{j=1}^{0} \\pi(x_j) = 0\\).\nObserve that segment \\(S_i\\) has length: \\[\n\\text{length}(S_i) = \\sum_{j=1}^{i} \\pi(x_j) - \\sum_{j=1}^{i-1} \\pi(x_j) = \\pi(x_i)\n\\]\nMoreover, these segments partition \\([0, 1]\\) since they are disjoint and: \\[\n\\bigcup_{i=1}^{N} S_i = [0, 1)\n\\] which follows from the fact that \\(\\sum_{i=1}^{N} \\pi(x_i) = 1\\) (since \\(\\pi\\) is a probability distribution). When we sample uniformly from \\([0, 1]\\), the probability of landing in segment \\(i\\) is exactly \\(\\pi(x_i)\\) (since the segment has length \\(\\pi(x_i)).\\) So if we return \\(x_i\\) whenever we land in segment \\(i\\), we get the desired distribution!\n\n\n\n\n\n\nTipIntuition\n\n\n\nThink of it like a dartboard: if you throw a dart uniformly at random on \\([0,1]\\), the probability of hitting a region is proportional to its length. By making segment \\(i\\) have length \\(\\pi(x_i)\\), we ensure the probability of hitting it matches our target probability.\n\n\nExample: Suppose \\(\\mathcal{X} = \\{x_1, x_2, x_3, x_4, x_5\\}\\) and \\(\\pi = (0.15, 0.25, 0.20, 0.30, 0.10)\\). We segment the \\([0, 1]\\) interval as shown below:\n\n\n\nSegmented [0, 1] Interval\n\n\nNow, we can sample a real number uniformly random, and return the segment that this number belongs to.\n\nFormalizing the Algorithm\nTo implement this efficiently, we use cumulative probabilities. Define: \\[\nF_i = \\sum_{j=1}^{i} \\pi(x_j) \\quad \\text{for } i = 1, 2, \\ldots, N\n\\] with \\(F_0 = 0\\). Note that \\(F_i\\) represents the cumulative probability up to and including \\(x_i\\), so segment \\(i\\) corresponds to the interval \\([F_{i-1}, F_i)\\).\nOur algorithm returns \\(x_i\\) when \\(U\\) falls in the interval \\([F_{i-1}, F_i)\\):\n\nAlgorithm 4\nPreprocessing:\nCompute cumulative probabilities: \\(F_0 \\gets 0\\)\nfor \\(i = 1\\) to \\(N\\) do\n\\(\\quad\\) \\(F_i \\gets F_{i-1} + \\pi(x_i)\\)\nSampling:\nGenerate \\(U \\sim \\text{Uniform}[0, 1]\\) using Algorithm 3\nfor \\(i = 1\\) to \\(N\\) do\n\\(\\quad\\) if \\(F_{i-1} \\leq U &lt; F_i\\) then\n\\(\\quad\\quad\\) return \\(x_i\\)\n\nLet’s verify that this algorithm produces the correct distribution. For any \\(x_i \\in \\mathcal{X}\\):\n\\[\\begin{align*}\n\\mathbb{P}[\\text{output} = x_i] &= \\mathbb{P}[F_{i-1} \\leq U &lt; F_i] \\\\\n&= F_i - F_{i-1} \\tag{$U$ is uniform on $[0,1]$} \\\\\n&= \\left(\\sum_{j=1}^{i} \\pi(x_j)\\right) - \\left(\\sum_{j=1}^{i-1} \\pi(x_j)\\right) \\\\\n&= \\pi(x_i)\n\\end{align*}\\]\nThe second equality uses the fact that for a uniform random variable \\(U\\) on \\([0,1]\\), we have \\(\\mathbb{P}[a \\leq U &lt; b] = b - a\\). Therefore, our algorithm produces samples distributed according to \\(\\pi\\). Problem solved!\n\n\n\n\n\n\nNoteComputational Complexity\n\n\n\nThe preprocessing step takes \\(O(N)\\) time to compute cumulative probabilities. Each sample then requires \\(O(N)\\) time in the worst case to find which segment \\(U\\) falls into (via linear search). This can be improved to \\(O(\\log N)\\) per sample using binary search, since the cumulative probabilities \\(F_i\\) are sorted."
  },
  {
    "objectID": "posts/markov-chains/index.html",
    "href": "posts/markov-chains/index.html",
    "title": "PageRank and Markov Chains",
    "section": "",
    "text": "Markov Chains are fundamental mathematical models for sequence of random events, where the probability of the next event depends only on the current event, not the entire past history. These models revolutionized computer science, economics, and even bioinformatics. In this post, I will cover much of the theory required to study these models. I will limit my discussion to mostly finite state space markov chains, but the proofs can be extended for the infinite state space as well."
  },
  {
    "objectID": "posts/markov-chains/index.html#introduction",
    "href": "posts/markov-chains/index.html#introduction",
    "title": "PageRank and Markov Chains",
    "section": "",
    "text": "Markov Chains are fundamental mathematical models for sequence of random events, where the probability of the next event depends only on the current event, not the entire past history. These models revolutionized computer science, economics, and even bioinformatics. In this post, I will cover much of the theory required to study these models. I will limit my discussion to mostly finite state space markov chains, but the proofs can be extended for the infinite state space as well."
  },
  {
    "objectID": "posts/markov-chains/index.html#motivating-example",
    "href": "posts/markov-chains/index.html#motivating-example",
    "title": "PageRank and Markov Chains",
    "section": "Motivating Example",
    "text": "Motivating Example\nImagine you’re Larry Page in 1996 researching a topic, say “jaguars.” There is a new database of information on the rise - the world wide web - so you decided to start there. You search through hundreds of thousands of web pages and find thousands that mention “jaguar”. However, you find pages about the animal, the car, the football team, a rock band, and random pages that just happen to use the word. You can’t manually read through all these pages to find the best ones.\nAmong all these pages that match your keywords, you want to automatically rank them in order of relative importance so you can start your search. Let’s model this formally: Suppose you have \\(N\\) webpages, interconnected via hyperlinks. A subset of these pages are relevant to your query. How do you rank this subset by importance?\nAn obvious thing to do would be to randomly pick, say the first 5 pages, from the subset and read them. The issue with this approach is that you could get really unlucky and pick 5 pages which are totally unrelated to your topic, like obscure pages that barely mention jaguars. But maybe using randomness wasn’t the problem here. Maybe the problem was that you were picking uniformly at random, treating all pages as equally likely to be useful. What if instead, you used randomness in a smarter way?\nImagine you’re browsing the web, starting from one of the pages that mentions “jaguar.” You read the page, then randomly click on one of its outbound links, moving to a new page. You repeat this process, following the web’s hyperlinks. But you don’t browse forever in a single path. Occasionally, say with probability \\(\\alpha\\) you get bored, close your browser, and jump to a completely random page in the subset to start exploring again. If you simulate this process for a long time, you will find that the surfer visits certain pages much more frequently than others. For example, you will find that pages with many incoming links (or links from other “popular” pages) will be visited more often.\nThe “importance” of a page can be defined as the long-run proportion of time this random surfer spends on that page. This simple heuristic is the core logic behind PageRank, the algorithm that powered Google’s initial success. We will see that the random surfer model is also a Markov model, and that the long-run proportion of time spent by the random surfer corresponds to the stationary distribution.\nTo get more intuition for how this works, I’ve written a simulation below. Each node in the graph represents a webpage (Notated by “Pg n” where n is the page number). A directed edge \\(i \\to j\\) represents a hyperlink in webpage \\(i\\) pointing to webpage \\(j\\). Initially, the surfer starts out on a random page. As the simulation progresses, we track the number of times the surfer visited each page, and redraw its size as a function of the relative visits. In the end, the bigger a node is, the more it was visited by the surfer (and therefore, the more “important” it is).\n\n  \n\nA question you might now ask is, will this random surfer always produce the same long term behavior? That is, is the heuristic we use to define the “importance” of a page going to change between each run of the random surfer? If it does, then this algorithm would not be any good, since we were looking for a definitive ranking of the importance of webpages.\nTo answer this question, we need to study Markov chains and the stationary distribution more rigorously."
  },
  {
    "objectID": "posts/markov-chains/index.html#markov-chains",
    "href": "posts/markov-chains/index.html#markov-chains",
    "title": "PageRank and Markov Chains",
    "section": "Markov Chains",
    "text": "Markov Chains\nLet’s start with the definition of a Markov Chain.\n\n\n\n\n\n\nNoteDefinition 1\n\n\n\nA Markov chain on a state space \\(\\chi\\) is a family of random variables \\(X_0, X_1, \\cdots\\) such that for all \\(n \\in \\mathbb{N}\\), and \\(x_0, \\cdots, x_n, x_{n+1} \\in \\chi\\) we have \\[\n\\mathbb{P}[X_{n+1} = x_{n+1} \\mid \\{X_k = x_k \\mid 0 \\leq k \\leq n\\}] = \\mathbb{P}[X_{n+1} = x_{n+1} \\mid X_n = x_n]\n\\]\n\n\nThe above property is also known as the Markov Property. A time homogeneous Markov chain is a Markov chain where \\[\n\\mathbb{P}[X_{n+1} = y \\mid X_n = x] = \\mathbb{P}[X_{1} = y \\mid X_0 = x]\n\\] for all \\(x, y \\in \\chi\\) and \\(n \\in \\mathbb{N}\\). In the remaining of the discussion we will only consider time homogeneous Markov chains because they can be represented as a single transition matrix, not dependent on time \\[\nP(x, y) := \\mathbb{P}[X_1 = y \\mid X_0 = x]\n\\] where \\(x, y\\) are the indices into the matrix \\(P\\). This seems a bit silly, but reframing the problem this way allows us to use powerful tools/techniques from linear algebra. Using purely probabilistic methods when working with Markov chains is often very ugly, and the matrix formulation is preferred.\n\n\n\n\n\n\nNoneExample\n\n\n\nLet’s try to come up with a transition matrix for the random web surfer Markov Chain we discussed above. Suppose that our state space is the set of all \\(N\\) webpages that are relevant to our query. So, \\(\\chi = \\{1, 2, \\cdots, N \\}\\). For each page \\(i \\in \\chi\\), let \\(d_i\\) denote the number of outbound links from that page. We allow our surfer to follow a random outbound link from the current page with probability \\((1 - \\alpha)\\). We also want the surfer to pick a uniformly random page from all \\(N\\) pages and go there with probability \\(\\alpha\\).\nTherefore, the transition probability from page \\(i\\) to page \\(j\\) is: \\[\nP(i, j) = \\begin{cases} (1- \\alpha) \\cdot \\frac{1}{d_i} + \\frac{\\alpha}{N} && \\text{ if there exists a link from } i \\to j \\\\ \\frac{\\alpha}{N} && \\text{ otherwise} \\end{cases}\n\\]\n\n\nLet’s prove some basic propositions about this new object we defined.\n\n\n\n\n\n\nTipProposition 1\n\n\n\nFor any \\(x_0, \\cdots, x_n \\in \\chi\\), \\[\n\\mathbb{P}[X_n = y \\mid X_0 = x] = P^n (x, y)\n\\] where \\(P^n\\) means the nth power of the matrix \\(P\\).\n\n\nProof: We proceed by induction on \\(n\\). The base cases \\(n=1\\) follows by definition. So assume that the result holds for \\(n\\). We need to show it holds for \\(n+1\\). We first condition on \\(X_n\\): \\[\n\\begin{flalign}\n\\mathbb{P}[X_{n + 1} = y \\mid X_0 = x] &= \\sum_{z \\in \\chi} \\mathbb{P}[X_{n+1} = y \\mid X_n = z, X_0 = x] \\cdot \\mathbb{P}[X_n = z \\mid X_0 = x] &&\\\\\n&= \\sum_{z \\in \\chi} \\mathbb{P}[X_{n+1} = y \\mid X_n = z] \\cdot \\mathbb{P}[X_n = z \\mid X_0 = x] \\tag{Markov Property} &&\\\\\n&= \\sum_{z \\in \\chi} P(z, y) \\cdot \\mathbb{P}[X_n = z \\mid X_0 = x] \\tag{Definition of $P$} &&\\\\\n&= \\sum_{z \\in \\chi} P(z, y) \\cdot P^n (x, z) \\tag{Induction hypothesis} &&\\\\\n&= P^{n+1} (x, y) \\tag{Matrix mult.}\n\\end{flalign}\n\\] as desired.\nNow, we can characterize how a Markov chain \\(X_n\\) with transition matrix \\(P\\) and initial distribution \\(\\mu_0\\) over time.\n\n\n\n\n\n\nTipProposition 2\n\n\n\nIf \\(X_0 \\sim \\mu_0\\) (i.e., \\(\\mathbb{P}[X_0 = x] = \\mu_0 (x)\\) for all \\(x \\in \\chi\\)) then \\(X_n \\sim \\mu_0 P^n\\) (taken as a matrix product)\n\n\nProof: We need to show that \\(\\mathbb{P}[X_n = y] = (\\mu_0 P^n)(y)\\) for all \\(y \\in \\chi\\).\nWe condition on the initial state \\(X_0\\):\n\\[\\begin{align*}\n\\mathbb{P}[X_n = y] &= \\sum_{x \\in \\chi} \\mathbb{P}[X_n = y \\mid X_0 = x] \\cdot \\mathbb{P}[X_0 = x] \\tag{Law of total probability} \\\\\n&= \\sum_{x \\in \\chi} P^n(x, y) \\cdot \\mathbb{P}[X_0 = x] \\tag{Proposition 1} \\\\\n&= \\sum_{x \\in \\chi} P^n(x, y) \\cdot \\mu_0(x) \\tag{Definition of $\\mu_0$} \\\\\n&= \\sum_{x \\in \\chi} \\mu_0(x) \\cdot P^n(x, y)  \\\\\n&= (\\mu_0 P^n)(y) \\tag{Matrix mult.}\n\\end{align*}\\]\nas desired."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "As each new author finds cleverer proofs or treatments of a theory, the treatment evolves towards the one that contains the ‘shortest proofs’. Unfortunately, these are often in a form that causes the new student to ponder ‘How did anyone think this?’ By going back to the original sources, one can usually see the subject evolving naturally.” — Peter Sarnak\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPageRank and Markov Chains\n\n\n\nmath\n\ncs\n\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithms for Sampling from a probability distribution\n\n\n\nmath\n\ncs\n\n\n\n\n\n\n\n\n\nOct 8, 2025\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Differentiation using Dual Numbers\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nApr 8, 2025\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nSending Messages over Noisy Channels\n\n\n\nmath\n\nece\n\n\n\n\n\n\n\n\n\nDec 25, 2024\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nMetric Spaces, dimension reduction, and Bourgain Embeddings\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\nKrish Suraparaju\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/markov-chains/index.html#the-stationary-distribution",
    "href": "posts/markov-chains/index.html#the-stationary-distribution",
    "title": "PageRank and Markov Chains",
    "section": "The Stationary Distribution",
    "text": "The Stationary Distribution\nLet’s return to our PageRank example. Notice that the surfer never stops exploring the web. They keep browsing indefinitely, either following links or opening up new pages in a browser. This raises a natural question: What happens to the distribution of the surfer’s location after a very long time?\nFrom Proposition 2, we can come up with the recurrence relation: \\[\n\\mu_{n + 1} = \\mu_0 P^{n + 1} = (\\mu_0 P^n) P = \\mu_n P\n\\] As \\(n \\to \\infty\\), does this converge to anything (i.e., does there exists a limiting distribution)? And if so, does the limiting distribution depend on where we started \\(\\mu_0\\)?\nOften, a good way to answer questions like these is to assume something about the statement and see what happens. So, lets assume that there exists a limiting distribution \\(\\pi\\) such that \\(\\mu_n \\to \\pi\\) as \\(n \\to \\infty\\). This leads us to proposition 3:\n\n\n\n\n\n\nTipProposition 3\n\n\n\nIf \\(\\pi\\) is a limiting distribution of the Markov chian \\(P\\), then \\[\\pi = \\pi P\\]\n\n\nProof: We take limits on both sides of our recurrence relation: \\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mu_{n + 1} &= \\lim_{n \\to \\infty}(\\mu_n P) \\\\\n\\lim_{n \\to \\infty} \\mu_{n + 1} &= (\\lim_{n \\to \\infty} \\mu_n) P   \\tag{Matrix mul is continuous}\\\\\n\\pi &= \\pi P   \\tag{Since $\\mu_n \\to \\pi$}\\\\\n\\end{align*}\\]\nThat’s a pretty interesting result! Any limiting distribution \\(\\pi\\) of a Markov chain (if it exists) is such that \\(\\pi = \\pi P\\). This special kind of distribution is known as stationary distribution. Let’s see if we can use this property to help us answer our original question. Specifically, is the converse of proposition 3 true? Is it the case that any distribution \\(\\pi\\) such that \\(\\pi = \\pi P\\) must be a limiting distribution? If it was, then we’d be done! We would have found a way to completely characterize what the limiting distribution of a Markov Chain looks like.\nBut sadly, that is not the case.\n\nA Counter Example\nConsider a Markov chain on \\(\\chi = \\{0, 1, 2, 3\\}\\) with transition matrix \\[\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 \\\\\n0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & 0 & 1\\\\\n\\end{pmatrix}\n\\]\nNotice that the distribution \\(\\pi_1 = \\begin{pmatrix} \\sqrt{2} & 0 & 0 & \\sqrt{2} \\end{pmatrix}\\) (read as a \\(1 \\times 4\\) row matrix) of \\(P\\) has the property discussed in proposition 3 because:\n\\[\n\\begin{align*}\n\\pi_1 P &= \\begin{pmatrix} \\sqrt{2} & 0 & 0 & \\sqrt{2} \\end{pmatrix}  \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 \\\\\n0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & 0 & 1\\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\sqrt{2} & 0 & 0 & \\sqrt{2} \\end{pmatrix} \\\\\n&= \\pi_1\n\\end{align*}\n\\]\nAdditionally, the distribution \\(\\pi_2 = \\begin{pmatrix} e & 0 & 0 & e \\end{pmatrix}\\) of \\(P\\) also has the property discussed in proposition 3 because:\n\\[\n\\begin{align*}\n\\pi_2 P &= \\begin{pmatrix} e & 0 & 0 & e\\end{pmatrix}  \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 \\\\\n0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & 0 & 1\\\\\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix} e & 0 & 0 & e \\end{pmatrix} \\\\\n&= \\pi_2\n\\end{align*}\n\\]\nThis is a problem because if the converse of proposition 3 were true, then any distribution that is stationary would be the limiting distribution of \\(P\\). But we’ve just shown that both \\(\\pi_1\\) and \\(\\pi_2\\) are stationary distributions, and they are different! A limiting distribution must be unique (a Markov chain can’t converge to two different distributions at the same time), so this shows that not every stationary distribution is a limiting distribution.\n\n\n\n\n\n\nNoneRemark\n\n\n\nYou may be wondering how I came up with the two different stationary distribution for \\(P\\). Notice that we can treat \\(\\pi\\) as a row-vector. In linear algebra terms, the stationary property means that \\(\\pi\\) is a left-eigenvector of \\(P\\) with eigenvalue \\(1\\). So, one way to find a stationary distribution is to compute a left-eigen vector with eigen-value \\(1\\) of \\(P\\). Of course, not all left-eigenvectors of \\(P\\) will be a valid probability distribution (some may have negative entries, or worse, complex entires). So you need to be careful when computing eigenvectors and treating them as probability distributions."
  },
  {
    "objectID": "posts/markov-chains/index.html#definition-1-1",
    "href": "posts/markov-chains/index.html#definition-1-1",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Definition 1",
    "text": "Definition 1\nA Markov chain on a state space \\(\\chi\\) is a family of random variables \\(X_0, X_1, \\cdots\\) such that for all \\(n \\in \\mathbb{N}\\), and \\(x_0, \\cdots, x_n, x_{n+1} \\in \\chi\\) we have \\[\n\\mathbb{P}[X_{n+1} = x_{n+1} \\mid \\{X_k = x_k \\mid 0 \\leq k \\leq n\\}] = \\mathbb{P}[X_{n+1} = x_{n+1} \\mid X_n = x_n]\n\\] :::"
  },
  {
    "objectID": "posts/markov-chains/index.html#definition-2",
    "href": "posts/markov-chains/index.html#definition-2",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Definition 2",
    "text": "Definition 2\nLet \\(P\\) be the transition matrix of a Markov chain on a state space \\(\\chi\\). A distribution \\(\\pi\\) is a stationary distibution for the chain \\(P\\) iff \\[\\begin{align*}\n\\pi P = \\pi &&  \\text{(read as a matrix product)}\n\\end{align*}\\]\n:::"
  },
  {
    "objectID": "posts/markov-chains/index.html#proposition-3",
    "href": "posts/markov-chains/index.html#proposition-3",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Proposition 3",
    "text": "Proposition 3\nIf a Markov chain with transition matrix \\(P\\) has a limiting distribution, then that limiting distribution must be stationary. :::\nProof: Suppose that a limiting distribution \\(\\pi\\) exists. By definition, this means that for any initial distribution \\(\\mu_0\\), we have \\[\n\\lim_{n \\to \\infty} \\mu_0 P^n = \\pi\n\\]\nNow consider the distribution at time \\(n+1\\). We can write this in two equivalent ways using Proposition 2: \\[\n\\mu_0 P^{n+1} = \\mu_0 P^n \\cdot P\n\\]\nTaking the limit as \\(n \\to \\infty\\) on both sides:\n\\[\\begin{align*}\n\\lim_{n \\to \\infty} \\mu_0 P^{n+1} &= \\lim_{n \\to \\infty} (\\mu_0 P^n \\cdot P) \\\\\n\\pi &= \\left(\\lim_{n \\to \\infty} \\mu_0 P^n \\right) \\cdot P \\tag{Continuity of matrix mult.} \\\\\n\\pi &= \\pi \\cdot P \\tag{Definition of $\\pi$} \\\\\n\\pi &= \\pi P\n\\end{align*}\\]\nTherefore, \\(\\pi\\) satisfies the stationary distribution equation \\(\\pi P = \\pi\\), so \\(\\pi\\) is a stationary distribution.\n\n\n\n\n\n\nImportantRemark\n\n\n\nThis proposition shows that stationarity is a necessary condition for being a limiting distribution. However, the converse is not true: a Markov chain can have a stationary distribution without having a limiting distribution (e.g., the periodic chain that alternates between two states). The conditions under which a stationary distribution becomes a limiting distribution involve irreducibility, aperiodicity, and positive recurrence, which we will explore later.\n\n\n\n\n\n\n\n\nNoteDefinition 3\n\n\n\nLet \\(P\\) be the transition matrix of a Markov chain. \\(P\\) is said to be irreducible if for any \\(x, y \\in \\chi\\) there exists an \\(n\\) (possibly dependent on \\(x, y\\)) such that \\[\nP^n (x, y) &gt; 0\n\\]"
  },
  {
    "objectID": "posts/markov-chains/index.html#when-does-convergence-happen",
    "href": "posts/markov-chains/index.html#when-does-convergence-happen",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "When Does Convergence Happen?",
    "text": "When Does Convergence Happen?\nWe’ve seen that a stationary distribution can exist without the chain converging to it. This leads to a fundamental question: Under what conditions does \\(\\mu_0 P^n\\) converge to the stationary distribution \\(\\pi\\) as \\(n \\to \\infty\\)?\nTo answer this, we need to understand two key properties of Markov chains: irreducibility and aperiodicity.\n\nIrreducibility\nIntuitively, irreducibility means that from any state, you can eventually reach any other state. The chain doesn’t get “stuck” in a subset of states.\n\n\n\n\n\n\nNoteDefinition 3\n\n\n\nA state \\(j\\) is accessible from state \\(i\\) (written \\(i \\to j\\)) if there exists some \\(n \\geq 0\\) such that \\(P^n(i, j) &gt; 0\\). In other words, there is a positive probability of reaching \\(j\\) from \\(i\\) in exactly \\(n\\) steps.\nA Markov chain is irreducible if every state is accessible from every other state. That is, for all \\(i, j \\in \\chi\\), there exists some \\(n \\geq 0\\) such that \\(P^n(i, j) &gt; 0\\).\n\n\n\n\n\n\n\n\nNoneExample: Reducible Chain\n\n\n\nConsider the transition matrix: \\[\nP = \\begin{pmatrix}\n0.5 & 0.5 & 0 \\\\\n0.5 & 0.5 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\]\nThis chain is not irreducible. State 2 is absorbing - once you reach it, you can never leave. You can never reach state 2 from states 0 or 1, so the chain breaks into separate components.\n\n\n\n\n\n\n\n\nNoneExample: PageRank is Irreducible\n\n\n\nRecall our PageRank transition matrix: \\[\nP(i, j) = \\begin{cases} (1- \\alpha) \\cdot \\frac{1}{d_i} + \\frac{\\alpha}{N} && \\text{ if there exists a link from } i \\to j \\\\ \\frac{\\alpha}{N} && \\text{ otherwise} \\end{cases}\n\\]\nThis chain is irreducible! From any page \\(i\\), there’s at least a probability \\(\\frac{\\alpha}{N}\\) of jumping to any other page \\(j\\) in one step (via the random jump mechanism). Therefore \\(P(i, j) \\geq \\frac{\\alpha}{N} &gt; 0\\) for all \\(i, j\\), which immediately implies every state is accessible from every other state.\n\n\n\n\nUniqueness of the Stationary Distribution\nWhy should we care about irreducibility? It turns out that irreducibility gives us a powerful uniqueness result.\n\n\n\n\n\n\nTipTheorem 6 (Uniqueness)\n\n\n\nIf \\(P\\) is irreducible, then the stationary distribution (if it exists) is unique.\n\n\nTo prove this, we first need a key observation about irreducible chains.\n\n\n\n\n\n\nTipLemma 7\n\n\n\nIf \\(P\\) is irreducible and \\(\\pi\\) is a stationary distribution, then \\(\\pi(x) &gt; 0\\) for all \\(x \\in \\chi\\).\n\n\nProof: Suppose for contradiction that \\(\\pi(x_0) = 0\\) for some state \\(x_0\\). Since \\(\\pi\\) is a stationary distribution, we have \\(\\pi = \\pi P\\), which means: \\[\n\\pi(x_0) = \\sum_{x \\in \\chi} \\pi(x) P(x, x_0)\n\\]\nFor this sum to equal 0, we need \\(\\pi(x) P(x, x_0) = 0\\) for all \\(x\\). Since \\(P(x, x_0) \\geq 0\\), this means either \\(\\pi(x) = 0\\) or \\(P(x, x_0) = 0\\) for every \\(x\\).\nNow pick any state \\(y\\) with \\(\\pi(y) &gt; 0\\) (such a state must exist since \\(\\pi\\) is a probability distribution). By irreducibility, there exists some \\(n\\) such that \\(P^n(y, x_0) &gt; 0\\). But if we expand \\(P^n(y, x_0)\\) using the definition of matrix multiplication, every path from \\(y\\) to \\(x_0\\) must go through some state \\(x\\) with \\(P(x, x_0) = 0\\) (since \\(\\pi(x) = 0\\) forces \\(P(x, x_0) = 0\\) for all \\(x\\)). This contradicts \\(P^n(y, x_0) &gt; 0\\).\nTherefore \\(\\pi(x) &gt; 0\\) for all \\(x \\in \\chi\\). \\(\\square\\)\nNow we can prove uniqueness.\nProof of Theorem 6: Suppose \\(\\pi_1\\) and \\(\\pi_2\\) are two stationary distributions for an irreducible chain \\(P\\). We’ll show they must be equal.\nChoose \\(x_0 \\in \\chi\\) that minimizes the ratio \\(\\pi_1(x)/\\pi_2(x)\\). By Lemma 7, both \\(\\pi_1(x)\\) and \\(\\pi_2(x)\\) are positive for all \\(x\\), so this ratio is well-defined.\nSince \\(\\pi_1\\) is stationary, we have \\(\\pi_1 = \\pi_1 P\\), which means: \\[\n\\pi_1(x_0) = \\sum_{x \\in \\chi} \\pi_1(x) P(x, x_0)\n\\]\nWe can rewrite this as: \\[\\begin{align*}\n\\pi_1(x_0) &= \\sum_{x \\in \\chi} \\frac{\\pi_1(x)}{\\pi_2(x)} \\pi_2(x) P(x, x_0) \\\\\n&\\geq \\sum_{x \\in \\chi} \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\pi_2(x) P(x, x_0) \\tag{Since $x_0$ minimizes the ratio} \\\\\n&= \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\sum_{x \\in \\chi} \\pi_2(x) P(x, x_0) \\\\\n&= \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\pi_2(x_0) \\tag{Since $\\pi_2$ is stationary} \\\\\n&= \\pi_1(x_0)\n\\end{align*}\\]\nSo we have \\(\\pi_1(x_0) \\geq \\pi_1(x_0)\\), which means equality must hold throughout. This can only happen if: \\[\n\\frac{\\pi_1(x)}{\\pi_2(x)} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\text{ for every } x \\text{ such that } P(x, x_0) &gt; 0\n\\]\nNow here’s the key: by irreducibility, for any state \\(y \\in \\chi\\), there exists some \\(N\\) such that \\(P^N(y, x_0) &gt; 0\\). Since \\(\\pi_1\\) and \\(\\pi_2\\) are also stationary for \\(P^N\\), we can repeat the above argument with \\(P^N\\) to conclude: \\[\n\\frac{\\pi_1(x)}{\\pi_2(x)} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\text{ for every } x \\in \\chi\n\\]\nLet \\(c = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)}\\). Then \\(\\pi_1(x) = c \\cdot \\pi_2(x)\\) for all \\(x\\). Since both are probability distributions: \\[\n1 = \\sum_{x \\in \\chi} \\pi_1(x) = \\sum_{x \\in \\chi} c \\cdot \\pi_2(x) = c \\sum_{x \\in \\chi} \\pi_2(x) = c \\cdot 1\n\\]\nTherefore \\(c = 1\\), which means \\(\\pi_1 = \\pi_2\\). \\(\\square\\)\n\n\nWhy Uniqueness Matters\nThis uniqueness result is powerful! Combined with our earlier existence result (that finite irreducible chains have at least one stationary distribution), we now know:\nIf \\(P\\) is a finite, irreducible Markov chain, then it has exactly one stationary distribution \\(\\pi\\).\nThis gives us hope: maybe this unique stationary distribution is also the limiting distribution? That is, maybe \\(\\mu_0 P^n \\to \\pi\\) as \\(n \\to \\infty\\) for any initial distribution \\(\\mu_0\\)?\nUnfortunately, irreducibility alone is not enough to guarantee convergence.\n\n\nA Counterexample: The Alternating Chain\nRecall our alternating chain from earlier: \\[\nP = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nThis chain is irreducible: You can reach state 1 from state 0 in one step (since \\(P(0,1) = 1 &gt; 0\\)), and you can reach state 0 from state 1 in one step (since \\(P(1,0) = 1 &gt; 0\\)).\nIt has a unique stationary distribution: We showed earlier that \\(\\pi = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2}\\end{pmatrix}\\) is stationary, and by Theorem 6, this is the only one.\nBut it does not converge: We showed that \\(\\mu_0 P^n\\) oscillates forever and never converges to any distribution.\nSo irreducibility guarantees uniqueness of the stationary distribution, but not convergence to it. We need one more condition.\n\n\nAperiodicity\nThe problem with the alternating chain is that it has a deterministic cycling pattern - it returns to each state at regular intervals (every 2 steps). We need to rule out this kind of periodic behavior.\n\n\n\n\n\n\nNoteDefinition 4\n\n\n\nThe period of a state \\(i\\) is defined as: \\[\nd(i) = \\gcd\\{n \\geq 1 : P^n(i, i) &gt; 0\\}\n\\] That is, the period is the greatest common divisor of all possible return times to state \\(i\\).\nA state is aperiodic if \\(d(i) = 1\\). A Markov chain is aperiodic if all states are aperiodic.\n\n\nLet’s understand this definition through examples.\n\n\n\n\n\n\nNoneExample: The Alternating Chain is Periodic\n\n\n\nFor the alternating chain \\(P = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\), let’s find the period of state 0.\nWe need all \\(n\\) such that \\(P^n(0, 0) &gt; 0\\): - \\(P^1(0, 0) = 0\\) - \\(P^2(0, 0) = 1\\) ✓ - \\(P^3(0, 0) = 0\\) - \\(P^4(0, 0) = 1\\) ✓ - \\(P^6(0, 0) = 1\\) ✓\nThe set of possible return times is \\(\\{2, 4, 6, 8, \\ldots\\}\\), so \\(d(0) = \\gcd(2, 4, 6, \\ldots) = 2\\).\nThis chain has period 2, which explains the oscillatory behavior!\n\n\n\n\n\n\n\n\nNoneExample: Breaking Periodicity with a Self-Loop\n\n\n\nConsider a slight modification: \\[\nP = \\begin{pmatrix}\n0.1 & 0.9 \\\\\n1 & 0\n\\end{pmatrix}\n\\]\nNow state 0 has a self-loop with probability 0.1. What is \\(d(0)\\)? - \\(P^1(0, 0) = 0.1 &gt; 0\\) ✓ - \\(P^2(0, 0) &gt; 0\\) ✓\nSince we can return in both 1 and 2 steps, \\(d(0) = \\gcd(1, 2) = 1\\), so state 0 is aperiodic.\nThis tiny self-loop breaks the periodic cycling!\n\n\n\n\n\n\n\n\nNoneExample: PageRank is Aperiodic\n\n\n\nIn the PageRank chain, we have \\(P(i, i) \\geq \\frac{\\alpha}{N} &gt; 0\\) for all pages \\(i\\) (due to the random jump). This means every state can return to itself in 1 step, so \\(d(i) = 1\\) for all \\(i\\). The PageRank chain is aperiodic.\n\n\nOne useful fact: in an irreducible chain, all states have the same period. So we can talk about “the” period of an irreducible chain. This means if one state is aperiodic in an irreducible chain, then all states are aperiodic.\n\n\nThe Convergence Theorem\nWe’re finally ready to state the main result.\n\n\n\n\n\n\nImportantTheorem 8 (Convergence to Stationarity)\n\n\n\nLet \\(P\\) be the transition matrix of a finite, irreducible, and aperiodic Markov chain. Then:\n\nThere exists a unique stationary distribution \\(\\pi\\)\nFor any initial distribution \\(\\mu_0\\), we have \\[\n\\lim_{n \\to \\infty} \\mu_0 P^n = \\pi\n\\]\nIn fact, for all states \\(i, j \\in \\chi\\): \\[\n\\lim_{n \\to \\infty} P^n(i, j) = \\pi(j)\n\\]\n\nIn other words, the chain converges to its stationary distribution regardless of where it starts.\n\n\nThe proof of this theorem uses the concept of total variation distance between distributions. For two distributions \\(\\mu\\) and \\(\\nu\\), the total variation distance is: \\[\n\\|\\mu - \\nu\\|_{TV} = \\frac{1}{2} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)|\n\\]\nThis measures how “different” two distributions are. The key is to show that \\(\\|\\mu_0 P^n - \\pi\\|_{TV} \\to 0\\) as \\(n \\to \\infty\\).\nProof sketch: The proof has two main steps:\nStep 1: Show that for an irreducible, aperiodic chain, there exists some time \\(t\\) such that \\(P^t(i, j) &gt; 0\\) for all states \\(i, j\\). This follows from aperiodicity - roughly speaking, without periodic cycling, you can reach any state from any other state in a “non-periodic” number of steps, and for large enough \\(t\\), you can reach it in exactly \\(t\\) steps.\nStep 2: Use this to show contraction of total variation distance. The key insight is that once you have \\(P^t(i, j) &gt; 0\\) for all \\(i, j\\), the chain has a chance to “mix” and forget its initial condition. More formally, one can show there exists \\(\\epsilon &gt; 0\\) such that: \\[\n\\|\\mu_0 P^t - \\pi\\|_{TV} \\leq (1 - \\epsilon) \\|\\mu_0 - \\pi\\|_{TV}\n\\]\nApplying this repeatedly (for times \\(t, 2t, 3t, \\ldots\\)), we get: \\[\n\\|\\mu_0 P^{nt} - \\pi\\|_{TV} \\leq (1 - \\epsilon)^n \\|\\mu_0 - \\pi\\|_{TV} \\to 0\n\\]\nas \\(n \\to \\infty\\). This shows convergence in total variation distance, which implies pointwise convergence of probabilities. \\(\\square\\)\nThe full details of this proof are quite technical, but the intuition is clear: irreducibility ensures the chain can reach all states, and aperiodicity ensures it can do so without getting stuck in deterministic cycles. Together, these properties guarantee that the chain “mixes” and converges to equilibrium.\n\n\nBack to PageRank\nWe’ve now verified that the PageRank chain satisfies both conditions:\n✓ Irreducible: The random jump ensures \\(P(i,j) &gt; 0\\) for all \\(i, j\\) ✓ Aperiodic: Self-loops ensure \\(P(i,i) &gt; 0\\) for all \\(i\\)\nTherefore, by Theorem 8, the PageRank chain converges to its unique stationary distribution! No matter where the random surfer starts, after enough time they’ll be visiting pages according to the stationary distribution \\(\\pi\\). This distribution captures the “importance” of each page, which is exactly what we wanted.\nThis is the mathematical foundation that powered Google’s early success: by modeling web browsing as an irreducible, aperiodic Markov chain, we’re guaranteed that the stationary distribution exists, is unique, and represents the long-run behavior of the system."
  },
  {
    "objectID": "posts/markov-chains/index.html#review-below",
    "href": "posts/markov-chains/index.html#review-below",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Review below",
    "text": "Review below"
  },
  {
    "objectID": "posts/markov-chains/index.html#uniqueness-of-stationary-distribution",
    "href": "posts/markov-chains/index.html#uniqueness-of-stationary-distribution",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Uniqueness of stationary distribution",
    "text": "Uniqueness of stationary distribution"
  },
  {
    "objectID": "posts/markov-chains/index.html#non-uniqueness-of-stationary-distribution",
    "href": "posts/markov-chains/index.html#non-uniqueness-of-stationary-distribution",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Non-Uniqueness of Stationary Distribution",
    "text": "Non-Uniqueness of Stationary Distribution"
  },
  {
    "objectID": "posts/markov-chains/index.html#uniqueness-of-a-stationary-distribution",
    "href": "posts/markov-chains/index.html#uniqueness-of-a-stationary-distribution",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Uniqueness of a stationary distribution",
    "text": "Uniqueness of a stationary distribution\nWe’ve seen that a stationary distribution need not be unique for a Markov Chain."
  },
  {
    "objectID": "posts/markov-chains/index.html#irreducibility",
    "href": "posts/markov-chains/index.html#irreducibility",
    "title": "PageRank and Markov Chains",
    "section": "Irreducibility",
    "text": "Irreducibility\nWe’ve seen that a stationary distribution need not be unique for a Markov Chain, which is a problem because if a stationary distribution could ever hope be a limiting distribution, then it has to be the unique stationary distribution. Let’s study the conditions under which a stationary distribution is the stationary distribution of a Markov Chain.\nGoing from the example above, you might notice that the issue there was that there were states which were “absorbing”, meaning once our chain entered that state, it remained there forever. For example, state \\(0\\) is absorbing because \\(P(0, 1) = P(0, 2) = P(0, 3)  = 0\\) and so the chain would never transition out of that state. Similarly, state \\(3\\) is also an absorbing state. If we eliminate this behavior, then we might have a unique stationary distribution?\nThis observation immediately leads us to the following definition of irreducibility\n\n\n\n\n\n\nNoteDefinition 3\n\n\n\nA Markov chain is irreducible if every state is eventually reachable from every other state. That is, for all \\(x, y \\in \\chi\\), there exists some \\(n \\geq 0\\) such that \\(P^n(x, y) &gt; 0\\).\n\n\n\n\n\n\n\n\nNoneExample: PageRank is Irreducible\n\n\n\nRecall our PageRank transition matrix: \\[\nP(i, j) = \\begin{cases} (1- \\alpha) \\cdot \\frac{1}{d_i} + \\frac{\\alpha}{N} && \\text{ if there exists a link from } i \\to j \\\\ \\frac{\\alpha}{N} && \\text{ otherwise} \\end{cases}\n\\]\nThis chain is irreducible. From any page \\(i\\), there’s at least a probability \\(\\frac{\\alpha}{N}\\) of jumping to any other page \\(j\\) in one step (via the random jump mechanism). Therefore \\(P(i, j) \\geq \\frac{\\alpha}{N} &gt; 0\\) for all \\(i, j\\), which immediately implies every state is accessible from every other state.\n\n\nNow, let’s try to link irreducibility with stationary distributions. Specifically, lets see if irreducibility gives us any information about what a stationary distribution \\(\\pi\\) may look like. Because we are assuming that every state is reachable from every other state, one could guess that \\(\\pi(x) &gt; 0\\) for all \\(x \\in \\chi\\), since otherwise, there would be a state \\(x\\) for which \\(\\mathbb{P}[X_n = x] = 0\\) and make this state unreachable.\nThis leads us to the following proposition:\n\n\n\n\n\n\nTipProposition 4\n\n\n\nIf \\(\\pi\\) is a stationary distribution and \\(P\\) is the transition matrix of an irreducible Markov Chain \\(P\\), then \\(\\pi (x) &gt; 0\\) for all \\(x \\in \\chi\\).\n\n\nProof: Assume for contradiction that there exists an \\(x \\in \\chi\\) such that \\(\\pi (x) = 0\\). Because \\(\\pi\\) is stationary, this means we have\n\\[\n\\begin{align*}\n0 = \\pi (x) &= (\\pi P) (x) \\\\\n&= \\sum_{y \\in \\chi} \\pi (y) P(y, x) & \\tag{Matrix mult.}\\\\\n&= \\sum_{\\substack{y \\in \\chi \\\\ y \\neq x}} \\pi (y) P(y, x) \\tag{Since $\\pi(x)$ = 0} \\\\\n\\end{align*}\n\\] Notice that each term on the right hand side is non negative (since \\(\\pi(y) \\geq 0\\) and \\(P(y, x) \\geq 0\\)). The only way a sum of non negative term can be zero is if each term is zero. This means we have \\[\n\\pi(y) P(y, x) = 0\n\\] for all \\(y \\in \\chi\\) and so either \\(\\pi(y) = 0\\) or \\(P(y, x) = 0\\). Now, because we assumed \\(P\\) was irreducible, there must exist a \\(y \\neq x\\) in \\(\\chi\\) such that \\(P(y, x) &gt; 0\\) (otherwise, \\(x\\) would be unreachable from any other state, no matter how many steps the chain takes). Therefore, for this \\(y\\), it must be the case that \\(\\pi(y) = 0\\).\nWe can now apply the same argument to \\(y\\) and get that there is a \\(z \\in \\chi\\) such that \\(P(z, y) &gt; 0\\), forcing \\(\\pi(z) = 0\\). We can continue this argument inductively, and since every state is reachable from every other state (by irreducibility), we can show that \\(\\pi(w) = 0\\) for all \\(w \\in \\chi\\).\nBut this contradicts the fact that \\(\\pi\\) is a probability distribution, which requires\n\\[\n\\sum_{w \\in \\chi} \\pi(w) = 1\n\\]\nTherefore, \\(\\pi(x) &gt; 0\\) for all \\(x \\in \\chi\\), as desired.\nUsing this proposition, we can now prove a big theorem which tells us when a Markov Chain has a unique stationary distribution.\n\n\n\n\n\n\nImportantTheorem 1 (Uniqueness)\n\n\n\nIf \\(P\\) is the transition matrix of an irreducible Markov Chain, then there exists a unique stationary distribution \\(\\pi\\)\n\n\nProof:\nI will skip the existence part of this theorem because it gets extremely technical and not suited for this post. If you are curious, you should check out this resource\nLets prove uniqueness. Suppose \\(\\pi_1\\) and \\(\\pi_2\\) are two stationary distributions of \\(P\\). Because our state space is finite, and \\(\\pi_1(x), \\pi_2(x) &gt; 0\\) by proposition 4, for all \\(x \\in \\chi\\), we can choose\n\\[\nx_0 :=\\text{arg}\\min\\limits_{x \\in \\chi}\\,\\left( \\frac{\\pi_1(x)}{\\pi_2(x)} \\right)\n\\]\nLet \\(y\\) be an arbitrary state in \\(\\chi\\). By irreducibility, there exists an \\(n \\in N\\) such that \\(P^n (y, x_0) &gt; 0\\). This means that the Markov Chain can reach to \\(x_0\\) from \\(y\\) in \\(n\\) steps via a finite step of transitions \\[y = x_n \\to x_{n-1} \\to \\dots \\to x_1 \\to x_0\\] where \\(P(x_i, x_{i-1}) &gt; 0\\) for all \\(i = 1, 2, \\ldots, n\\).\nWe will prove by induction on the path length that \\[ \\frac{\\pi_1(y)}{\\pi_2(y)} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\] The base case is trivial since it follows by definition. So, assume \\[ \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)} \\] for some \\(1 \\leq i &lt; n\\).\nNow, we can write:\n\\[\n\\begin{flalign}\n\\pi_1(x_{i}) = (\\pi_1 P) (x_{i}) &= \\sum_{x \\in \\chi} \\pi_1(x) P(x, x_{i}) \\tag{$\\pi_1$ is stationary}\\\\\n&= \\sum_{x \\in \\chi} \\frac{\\pi_1(x)}{\\pi_2(x)} \\pi_2(x) P(x, x_{i}) \\tag{Multiply RHS by $\\pi_2(x) / \\pi_2(x)$}\\\\\n&\\geq \\sum_{x \\in \\chi} \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\pi_2(x) P(x, x_{i}) \\tag{$x_{i}$ achieves minimum, I.H.}\\\\\n&= \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\sum_{x \\in \\chi} \\pi_2(x) P(x, x_{i}) \\tag{Ratio is constant}\\\\\n&= \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\pi_2(x_{i}) \\tag{$\\pi_2$ is stationary}\\\\\n&= \\pi_1(x_{i})\n\\end{flalign}\n\\]\nNotice that because LHS = RHS, the inequality in between must be an equality. Therefore: \\[\n\\sum_{x \\in \\chi} \\frac{\\pi_1(x)}{\\pi_2(x)} \\pi_2(x) P(x, x_{i}) = \\sum_{x \\in \\chi} \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\pi_2(x) P(x, x_{i})\n\\]\nRearranging: \\[\n\\sum_{x \\in \\chi} \\left[ \\frac{\\pi_1(x)}{\\pi_2(x)} - \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\right] \\pi_2(x) P(x, x_{i}) = 0\n\\]\nNow, note that \\(\\pi_2(x) &gt; 0\\) by proposition 4, and because \\(x_{i}\\) achieves the minimum, we have \\(\\frac{\\pi_1(x)}{\\pi_2(x)} - \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} \\geq 0\\) and \\(P(x, x_{i}) \\geq 0\\) by definition. Therefore, the only way a sum of non-negative terms is zero is if each term is zero. Thus, for any state \\(x \\in \\chi\\) with \\(P(x, x_{i}) &gt; 0\\): \\[\n\\frac{\\pi_1(x)}{\\pi_2(x)} - \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} = 0 \\implies \\frac{\\pi_1(x)}{\\pi_2(x)} = \\frac{\\pi_1(x_{i})}{\\pi_2(x_{i})} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)}\n\\]\nIn particular, since \\(P(x_{i + 1}, x_{i}) &gt; 0\\) by our path construction, we have: \\[\n\\frac{\\pi_1(x_{i+1})}{\\pi_2(x_{i+1})} = \\frac{\\pi_1(x_0)}{\\pi_2(x_0)}\n\\]\nand by the principle of induction, we can conclude that this holds for all \\(1 \\leq i &lt; n\\). Therefore, we can conclude that \\[\n\\frac{\\pi_1(y)}{\\pi_2(y)} = \\frac{\\pi_1(x_{n})}{\\pi_2(x_{n})} =\\frac{\\pi_1(x_0)}{\\pi_2(x_0)}\n\\] as desired.\nNow, since \\(y\\) was an arbitrary state, we now know that \\[ \\frac{\\pi_1(x)}{\\pi_2(x)} = c \\] for all \\(x \\in \\chi\\), where \\(c\\) is some positive constant. This means \\(\\pi_1(x) = c \\cdot \\pi_2(x)\\) for all \\(x\\). Now, since both \\(\\pi_1, \\pi_2\\) are probability distributions, we have: \\[\n1 = \\sum_{x \\in \\chi} \\pi_1(x) =  \\sum_{x \\in \\chi} c  \\pi_2(x) = c \\sum_{x \\in \\chi} \\pi_2(x) = c\n\\]\nTherefore \\(c = 1\\), which implies \\(\\pi_1(x) = \\pi_2(x)\\) for all \\(x \\in \\chi\\), as desired.\n\nAnother Counter Example\nThat was a lot of hard work! But are we done? Surely now that we have a unique stationary distribution, we can claim that the limiting distribution converges to this stationary distribution right?\nWell, not so fast. Let’s consider another example, where \\(\\chi = \\{0, 1\\}\\) and the transition matrix is \\[\nP = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\] which is the chain that transitions from \\(0 \\to 1 \\to 0 \\to 1 \\cdots\\) deterministically. Notice that the distribution \\(\\pi = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix}\\) is a stationary distribution since \\[\\begin{align*}\n\\pi P &= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} = \\pi\n\\end{align*}\\]\nAdditionally, this chain is clearly irreducible, since every state is reachable from every other state. Therefore, \\(\\pi\\) is the unique stationary distribution from theorem 1. But it is not a limiting distribution. To see why, first note that \\[\\begin{align*}\nP^2 &= \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I\n\\end{align*}\\]\nTherefore, for any \\(n \\in \\mathbb{N}\\): \\[\nP^n = \\begin{cases}\nP^{2k} = (P^2)^k = I & \\text{if } n \\text{ is even} \\\\\nP^{2k + 1} = P^{2k} P = P & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\]\nNow consider an initial distribution (read as a 1x2 matrix) \\(\\mu_0 = \\begin{pmatrix}1 & 0\\end{pmatrix}\\). This means we can make the simplification: \\[\\begin{align*}\n\\mu_0 P^n &= \\begin{pmatrix}1 & 0\\end{pmatrix}P^n \\\\\n&= \\begin{cases}\n\\begin{pmatrix}1 & 0\\end{pmatrix} & \\text{if } n \\text{ is even} \\\\\n\\begin{pmatrix}0 & 1\\end{pmatrix} & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\end{align*}\\]\nSince \\(\\mu_0 P^n\\) alternates between \\(\\begin{pmatrix}1 & 0\\end{pmatrix}\\) and \\(\\begin{pmatrix}0 & 1\\end{pmatrix}\\), the sequence does not converge as \\(n \\to \\infty\\). Therefore, no limiting distribution exists."
  },
  {
    "objectID": "posts/markov-chains/index.html#proposition-4",
    "href": "posts/markov-chains/index.html#proposition-4",
    "title": "Markov Chains and the Stationary Distribution",
    "section": "Proposition 4",
    "text": "Proposition 4\nIf \\(\\pi\\) is a stationary distribution and \\(P\\) is the transition matrix of an irreducible Markov Chain \\(P\\), then \\(\\pi (x) &gt; 0\\) for all \\(x \\in \\chi\\). :::\nProof: Assume for contradiction that there exists an \\(x \\in \\chi\\) such that \\(\\pi (x) = 0\\)."
  },
  {
    "objectID": "posts/markov-chains/index.html#aperiodicity",
    "href": "posts/markov-chains/index.html#aperiodicity",
    "title": "PageRank and Markov Chains",
    "section": "Aperiodicity",
    "text": "Aperiodicity\nThat was a lot of hard work! But are we done? Surely now that we have a unique stationary distribution, we can claim that the limiting distribution converges to this stationary distribution right?\nWell, not so fast. Let’s consider another example, where \\(\\chi = \\{0, 1\\}\\) and the transition matrix is \\[\nP = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\] which is the chain that transitions from \\(0 \\to 1 \\to 0 \\to 1 \\cdots\\) deterministically. Notice that the distribution \\(\\pi = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix}\\) is a stationary distribution since \\[\\begin{align*}\n\\pi P &= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} = \\pi\n\\end{align*}\\]\nAdditionally, this chain is clearly irreducible, since every state is reachable from every other state. Therefore, \\(\\pi\\) is the unique stationary distribution from theorem 1. But it is not a limiting distribution. To see why, first note that \\[\\begin{align*}\nP^2 &= \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I\n\\end{align*}\\]\nTherefore, for any \\(n \\in \\mathbb{N}\\): \\[\nP^n = \\begin{cases}\nP^{2k} = (P^2)^k = I & \\text{if } n \\text{ is even} \\\\\nP^{2k + 1} = P^{2k} P = P & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\]\nNow consider an initial distribution (read as a 1x2 matrix) \\(\\mu_0 = \\begin{pmatrix}1 & 0\\end{pmatrix}\\). This means we can make the simplification: \\[\\begin{align*}\n\\mu_0 P^n &= \\begin{pmatrix}1 & 0\\end{pmatrix}P^n \\\\\n&= \\begin{cases}\n\\begin{pmatrix}1 & 0\\end{pmatrix} & \\text{if } n \\text{ is even} \\\\\n\\begin{pmatrix}0 & 1\\end{pmatrix} & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\end{align*}\\]\nSince \\(\\mu_0 P^n\\) alternates between \\(\\begin{pmatrix}1 & 0\\end{pmatrix}\\) and \\(\\begin{pmatrix}0 & 1\\end{pmatrix}\\), the sequence does not converge as \\(n \\to \\infty\\). Therefore, no limiting distribution exists.\n\nAperidocity\nWhat went wrong here? The chain is irreducible and has a unique stationary distribution, but it still doesn’t converge to that distribution. The problem is that the chain exhibits periodic behavior. It oscillates between states \\(0\\) and \\(1\\) in a predictable cycle, never settling down into a stable long-run pattern.\nThis observation motivates our final piece of the puzzle: aperiodicity.\n\n\n\n\n\n\nNoteDefinition 4\n\n\n\nThe period of a state \\(x\\) is the greatest common divisor (gcd) of all possible return times to that state. That is, \\[\nd(x) = \\gcd\\{n \\geq 1 : P^n(x, x) &gt; 0\\}\n\\]\nA state \\(x\\) is aperiodic if \\(d(x) = 1\\). A Markov chain is aperiodic if all states are aperiodic.\n\n\nLet’s unpack this definition. The set \\(\\{n \\geq 1 : P^n(x, x) &gt; 0\\}\\) contains all the time steps at which the chain can return to state \\(x\\). If this set is \\(\\{2, 4, 6, 8, \\ldots\\}\\) (only even numbers), then \\(\\gcd = 2\\) and the state has period 2. If the set is \\(\\{1, 2, 3, 4, \\ldots\\}\\) (all positive integers), then \\(\\gcd = 1\\) and the state is aperiodic.\nIn our problematic example above, state \\(0\\) has period 2 because the chain can only return to state \\(0\\) in an even number of steps: \\(P^2(0, 0) = 1\\), \\(P^4(0, 0) = 1\\), and so on. The same is true for state \\(1\\). This periodic structure causes the distribution to oscillate rather than converge.\n\n\n\n\n\n\nNoneExample: PageRank is Aperiodic\n\n\n\nRecall our PageRank transition matrix where every entry satisfies \\(P(i, j) \\geq \\frac{\\alpha}{N} &gt; 0\\). This means that for any state \\(i\\), we have \\(P^1(i, i) \\geq \\frac{\\alpha}{N} &gt; 0\\). Therefore, \\(1\\) is in the set of possible return times, which immediately gives us \\(\\gcd = 1\\). So every state is aperiodic, making the entire chain aperiodic.\n\n\nFor irreducible chains, we have a useful fact: if one state is aperiodic, then all states are aperiodic. This means we only need to check one state to verify aperiodicity of the entire chain. I won’t prove this here, but the key insight is that irreducibility creates a strong connection between states, so their period structure must match.\nNow we have all the pieces we need. Irreducibility ensures that every state is reachable and gives us a unique stationary distribution with \\(\\pi(x) &gt; 0\\) for all states. Aperiodicity prevents the chain from getting stuck in oscillating cycles. Together, these two properties guarantee that the chain converges to its stationary distribution.\n\n\n\n\n\n\nImportantTheorem 2 (Convergence)\n\n\n\nLet \\(P\\) be the transition matrix of an irreducible and aperiodic Markov chain with stationary distribution \\(\\pi\\). Then for any initial distribution \\(\\mu_0\\) and any state \\(y \\in \\chi\\): \\[\n\\lim_{n \\to \\infty} (\\mu_0 P^n)(y) = \\pi(y)\n\\]\nIn other words, \\(\\lim_{n \\to \\infty} \\mu_n = \\pi\\), regardless of where we started.\n\n\nThis is the main result we’ve been building toward! It tells us that if our Markov chain is irreducible and aperiodic, then no matter what initial distribution we start with, after running the chain for a long time, the distribution over states will converge to the unique stationary distribution \\(\\pi\\).\nFor our PageRank example, this theorem guarantees that the random surfer’s long-run behavior is well-defined and deterministic. The proportion of time spent on each page converges to the stationary distribution, regardless of which page we started on. This means PageRank produces a consistent, reliable ranking of webpage importance.\nThe proof of this theorem requires some sophisticated machinery from probability theory and linear algebra, involving coupling arguments and convergence of distributions. I won’t include the full proof here because it’s quite technical, but the key ideas involve showing that the difference between \\(\\mu_n\\) and \\(\\pi\\) shrinks exponentially fast as \\(n\\) increases."
  },
  {
    "objectID": "posts/markov-chains/index.html#aperidocity",
    "href": "posts/markov-chains/index.html#aperidocity",
    "title": "PageRank and Markov Chains",
    "section": "Aperidocity",
    "text": "Aperidocity\nWhat went wrong here? The chain is irreducible and has a unique stationary distribution, but it still doesn’t converge to that distribution. The problem is that the chain exhibits periodic behavior. It oscillates between states \\(0\\) and \\(1\\) in a predictable cycle, never settling down into a stable long-run pattern.\nThis observation motivates our final piece of the puzzle: aperiodicity.\n\n\n\n\n\n\nNoteDefinition 4\n\n\n\nThe period of a state \\(x\\) is the greatest common divisor (gcd) of all possible return times to that state. That is, \\[\nd(x) = \\gcd\\{n \\geq 1 : P^n(x, x) &gt; 0\\}\n\\]\nA state \\(x\\) is aperiodic if \\(d(x) = 1\\). A Markov chain is aperiodic if all states are aperiodic.\n\n\nIntuitively, we can think of the set \\(\\{n \\geq 1 : P^n(x, x) &gt; 0\\}\\) as containing all the time steps at which the chain can return to state \\(x\\). If this set is \\(\\{2, 4, 6, 8, \\ldots\\}\\) (only even numbers), then \\(\\gcd = 2\\) and the state has period 2. If the set is \\(\\{1, 2, 3, 4, \\ldots\\}\\) (all positive integers), then \\(\\gcd = 1\\) and the state is aperiodic.\nIn our example above, state \\(0\\) has period 2 because the chain can only return to state \\(0\\) in an even number of steps: \\(P^2(0, 0) = 1\\), \\(P^4(0, 0) = 1\\), and so on. The same is true for state \\(1\\). This causes the distribution \\(\\mu_0 P^n\\) to oscillate rather than converge as \\(n \\to \\infty\\).\n\n\n\n\n\n\nNoneExample: PageRank is Aperiodic\n\n\n\nRecall our PageRank transition matrix where every entry satisfies \\(P(i, j) \\geq \\frac{\\alpha}{N} &gt; 0\\). This means that for any state \\(i\\), we have \\(P^1(i, i) \\geq \\frac{\\alpha}{N} &gt; 0\\). Therefore, \\(1\\) is in the set of possible return times, which immediately gives us \\(\\gcd = 1\\) (think about what the greatest common divisor of \\(1\\) and any number is). So every state is aperiodic, making the entire chain aperiodic.\n\n\nMaybe now we can prove convergence? Is Irreducibility and Aperiodicity enough? It turns out yes, but to answer that formally, we need a bit more machinery.\n\nConvergence\nBefore we can even hope to prove convergence, we first need a way to measure how “close” two probability distributions are to each other. One intuitive approach is to ask: what’s the largest possible difference in the probabilities that \\(\\mu, \\nu\\) assign to any event?\nFormally, for any event \\(A \\subseteq \\chi\\), we could compute \\(|\\mu(A) - \\nu(A)|\\). If we take the worst case scenario over all events, we get \\[\n\\max_{A \\subseteq \\chi} |\\mu(A) - \\nu(A)|\n\\] This tells us, in the most extreme case, how much these distributions disagree about the probability of an event. Let’s see if we can simplify this definition a little bit:\n\\[\n\\mu(A) - \\nu(A) = \\sum_{x \\in A} \\mu(x) - \\sum_{x \\in A} \\nu(x) = \\sum_{x \\in A} [\\mu(x) - \\nu(x)]\n\\] Notice that this difference is maximized if we choose \\(A\\) to contain exactly the points for which \\(\\mu(x) &gt; \\nu(x)\\). Let’s call this set \\(A^+ = \\{x \\in \\chi \\mid \\mu(x) &gt; \\nu(x)\\}\\). Then, we can write \\[\n\\max_{A \\subseteq \\chi} |\\mu(A) - \\nu(A)| = \\sum_{x \\in A^+} [\\mu(x) - \\nu(x)]\n\\] Now, since both \\(\\mu, \\nu\\) are probability distributions, they must sum to one, so we have \\[\n\\sum_{x \\in \\chi}[\\mu(x) - \\nu(x)] = \\sum_{x \\in \\chi}\\mu(x) - \\sum_{x \\in \\chi} \\nu(x) = 0\n\\] This means that when we partition the sum over all points, we can write \\[\\begin{align*}\n&0 = \\sum_{x \\in \\chi}[\\mu(x) - \\nu(x)] = \\sum_{x \\in A^+} [\\mu(x) - \\nu(x)] + \\sum_{x \\not \\in A^+} [\\mu(x) - \\nu(x)] \\\\\n\\end{align*}\\]\nwhich implies that\n\\[\n\\sum_{x \\in A^+} [\\mu(x) - \\nu(x)] = - \\sum_{x \\not \\in A^+} [\\mu(x) - \\nu(x)] = \\sum_{x \\not \\in A^+} [\\nu(x) - \\mu(x)]\n\\]\nNow, when we take the absolute difference over all points in \\(\\chi\\), we have\n\\[\\begin{align*}\n\\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)| &= \\sum_{x \\in A^+} |\\mu(x) - \\nu(x)| + \\sum_{x \\not \\in A^+} |\\nu(x) - \\mu(x)| \\\\\n&= 2 \\sum_{x \\in A^+} |\\mu(x) - \\nu(x)|\n\\end{align*}\\]\nRearranging, we can finally write: \\[\\begin{align*}\n2 \\sum_{x \\in A^+} |\\mu(x) - \\nu(x)| &= 2 \\max_{A \\subseteq \\chi} |\\mu(A) - \\nu(A)| = \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)|\\\\\n\\max_{A \\subseteq \\chi} |\\mu(A) - \\nu(A)| &= \\frac{1}{2} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)|\n\\end{align*}\\]\nThis leads us to the convenient definition of distances between two distributions, known as the total variation difference:\n\n\n\n\n\n\nNoteDefinition 5 (Total Variation Distance)\n\n\n\nFor any two probability distributions \\(\\mu\\) and \\(\\nu\\) over \\(\\chi\\), we define the total variation distance between \\(\\mu\\) and \\(\\nu\\) by \\[\n\\|\\mu - \\nu\\|_{\\text{TV}} := \\frac{1}{2} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)|\n\\]\n\n\nThe total variation distance is a metric on the space of probability distributions. It ranges from \\(0\\) (when \\(\\mu = \\nu\\)) to \\(1\\) (when \\(\\mu\\) and \\(\\nu\\) are completely different). You should check that it is indeed a valid distance metric.\nWe can now formalize what we mean by “convergence to the stationary distribution” using this notion of distance.\n\n\n\n\n\n\nNoteDefinition 6 (Convergence in Total Variation)\n\n\n\nWe say that the distribution of \\(X_n\\) converges to \\(\\pi\\) in total variation as \\(n \\to \\infty\\) if for all starting distribution \\(\\mu_0\\), \\[\n\\|\\mu_0 P^n - \\pi\\|_{\\text{TV}} \\to 0 \\text{ as } n \\to \\infty\n\\]\n\n\nThis is equivalent to saying that for any state \\(x \\in \\chi\\), \\[\n\\lim_{n \\to \\infty} \\frac{1}{2} \\sum_{x \\in \\chi} |\\mathbb{P}[X_n = x] - \\pi(x)| = 0\n\\]\nNow we can restate the running question asked throughout this blog more precisely: we want to show that if \\(P\\) is irreducible and aperiodic and \\(\\mu_0\\) is an arbitrary initial distribution for \\(P\\), then \\(\\mu_0 P^n \\to \\pi\\) in total variation.\n\n\nMachinery For Convergence\nTo prove our formalized statement, we will need several technical results. Let’s build them up step by step.\n\n\n\n\n\n\nTipProposition 5\n\n\n\nIf a Markov chain is aperiodic, then there exists \\(N \\in \\mathbb{N}\\) such that for all \\(x \\in \\chi\\) and all \\(n \\geq N\\), we have \\(P^n(x, x) &gt; 0\\)\n\n\nProof Sketch:\nFix a state \\(x\\). By definition of aperiodicity, we have \\(\\gcd\\{n \\geq 1 : P^n(x, x) &gt; 0\\} = 1\\). Let \\(S = \\{n \\geq 1 : P^n(x, x) &gt; 0\\}\\) be this set.\nA classical result from number theory tells us that because \\(S\\) consists of positive integers and \\(\\gcd(S) = 1\\), then there exists some \\(N \\in \\mathbb{N}\\) such that every integer \\(n \\geq N\\) can be written as a non-negative integer linear combination of elements from \\(S\\) (called the Chicken McNugget Theorem). In other words, for all \\(n \\geq N\\), we can write \\[\nn = \\sum_{i=1}^{k} m_i s_i\n\\] where \\(s_1, \\ldots, s_k \\in S\\) and \\(m_1, \\ldots, m_k\\) are non-negative integers.\nNow, for each \\(n \\geq N\\), this means we can return to \\(x\\) at time \\(n\\) by taking the path that returns to \\(x\\) at times \\(s_1\\), or the path that returns to \\(x\\) at time \\(2s_1, 3 s_1 \\ldots, m_1 s_1\\). Similarly, we could also take the path that returns at times \\(m_1 s_1 + s_2, m_1 s_1 + 2s_2, \\ldots, m_1 s_1 + m_2 s_2\\), and so on. At each of these intermediate times, the chain is at state \\(x\\), and we know \\(P^{s_i}(x, x) &gt; 0\\) for each \\(i\\) because \\(s_i \\in S\\).\nBy the Markov property, the probability of following this entire path is the product of the transition probabilities:\n\\[\n\\begin{align*}\nP^n(x,x) \\geq \\underbrace{P^{s_1}(x, x) \\cdot P^{s_1}(x, x) \\cdots P^{s_1}(x, x)}_{m_1 \\text{ times}} \\cdot \\underbrace{P^{s_2}(x, x) \\cdots P^{s_2}(x, x)}_{m_2 \\text{ times}} \\cdots &gt; 0\n\\end{align*}\n\\]\nTherefore, \\(P^n(x, x) &gt; 0\\) for all \\(n \\geq N\\), as desired.\n\n\n\n\n\n\nTipProposition 6\n\n\n\nIf a Markov chain is irreducible and aperiodic, then there exists \\(N \\in \\mathbb{N}\\) such that \\(P^N(x, y) &gt; 0\\) for all \\(x, y \\in \\chi\\).\n\n\nProof Sketch:\nBy Proposition 5, we know that for each state \\(x \\in \\chi\\), there exists some \\(N_x\\) such that \\(P^n(x, x) &gt; 0\\) for all \\(n \\geq N_x\\). Because our state space is finite, we can take \\(N_1 = \\max_{x \\in \\chi} N_x\\), which ensures that \\(P^n(x, x) &gt; 0\\) for all \\(x \\in \\chi\\) and all \\(n \\geq N_1\\).\nNow, by irreducibility, for any two states \\(x\\) and \\(y\\), there exists some \\(n_{xy} \\in \\mathbb{N}\\) such that \\(P^{n_{xy}}(x, y) &gt; 0\\). Again, because the state space is finite, we can take \\(N_2 = \\max_{x, y \\in \\chi} n_{xy}\\).\nLet \\(N = N_1 + N_2\\). For any states \\(x, y \\in \\chi\\), we can go from \\(x\\) to \\(y\\) in exactly \\(N\\) steps by first spending \\(N_2\\) steps to get from \\(x\\) to \\(y\\), and then spending \\(N_1\\) steps returning from \\(y\\) to \\(y\\). We know both of these are possible with positive probability, so: \\[\nP^N(x, y) \\geq P^{N_2}(x, y) \\cdot P^{N_1}(y, y) &gt; 0\n\\]\nSince \\(x, y\\) were arbitrary this holds for all \\(x, y \\in \\chi\\), as desired.\nThe next result is a key technical tool that shows how the transition matrix acts as a contraction in total variation distance.\n\n\n\n\n\n\nTipProposition 7\n\n\n\nLet \\(P\\) be the transition matrix of a Markov chain. If \\(\\mu\\) and \\(\\nu\\) are any two probability distributions, then \\[\n\\|\\mu P - \\nu P\\|_{\\text{TV}} \\leq \\|\\mu - \\nu\\|_{\\text{TV}}\n\\]\n\n\nProof Sketch:\nWe need to show that applying one step of the Markov chain cannot increase the total variation distance between two distributions. Let’s compute:\n\\[\n\\begin{align*}\n\\|\\mu P - \\nu P\\|_{\\text{TV}} &= \\frac{1}{2} \\sum_{y \\in \\chi} |(\\mu P)(y) - (\\nu P)(y)| \\\\\n&= \\frac{1}{2} \\sum_{y \\in \\chi} \\left| \\sum_{x \\in \\chi} \\mu(x) P(x, y) - \\sum_{x \\in \\chi} \\nu(x) P(x, y) \\right| \\\\\n&= \\frac{1}{2} \\sum_{y \\in \\chi} \\left| \\sum_{x \\in \\chi} (\\mu(x) - \\nu(x)) P(x, y) \\right| \\\\\n&\\leq \\frac{1}{2} \\sum_{y \\in \\chi} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)| P(x, y) \\tag{By triangle inequality} \\\\\n&= \\frac{1}{2} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)| \\sum_{y \\in \\chi} P(x, y) \\tag{Swap sums bc $|\\chi| &lt; \\infty$}\\\\\n&= \\frac{1}{2} \\sum_{x \\in \\chi} |\\mu(x) - \\nu(x)| \\cdot 1 \\tag{rows of $P$ sum to 1}\\\\\n&= \\|\\mu - \\nu\\|_{\\text{TV}}\n\\end{align*}\n\\]\nas desired.\nThis is very important! It tells us that the Markov Chain’s transition matrix can decrease distances but never increases them. However, to get convergence, we need something stronger. We need the chain to actually contract distances, not just preserve them. This is where irreducibility and aperiodicity come in.\n\n\n\n\n\n\nTipProposition 8 (Harris Lemma)\n\n\n\nSuppose there exists a probability distribution \\(\\gamma\\) and a number \\(\\delta &gt; 0\\) such that \\(P(x, y) \\geq \\delta \\gamma(y)\\) for all \\(x, y \\in \\chi\\). Then for any two probability distributions \\(\\mu\\) and \\(\\nu\\), \\[\n\\|\\mu P - \\nu P\\|_{\\text{TV}} \\leq (1 - \\delta) \\|\\mu - \\nu\\|_{\\text{TV}}\n\\]\n\n\nProof Sketch:\nThe main idea is to decompose the transition matrix \\(P\\) into two parts: one that both distributions agree on, and a remaining part \\(P_2\\) that has the differences. More precisely, I claim we can write: \\[\nP(x, y) = \\delta \\gamma(y) + (1 - \\delta) P_2(x, y)\n\\]\nwhere \\(P_2\\) is itself a stochastic matrix defined as\n\\[\nP_2(x, y) := \\frac{P(x, y) - \\delta \\gamma(y)}{1 - \\delta}\n\\]\nWe need to show that this is a stochastic matrix, which means showing that the entries are non negative, and the rows sum up to one. Since we’re given that \\(P(x, y) \\geq \\delta \\gamma(y)\\) for all \\(x, y \\in \\chi\\), we have: \\[\nP(x, y) - \\delta \\gamma(y) \\geq 0\n\\]\nAnd since \\(\\delta &lt; 1\\) (otherwise the proposition is trivial), we have \\(1 - \\delta &gt; 0\\). Therefore:\n\\[\nP_2(x, y) = \\frac{P(x, y) - \\delta \\gamma(y)}{1 - \\delta} \\geq 0\n\\]\nFinally, the rows of \\(P_2\\) sum to \\(1\\) since for any fixed \\(x \\in \\chi\\): \\[\\begin{align}\n\\sum_{y \\in \\chi} P_2(x, y) &= \\sum_{y \\in \\chi} \\frac{P(x, y) - \\delta \\gamma(y)}{1 - \\delta} \\\\\n&= \\frac{1}{1 - \\delta} \\sum_{y \\in \\chi} [P(x, y) - \\delta \\gamma(y)] \\\\\n&= \\frac{1}{1 - \\delta} \\left[\\sum_{y \\in \\chi} P(x, y) - \\delta \\sum_{y \\in \\chi} \\gamma(y)\\right] \\\\\n&= \\frac{1}{1 - \\delta} [1 - \\delta \\cdot 1] \\\\\n&= 1\n\\end{align}\\]\nwhere we used that \\(P\\) is stochastic (so \\(\\sum_y P(x, y) = 1\\)) and \\(\\gamma\\) is a probability distribution (so \\(\\sum_y \\gamma(y) = 1\\)).\nNow, when we apply \\(P\\) to any distribution \\(\\mu\\), we see\n\\[\\begin{align}\n(\\mu P)(y) &= \\sum_{x \\in \\chi} \\mu(x) P(x, y) \\\\\n&= \\sum_{x \\in \\chi} \\mu(x) [\\delta \\gamma(y) + (1 - \\delta) P_2(x, y)] \\\\\n&= \\sum_{x \\in \\chi} \\mu(x) \\delta \\gamma(y) + \\sum_{x \\in \\chi} \\mu(x) (1 - \\delta) P_2(x, y) \\\\\n&= \\delta \\gamma(y) \\sum_{x \\in \\chi} \\mu(x) + (1 - \\delta) \\sum_{x \\in \\chi} \\mu(x) P_2(x, y) \\\\\n&= \\delta \\gamma(y) \\cdot 1 + (1 - \\delta) (\\mu P_2)(y) \\\\\n&= \\delta \\gamma(y) + (1 - \\delta) (\\mu P_2)(y)\n\\end{align}\\]\nPlugging everything in, we can now show the result: \\[\n\\begin{align*}\n\\|\\mu P - \\nu P\\|_{\\text{TV}} &= \\left\\| (\\delta \\gamma + (1 - \\delta) \\mu P_2) - (\\delta \\gamma + (1 - \\delta) \\nu P_2 )\\right\\|_{\\text{TV}} \\\\\n&= \\left\\| \\delta \\gamma + (1 - \\delta) \\mu P_2 - \\delta \\gamma - (1 - \\delta) \\nu P_2 \\right\\|_{\\text{TV}} \\\\\n&= (1 - \\delta) \\|\\mu P_2 - \\nu P_2\\|_{\\text{TV}} \\\\\n&\\leq (1 - \\delta) \\|\\mu - \\nu\\|_{\\text{TV}} \\tag{By Proposition 7}\n\\end{align*}\n\\]\nThis shows that the distance contracts by a factor of at least \\((1 - \\delta)\\) in each step.\nNow we have all the tools we need to prove the main convergence theorem\n\n\nProof of Convergence\n\n\n\n\n\n\nImportantTheorem 2 (Convergence)\n\n\n\nLet \\(P\\) be the transition matrix of an irreducible and aperiodic Markov chain with stationary distribution \\(\\pi\\). Then for any initial distribution \\(\\mu_0\\) and any state \\(y \\in \\chi\\): \\[\n\\lim_{n \\to \\infty} (\\mu_0 P^n)(y) = \\pi(y)\n\\]\nIn other words, \\(\\lim_{n \\to \\infty} \\mu_n = \\pi\\), regardless of where we started.\n\n\nProof Sketch of Theorem 2:\nBy Proposition 6, since our chain is irreducible and aperiodic, there exists some \\(N \\in \\mathbb{N}\\) such that \\(P^N(x, y) &gt; 0\\) for all \\(x, y \\in \\chi\\).\nLet \\(\\delta = \\min_{x, y \\in \\chi} P^N(x, y)\\). This minimum exists and is positive because we’re taking the minimum over a finite set of positive numbers. Now define \\(\\gamma(y) = \\frac{1}{|\\chi|}\\) to be the uniform distribution over all states.\nI claim that \\(P^N(x, y) \\geq \\delta \\gamma(y)\\) for all \\(x, y\\). To see this, note that: \\[\\begin{align*}\nP^N(x, y) &\\geq \\delta \\tag{By defn} \\\\\n&= \\delta \\cdot \\frac{1}{|\\chi|} \\cdot |\\chi| \\\\\n&\\geq \\delta \\cdot \\frac{1}{|\\chi|} \\tag{B/c $|\\chi| &gt; 0$ } \\\\\n&= \\delta \\gamma(y)\n\\end{align*}\\]\nTherefore, by Proposition 8, for any two distributions \\(\\mu\\) and \\(\\nu\\): \\[\n\\|\\mu P^N - \\nu P^N\\|_{\\text{TV}} \\leq (1 - \\delta) \\|\\mu - \\nu\\|_{\\text{TV}}\n\\]\nLet \\(\\mu_0\\) be any arbitrary initial distribution for \\(P\\). We can use induction and the above observation to show that for all \\(k \\geq 1\\), we have: \\[\n\\|(\\mu_0 P^{kN}) - \\pi\\|_{\\text{TV}} \\leq (1 - \\delta)^k \\|\\mu_0 - \\pi\\|_{\\text{TV}}\n\\]\nThe base case \\(k = 1\\) follows from Proposition 8: \\[\n\\|(\\mu_0 P^{N}) - \\pi\\|_{\\text{TV}} = \\|(\\mu_0 P^{N}) - \\pi P^N\\|_{\\text{TV}} \\leq (1 - \\delta) \\|\\mu_0 - \\pi\\|_{\\text{TV}}\n\\]\nNow assume the result holds for \\(k\\). We need to show it holds for \\(k+1\\): \\[\\begin{align}\n\\|(\\mu_0 P^{(k+1)N}) - \\pi\\|_{\\text{TV}} &= \\|(\\mu_0 P^{k N} P^N) - (\\pi P^N)\\|_{\\text{TV}} \\\\\n&\\leq (1 - \\delta) \\|(\\mu_0 P^{kN}) - \\pi\\|_{\\text{TV}} \\tag{Proposition 8} \\\\\n&\\leq (1 - \\delta) \\cdot (1 - \\delta)^k \\|\\mu_0 - \\pi\\|_{\\text{TV}} \\tag{Induction hypothesis} \\\\\n&= (1 - \\delta)^{k+1} \\|\\mu_0 - \\pi\\|_{\\text{TV}}\n\\end{align}\\]\nwhich completes the induction. Now, since \\(0 &lt; (1 - \\delta) &lt; 1\\), we have \\((1 - \\delta)^k \\to 0\\) as \\(k \\to \\infty\\). This means: \\[\n\\lim_{k \\to \\infty} \\|(\\mu_0 P^{kN}) - \\pi\\|_{\\text{TV}} = 0\n\\]\nTo handle times that aren’t multiples of \\(N\\), note that for any \\(n \\geq 0\\), we can write \\(n = kN + r\\) where \\(0 \\leq r &lt; N\\). Then: \\[\\begin{align}\n\\|(\\mu_0 P^n) - \\pi\\|_{\\text{TV}} &= \\|(\\mu_0 P^{kN + r}) - \\pi\\|_{\\text{TV}} \\\\\n&= \\|(\\mu_0 P^{kN}) P^r - \\pi P^r\\|_{\\text{TV}} \\\\\n&\\leq \\|(\\mu_0 P^{kN}) - \\pi\\|_{\\text{TV}} \\tag{Proposition 7} \\\\\n&\\leq (1 - \\delta)^k \\|\\mu_0 - \\pi\\|_{\\text{TV}}\n\\end{align}\\]\nSince \\(k \\to \\infty\\) as \\(n \\to \\infty\\), we conclude that: \\[\n\\lim_{n \\to \\infty} \\|(\\mu_0 P^n) - \\pi\\|_{\\text{TV}} = 0\n\\]\nThis proves that the distribution converges to \\(\\pi\\) in total variation, completing the main proof.\n\n\n\n\n\n\nNoneRemark\n\n\n\nThe convergence is exponentially fast with rate \\((1 - \\delta)\\), which depends on the smallest entry of \\(P^N\\). This tells us not just that convergence happens, but also gives us a quantitative bound on how quickly the chain converges."
  },
  {
    "objectID": "posts/markov-chains/index.html#another-counter-example",
    "href": "posts/markov-chains/index.html#another-counter-example",
    "title": "PageRank and Markov Chains",
    "section": "Another Counter Example",
    "text": "Another Counter Example\nThat was a lot of hard work! But are we done? Surely now that we have a unique stationary distribution, we can claim that the limiting distribution converges to this stationary distribution right?\nWell, not so fast. Let’s consider another example, where \\(\\chi = \\{0, 1\\}\\) and the transition matrix is \\[\nP = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix}\n\\] which is the chain that transitions from \\(0 \\to 1 \\to 0 \\to 1 \\cdots\\) deterministically. Notice that the distribution \\(\\pi = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix}\\) is a stationary distribution since \\[\\begin{align*}\n\\pi P &= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} = \\pi\n\\end{align*}\\]\nAdditionally, this chain is clearly irreducible, since every state is reachable from every other state. Therefore, \\(\\pi\\) is the unique stationary distribution from theorem 1. But it is not a limiting distribution. To see why, first note that \\[\\begin{align*}\nP^2 &= \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I\n\\end{align*}\\]\nTherefore, for any \\(n \\in \\mathbb{N}\\): \\[\nP^n = \\begin{cases}\nP^{2k} = (P^2)^k = I & \\text{if } n \\text{ is even} \\\\\nP^{2k + 1} = P^{2k} P = P & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\]\nNow consider an initial distribution (read as a 1x2 matrix) \\(\\mu_0 = \\begin{pmatrix}1 & 0\\end{pmatrix}\\). This means we can make the simplification: \\[\\begin{align*}\n\\mu_0 P^n &= \\begin{pmatrix}1 & 0\\end{pmatrix}P^n \\\\\n&= \\begin{cases}\n\\begin{pmatrix}1 & 0\\end{pmatrix} & \\text{if } n \\text{ is even} \\\\\n\\begin{pmatrix}0 & 1\\end{pmatrix} & \\text{if } n \\text{ is odd}\n\\end{cases}\n\\end{align*}\\]\nSince \\(\\mu_0 P^n\\) alternates between \\(\\begin{pmatrix}1 & 0\\end{pmatrix}\\) and \\(\\begin{pmatrix}0 & 1\\end{pmatrix}\\), the sequence does not converge as \\(n \\to \\infty\\). Therefore, no limiting distribution exists."
  },
  {
    "objectID": "posts/markov-chains/index.html#conclusion",
    "href": "posts/markov-chains/index.html#conclusion",
    "title": "PageRank and Markov Chains",
    "section": "Conclusion",
    "text": "Conclusion\nWe began this journey with a simple question: will the PageRank random surfer always produce the same long term behavior? After developing the full theory of Markov chains, we can now definitively answer this question as yes.\nThe PageRank chain is both irreducible (every page is reachable from every other page due to the random jump mechanism) and aperiodic (we can return to any page in one step with positive probability). By Theorem 2, this guarantees that:\n\nThere exists a unique stationary distribution \\(\\pi\\)\nStarting from any initial distribution \\(\\mu_0\\), the chain converges to \\(\\pi\\) in total variation\nThe convergence is exponentially fast\n\nThis means that no matter where the random surfer starts, after enough time, the proportion of visits to each page will converge to the same values given by \\(\\pi\\). These limiting proportions define a well-defined, consistent ranking of webpage importance.\nBut the power of this theory extends far beyond web search. Irreducible and aperiodic Markov chains appear throughout science, engineering and financial markets. For example, in statistical physics, they can model particle systems reaching thermal equilibrium. In machine learning, they allow for MCMC sampling algorithms like Metropolis-Hastings. In queueing theory, they predict long-run behavior of service systems. Finally, in finance, they model the evolution of asset prices and credit ratings.\nHaving this rigorous theoretical background on Markov Chains now allows you to tackle all these problems."
  },
  {
    "objectID": "posts/markov-chains/index.html#remark-1",
    "href": "posts/markov-chains/index.html#remark-1",
    "title": "PageRank and Markov Chains",
    "section": "Remark",
    "text": "Remark\nThe convergence is exponentially fast with rate \\((1 - \\delta)\\), which depends on the smallest entry of \\(P^N\\). This tells us not just that convergence happens, but also gives us a quantitative bound on how quickly the chain converges. :::"
  }
]