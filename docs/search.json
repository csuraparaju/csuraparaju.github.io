[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "I’m an undergraduate at Carnegie Mellon University studying mathematics and computer science. I blog here about things I find interesting.\nMy favorite classes at CMU have been Theoretical Computer Science (15-251), Markov Chains (21-326) and Computational Biology (02-251).\n\n\nCarnegie Mellon University | Pittsburgh, PA | Aug 2022 - May 2026\nB.S. in Mathematics, Additional Major in Computer Science\n\n\n\nMeta | SWE Intern | May 2025 - Aug 2025\nAmazon | SDE Intern | May 2024 - Aug 2024\nCMU | Head TA for 15-122 | Aug 2023 - present\nMohimani Lab @ CMU | Undergraduate RA | Jan 2023 - Aug 2024"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "Carnegie Mellon University | Pittsburgh, PA | Aug 2022 - May 2026\nB.S. in Mathematics, Additional Major in Computer Science"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Krish Suraparaju",
    "section": "",
    "text": "Meta | SWE Intern | May 2025 - Aug 2025\nAmazon | SDE Intern | May 2024 - Aug 2024\nCMU | Head TA for 15-122 | Aug 2023 - present\nMohimani Lab @ CMU | Undergraduate RA | Jan 2023 - Aug 2024"
  },
  {
    "objectID": "posts/dual-numbers/index.html",
    "href": "posts/dual-numbers/index.html",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "",
    "text": "I recently came across a curious algebraic structure called the Dual Number system, denoted as \\(\\mathbb{R} (\\epsilon)\\). The dual numbers are an extension of the real numbers, with a very interesting property: evaluating a differentiable function \\(f\\) at a dual number \\(x\\) will give us both \\(f(x)\\) and \\(f'(x)\\) at the same time. This is a powerful idea in numerical analysis and machine learning, as it allows for efficient computation of derivatives."
  },
  {
    "objectID": "posts/dual-numbers/index.html#introduction",
    "href": "posts/dual-numbers/index.html#introduction",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "",
    "text": "I recently came across a curious algebraic structure called the Dual Number system, denoted as \\(\\mathbb{R} (\\epsilon)\\). The dual numbers are an extension of the real numbers, with a very interesting property: evaluating a differentiable function \\(f\\) at a dual number \\(x\\) will give us both \\(f(x)\\) and \\(f'(x)\\) at the same time. This is a powerful idea in numerical analysis and machine learning, as it allows for efficient computation of derivatives."
  },
  {
    "objectID": "posts/dual-numbers/index.html#dual-numbers",
    "href": "posts/dual-numbers/index.html#dual-numbers",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Dual Numbers",
    "text": "Dual Numbers\nLet \\(\\epsilon \\neq 0\\) be a new “number” such that \\(\\epsilon^2 = 0\\) (Penn 2022). Before we go any further, we must first ask ourselves if such a number can even exist. The answer is of course not if you want to work in complete fields like \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\). No matter how small a real (or complex) number is, as long as it is non-zero, its square will also be non-zero. So why do we care about this number then?\nOne reason is that it can help us model the notion of an “infinitesimal” from calculus, which can be thought of as a non-zero number that is “smaller” than any other number. Using the definition above then, we note that even though \\(\\epsilon\\) is non-zero, its square is zero, and so the number is somehow “smaller” than any other real (or complex) number. In order to study this more, let’s assume that such a number actually exists and try to see if any laws we know about \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\) does (or does not) break down. Then, let’s see how it can be used to compute derivatives of a function.\nThe structure we want to study is defined as follows \\[\n\\mathbb{R} (\\epsilon) = \\{ a + b \\epsilon \\mid a, b \\in \\mathbb{R} \\}\n\\]\nLet’s see what we can do with this set."
  },
  {
    "objectID": "posts/dual-numbers/index.html#basic-arithmetic",
    "href": "posts/dual-numbers/index.html#basic-arithmetic",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Basic Arithmetic",
    "text": "Basic Arithmetic\nFirst, let’s define how to add, subtract, and multiply. Given a dual number \\(x = a + b \\epsilon\\) and \\(y = c + d \\epsilon\\), define: \\[\\begin{align*}\nx + y &:= (a + c) + (b + d) \\epsilon \\\\\nx - y &:= (a - c) + (b - d) \\epsilon \\\\\nx \\cdot y &:= (a \\cdot c) + (a \\cdot d + d \\cdot c) \\epsilon \\\\\n\\end{align*}\\]\nWhenever we create new objects and define operations on the objects, we should always check that they are well defined. That means, for each operation above we need to check that if \\(x = y\\) and \\(z\\) is any other dual number then \\(x + z = y + z\\) and \\(x - z = y - z\\) and \\(x \\cdot z = y \\cdot z\\). In this case, checking this is very easy so I won’t do that. We can also divide \\(x / y\\), given that \\(c \\neq 0\\): \\[\n\\frac{x}{y} := \\frac{a}{c} + \\frac{b \\cdot c - a \\cdot d}{c^2}\n\\] So far so good. Hopefully it is not hard to see that distributivity, associativity, and commutativity follow because we are simply using \\(+, -, \\cdot\\) from \\(\\mathbb{R}\\). This means all the nice properties we learned in high school about algebra on real numbers (ex. FOIL) still hold. So far so good.\nHowever, things get interesting when we starting playing with multiplication and division operator a bit more. Specifically, what happens when we multiply two numbers \\(0 + b \\epsilon\\) and \\(0 + d \\epsilon\\) where \\(b, d \\neq 0\\). Clearly these are non-zero numbers. However, note that \\[\n(0 + b\\epsilon) + (0 + d\\epsilon) = (0 \\cdot 0) + (0 \\cdot d + b \\cdot 0)\\epsilon = 0\n\\] This s strange! Recall that in \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\) there are no zero divisors. That is, if \\(a, b \\in \\mathbb{R} (\\text{or } \\mathbb{C})\\), and \\(a \\cdot b = 0\\) then either \\(a = 0\\) or \\(b = 0\\). We just saw that this need not be the case for dual numbers anymore! Finally, a place where dual number arithmetic breaks down. Nonetheless, it is still remarkable that most of the basic arithmetic operations and laws from \\(\\mathbb{R}\\) still hold, so let’s see what kind of algebra we can do on these numbers."
  },
  {
    "objectID": "posts/dual-numbers/index.html#polynomials",
    "href": "posts/dual-numbers/index.html#polynomials",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Polynomials",
    "text": "Polynomials\nLet’s consider a simple polynomial function over the real numbers \\(f(x) = x^2\\). We can extend this function to accept dual numbers as input by replacing the real number \\(x = a\\) with the dual number \\(x = a + \\epsilon\\). \\[\nf((a + \\epsilon)) = (a+\\epsilon)\\cdot (a + \\epsilon) = a^2 + (a + a) \\epsilon = a^2 + 2 a \\epsilon\n\\] Notice what just happened! The term \\(2 a \\epsilon\\) is the derivative of the function \\(f\\) evaluated at \\(x = a\\)! By just computing the function \\(f\\) at the modified dual number, we were able to get the derivative as well in one computation!\nLet’s use the Binomial Theorem to see if this generalizes to \\(f(a + \\epsilon) = (a+\\epsilon)^n\\) for an arbitrary \\(n\\): \\[\\begin{align*}\nf(a + \\epsilon) &= (a + \\epsilon)^n \\\\\n&= \\sum_{i = 0}^n {n \\choose i} a^{n - i} \\epsilon^i \\\\\n&= {n \\choose 0} a^n \\epsilon^0 + {n \\choose 1} a^{n-1} \\epsilon^1 + \\sum_{i = 2}^n {n \\choose i} a^{n - i} \\epsilon^i \\\\\n&= a^n + n \\cdot a^{n-1} \\epsilon\n\\end{align*}\\] Where the last step follows because \\(\\epsilon^2 = 0\\) (by definition) and so all the other terms with \\(\\epsilon^i\\) for \\(i \\geq 2\\) cancels out. Notice that this is exactly what we wanted to see: \\(f(a+\\epsilon) = f(a) + f'(a) \\epsilon\\)\nNow, lets go even futher and see if this notion generalizes to an arbitrary polynomial \\(f(a + \\epsilon) = p_n (a + \\epsilon)^n + p_{n-1} (a + \\epsilon)^{n-1} + \\cdots + p_1 (a + \\epsilon) + p_0\\): \\[\\begin{align*}\nf(a + \\epsilon) &= p_n (a + \\epsilon)^n + p_{n-1} (a + \\epsilon)^{n-1} + \\cdots + p_1 (a + \\epsilon) + p_0 \\\\\n&= p_n \\sum_{i = 0}^n {n \\choose i} a^{n - i} \\epsilon^i + p_{n-1} \\sum_{i = 0}^{n-1} {n-1 \\choose i} a^{n - 1- i} \\epsilon^i +p_1 (a + \\epsilon) + p_0  \\\\\n&= p_n (a^n + n ^{n-1} \\epsilon) + p_{n-1} (a^{n-1} + (n-1) a^{n-2} \\epsilon) + \\cdots + p_1 (a + \\epsilon) + p_0 \\\\\n&= p_n a^n + p_n n a^{n-1}\\epsilon + p_{n-1}a^{n-1} + p_{n-1} (n-1)a^{n-2}\\epsilon + \\cdots + p_1 a + p_1 \\epsilon + p_0 \\\\\n&= (p_n a^n + p_{n-1} a^{n-1} + \\cdots + p_1 a + p_0) +(p_n n a^{n-1} + p_{n-1} (n-1) a^{n-2} + \\cdots + p_1) \\epsilon \\\\\n&= f(a) + f'(a) \\epsilon\n\\end{align*}\\] and indeed it does!"
  },
  {
    "objectID": "posts/dual-numbers/index.html#other-functions",
    "href": "posts/dual-numbers/index.html#other-functions",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Other functions",
    "text": "Other functions\nMaybe we can actually make a stronger claim: does this property holds for all infinitely differentiable functions. So, let \\(f\\) be such a function evaluated at the dual number \\(a + c \\epsilon\\) (note that the coefficient for the dual part need not be 1 anymore). Then we can use the taylor series expansion of \\(f\\) to write \\[\\begin{align*}\nf(a + c\\epsilon) &= \\sum_{n = 0}^\\infty \\frac{f^{(n)} (a)}{n!} ((a + c\\epsilon) - a)^n \\\\\n&= \\sum_{n = 0}^\\infty \\frac{f^{(n)} (a)}{n!} (c\\epsilon)^n \\\\\n&= \\frac{f^{(0)} (a)}{0!} (c\\epsilon)^0 + \\frac{f^{(1)} (a)}{1!} (c\\epsilon)^1 + \\sum_{n = 2}^\\infty \\frac{f^{(n)} (a)}{n!} (c\\epsilon)^n \\\\\n& f(a) + f'(a) c \\epsilon\n\\end{align*}\\]\nand again, it does!\nBut what about composition of functions? Let \\(f, g\\) be functions extended to dual numbers such that \\(f(a + c \\epsilon) = f(a) + f'(a) c \\epsilon\\) and \\(g(a + c \\epsilon) = g(a) + g'(a) c \\epsilon\\). Then, note that: \\[\\begin{align*}\nf(g(a+\\epsilon)) &= f(g(a) + g'(a)\\epsilon) \\\\\n&= f(g(a)) + f'(g(a)) \\cdot g'(a) \\epsilon \\\\\n\\end{align*}\\] and notice that the dual component \\(f'(g(a)) \\cdot g'(a)\\) is exactly the chain rule of derivatives! So, now we can imagine all sorts of complicated functions like \\[\nf(x) = \\sin(e^{x^2 + 1})\n\\] and by using the dual number representation, we can compute both the function value and its derivative in a single forward pass through the function."
  },
  {
    "objectID": "posts/dual-numbers/index.html#closing-thoughts",
    "href": "posts/dual-numbers/index.html#closing-thoughts",
    "title": "Automatic Differentiation using Dual Numbers",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nWhat I find most beautiful about dual numbers is how they transform the problem of differentiation from a difficult limiting process \\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\] into a problem of pure algebraic manipulation. This is incredibly beautiful because limits are (arguably) one of the ugliest tools in mathematics, and turning it into a formulation of abstract algebra is extremely satisfying.\nWhile we gained zero divisors in dual numbers, what we got in return makes up for it: a computational method for computing derivatives that is exact (no truncation error like finite differences) and efficient (requires roughly the same number of operations as computing \\(f\\) itself). This is why dual numbers, despite being a somewhat obscure mathematical structure, power automatic differentiation systems used in training neural networks."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html",
    "href": "posts/bourgain-embedding/index.html",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "",
    "text": "Metric embeddings and dimension reduction are important tools in computer science and mathematics for solving the curse of dimensionality. In this post, I want to examine the Bourgain embedding, which shows that any finite metric space can be embedded into an Euclidean metric space space with \\(O(\\log ⁡n)\\) distortion. While Bourgain embeddings are very impractical (the constant factors in the big - O are huge and the “reduced” dimensions is still high), it is an important result that shows us that no matter how “horrible” the initial metric space is, we can be achieve a distortion factor of \\(O(\\log n)\\).\nNote: I’ve written this post assuming that the reader has at least taken an undergraduate discrete math course. However, I will still define the important objects we will be using today.\n\n\nA metric space \\((X, d)\\) is a set \\(X\\) with a distance function \\(d: X \\times X \\to \\mathbb{R}\\). The distance function satisfies the following properties:\n\n\\(d(x, y) \\geq 0\\) for all \\(x, y \\in X\\)\n\\(d(x, y) = 0\\) if and only if \\(x = y\\)\n\\(d(x, y) = d(y, x)\\) for all \\(x, y \\in X\\)\n\\(d(x, y) \\leq d(x, z) + d(z, y)\\) for all \\(x, y, z \\in X\\)\n\nSome examples of metric spaces:\n\nA weighted graph \\(G = (V, E)\\) with \\(X = V\\) and \\(d(x, y) =\\) length of the shortest path between \\(x\\) and \\(y\\).\nThe DNA space with \\(X = \\{A, C, G, T\\}^n\\) and \\(d(x, y) =\\) number of positions where \\(x\\) and \\(y\\) differ.\nThe Euclidean space \\(\\mathbb{R}^n\\) with \\(X = \\mathbb{R}^n\\) and \\(d(x, y) = \\|x - y\\|_2 = \\sqrt{(x_1 - y_1)^2 + \\cdots + (x_n - y_n)^2}\\).\n\n\n\n\nA map \\(f: X \\to Y\\) between two metric spaces \\((X, d_X)\\) and \\((Y, d_Y)\\) is called an embedding that maps elements of \\(X\\) to elements of \\(Y\\).\nThe embedding is said to be distance-preserving (isometric) if \\(d_Y(f(x), f(y)) = d_X(x, y)\\) for all \\(x, y \\in X\\). However, very rarely do we have distance-preserving embeddings between metric spaces. Instead, we often consider embeddings that are “almost” distance-preserving.\nAn embedding with distortion of \\(\\alpha\\) of a metric space \\((X, d_X)\\) into another metric space \\((Y, d_Y)\\) is a map \\(f: X \\to Y\\) such that there exists constant \\(r &gt; 0\\) for which \\[r \\cdot d_X(x, y) \\leq d_Y(f(x), f(y)) \\leq \\alpha r \\cdot d_X(x, y) \\text{ for all } x, y \\in X\\] The distortion of an embedding is the smallest \\(\\alpha\\) for which such a map exists.\nBecause we are working in a finite set for now, we can equivalently define the distortion in terms of the contraction and expansion. Given a map \\(f: X \\to Y\\), let:\n\\[\\text{Contraction}(f) = \\max_{x, y \\in X} \\frac{d_Y(f(x), f(y))}{d_X(x, y)}\\]\n\\[ \\text{Expansion}(f) = \\max_{x, y \\in X} \\frac{d_X(x, y)}{d_Y(f(x), f(y))}\\]\nDefine the distortion of \\(f\\) as \\(\\alpha = \\text{Expansion}(f)\\cdot\\text{Contraction}(f)\\)."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#introduction",
    "href": "posts/bourgain-embedding/index.html#introduction",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "",
    "text": "Metric embeddings and dimension reduction are important tools in computer science and mathematics for solving the curse of dimensionality. In this post, I want to examine the Bourgain embedding, which shows that any finite metric space can be embedded into an Euclidean metric space space with \\(O(\\log ⁡n)\\) distortion. While Bourgain embeddings are very impractical (the constant factors in the big - O are huge and the “reduced” dimensions is still high), it is an important result that shows us that no matter how “horrible” the initial metric space is, we can be achieve a distortion factor of \\(O(\\log n)\\).\nNote: I’ve written this post assuming that the reader has at least taken an undergraduate discrete math course. However, I will still define the important objects we will be using today.\n\n\nA metric space \\((X, d)\\) is a set \\(X\\) with a distance function \\(d: X \\times X \\to \\mathbb{R}\\). The distance function satisfies the following properties:\n\n\\(d(x, y) \\geq 0\\) for all \\(x, y \\in X\\)\n\\(d(x, y) = 0\\) if and only if \\(x = y\\)\n\\(d(x, y) = d(y, x)\\) for all \\(x, y \\in X\\)\n\\(d(x, y) \\leq d(x, z) + d(z, y)\\) for all \\(x, y, z \\in X\\)\n\nSome examples of metric spaces:\n\nA weighted graph \\(G = (V, E)\\) with \\(X = V\\) and \\(d(x, y) =\\) length of the shortest path between \\(x\\) and \\(y\\).\nThe DNA space with \\(X = \\{A, C, G, T\\}^n\\) and \\(d(x, y) =\\) number of positions where \\(x\\) and \\(y\\) differ.\nThe Euclidean space \\(\\mathbb{R}^n\\) with \\(X = \\mathbb{R}^n\\) and \\(d(x, y) = \\|x - y\\|_2 = \\sqrt{(x_1 - y_1)^2 + \\cdots + (x_n - y_n)^2}\\).\n\n\n\n\nA map \\(f: X \\to Y\\) between two metric spaces \\((X, d_X)\\) and \\((Y, d_Y)\\) is called an embedding that maps elements of \\(X\\) to elements of \\(Y\\).\nThe embedding is said to be distance-preserving (isometric) if \\(d_Y(f(x), f(y)) = d_X(x, y)\\) for all \\(x, y \\in X\\). However, very rarely do we have distance-preserving embeddings between metric spaces. Instead, we often consider embeddings that are “almost” distance-preserving.\nAn embedding with distortion of \\(\\alpha\\) of a metric space \\((X, d_X)\\) into another metric space \\((Y, d_Y)\\) is a map \\(f: X \\to Y\\) such that there exists constant \\(r &gt; 0\\) for which \\[r \\cdot d_X(x, y) \\leq d_Y(f(x), f(y)) \\leq \\alpha r \\cdot d_X(x, y) \\text{ for all } x, y \\in X\\] The distortion of an embedding is the smallest \\(\\alpha\\) for which such a map exists.\nBecause we are working in a finite set for now, we can equivalently define the distortion in terms of the contraction and expansion. Given a map \\(f: X \\to Y\\), let:\n\\[\\text{Contraction}(f) = \\max_{x, y \\in X} \\frac{d_Y(f(x), f(y))}{d_X(x, y)}\\]\n\\[ \\text{Expansion}(f) = \\max_{x, y \\in X} \\frac{d_X(x, y)}{d_Y(f(x), f(y))}\\]\nDefine the distortion of \\(f\\) as \\(\\alpha = \\text{Expansion}(f)\\cdot\\text{Contraction}(f)\\)."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#bourgain-embedding",
    "href": "posts/bourgain-embedding/index.html#bourgain-embedding",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "Bourgain Embedding",
    "text": "Bourgain Embedding\nGiven an arbitrary finite metric space \\((X, d)\\) with \\(n\\) points, Bourgain’s theorem says that there exists a map \\(f: X \\to \\mathbb{R}^k\\) such that the distortion of \\(f\\) is \\(\\alpha \\in O(\\log n)\\), and \\(k \\in O(\\log^2 n)\\). The proof of this theorem is beyond the scope of this post, but it is a constructive proof so there is a natural algorithm that arises from the proof which we can implement.\nThe Bourgain embedding algorithm is as follows (Ye 2023):\n\nLet \\(c\\) be a sufficiently large constant, and let \\(\\log(n)\\) denote the base-2 logarithm of \\(n\\).\nFor each point \\(x \\in X\\), define its embedding vector \\(f(x)\\) in the following steps:\n\nFor \\(i \\in \\{1, 2, \\dots, \\lceil \\log_2(n) \\rceil \\}\\):\n\nFor \\(j \\in \\{1, 2, \\dots, c \\cdot \\lceil \\log_2(n) \\rceil \\}\\):\n\nChoose a random subset \\(S_{ij} \\subseteq X\\), where each \\(y \\in X\\) is included in \\(S_{ij}\\) with probability \\(2^{-i}\\).\nCompute \\(d(x, S_{ij})\\), the minimum distance from \\(x\\) to any point in \\(S_{ij}\\).\n\nConstruct the embedding vector: \\(f(x) = \\langle d(x, S_{11}), d(x, S_{12}), \\dots, d(x, S_{\\lceil \\log_2(n) \\rceil \\cdot c \\cdot \\lceil \\log_2(n) \\rceil})\\rangle.\\)\n\n\n\nAn intuition for the algorithm is that it creates a “fingerprint” for each point by measuring its distance to random subsets of the space at multiple scales. So, for each point, we answer the question:\n\n“How far am I from a randomly chosen half of all points?”\n“How far am I from a randomly chosen quarter of all points?”\n“How far am I from a randomly chosen eighth of all points?”\n…\n\nWhy does this capture distance information? Imagine two points \\(x\\) and \\(y\\) in our original metric space. If they are close, they’ll have similar distances to most random subsets. When we sample a random subset \\(S\\), chances are the nearest point in \\(S\\) to \\(x\\) will also be close to \\(y\\) so, the embedded distances will also be similar.\nHowever, if \\(x\\) and \\(y\\) are far apart, then at some scale, we’ll sample points that “separate” them. That is, there will be random subsets where \\(x\\) is close to some sampled point but \\(y\\) is far from all sampled points (or vice versa). This creates a difference in their fingerprints that the algorithm can capture when construction the embedding.\nNote that in practice, this algorithm can be really bad because of the big-O constants. For example, if \\(X = \\mathbb{R}^{1000}\\), and we chose \\(c = 100\\) (for less distortion), then the reduced dimension \\(k = 100 \\cdot \\log^2{1000} = 900\\). This is an improvement over the \\(1000\\) dimensional space, but just barely."
  },
  {
    "objectID": "posts/bourgain-embedding/index.html#implementation",
    "href": "posts/bourgain-embedding/index.html#implementation",
    "title": "Metric Spaces, dimension reduction, and Bourgain Embeddings",
    "section": "Implementation",
    "text": "Implementation\nLet’s implement this in python using numpy so we can vectorize parts of the code.\n\nimport numpy as np\n\ndef bourgain_embedding(dist_mat, c=10):\n    n = len(dist_mat)\n\n    if n &lt;= 1:\n        return np.zeros((n, 1))\n\n    log_n = int(np.ceil(np.log2(n)))\n    k = c * log_n * log_n\n    max_dist = dist_mat.max()\n\n    f_x = np.zeros((n, k))\n\n    j = 0\n    for i in range(1, log_n + 1):\n        p = 2 ** (-i)\n        for _ in range(c * log_n):\n            subset_mask = np.random.rand(n) &lt; p\n\n            if not np.any(subset_mask):\n                f_x[:, j] = max_dist\n            else:\n                f_x[:, j] = dist_mat[:, subset_mask].min(axis=1)\n            j += 1\n\n    return f_x\n\nWe can plot the distortion of this algorithm on a randomly generated metric space, as a function of \\(n\\) to see the logarithmic curve.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef distortion(dist_mat, emb):\n    n = dist_mat.shape[0]\n    max_expand = 0.0\n    max_contract = 0.0\n\n    for i in range(n):\n        for j in range(i + 1, n):\n            d0 = dist_mat[i, j]\n            d1 = np.linalg.norm(emb[i] - emb[j], ord=1)\n\n            if d0 &gt; 0 and d1 &gt; 0:\n                max_expand = max(max_expand, d1 / d0)\n                max_contract = max(max_contract, d0 / d1)\n\n    return max_expand * max_contract\n\ndef gen_random_space(n, d=5, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    X = np.random.randn(n, d)\n    diff = X[:, None, :] - X[None, :, :]\n    return np.linalg.norm(diff, axis=2)\n\nc_fixed = 8\nns = [10, 50, 100, 350, 500, 750, 1000]\n\ndistortions = []\nfor n in ns:\n    dist_mat = gen_random_space(n, d=5, seed=0)\n    emb = bourgain_embedding(dist_mat, c=c_fixed)\n    distortions.append(distortion(dist_mat, emb))\n\nlog_ns = np.log(ns)\nlog_ns_scaled = log_ns / log_ns[0] * distortions[0]\n\nplt.figure(figsize=(7, 5))\nplt.plot(ns, distortions, marker='o', label=\"Bourgain distortion\")\nplt.plot(ns, log_ns_scaled, linestyle='--', label=r\"scaled $\\log n$\")\n\nplt.xlabel(r\"$n$\")\nplt.ylabel(\"Distortion\")\nplt.title(\"Bourgain embedding distortion vs log n\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNote that the variable \\(c\\) is a hyper-parameter to this algorithm, and we can observe distortion as a function of \\(c\\) as well.\n\n\nCode\nn_fixed = 100\ncs = [10, 50, 100, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300]\n\ndistortions = []\nfor c in cs:\n    dist_mat = gen_random_space(n_fixed, d=5, seed=0)\n    emb = bourgain_embedding(dist_mat, c=c)\n    distortions.append(distortion(dist_mat, emb))\n\n\nplt.figure(figsize=(7, 5))\nplt.plot(cs, distortions, marker='o', label=\"Bourgain distortion\")\n\nplt.xlabel(r\"$c$\")\nplt.ylabel(\"Distortion\")\nplt.title(\"Bourgain embedding distortion vs c\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/noisy-channels/index.html",
    "href": "posts/noisy-channels/index.html",
    "title": "Sending Messages over Noisy Channels",
    "section": "",
    "text": "A fundamental problem in electrical engineering is sending messages over noisy channels. For example, all messages sent over the internet have to eventually be transmitted over physical channels (copper wires or fiber optic cables), and these channels can be damaged by the elements (eg. sharks)\nI recently came across a method to reduce this problem to a purely mathematical one: the sphere packing problem, which aims to find the densest packing of non-overlapping \\(n\\) dimensional spheres in a given space."
  },
  {
    "objectID": "posts/noisy-channels/index.html#introduction",
    "href": "posts/noisy-channels/index.html#introduction",
    "title": "Sending Messages over Noisy Channels",
    "section": "",
    "text": "A fundamental problem in electrical engineering is sending messages over noisy channels. For example, all messages sent over the internet have to eventually be transmitted over physical channels (copper wires or fiber optic cables), and these channels can be damaged by the elements (eg. sharks)\nI recently came across a method to reduce this problem to a purely mathematical one: the sphere packing problem, which aims to find the densest packing of non-overlapping \\(n\\) dimensional spheres in a given space."
  },
  {
    "objectID": "posts/noisy-channels/index.html#signals-and-code",
    "href": "posts/noisy-channels/index.html#signals-and-code",
    "title": "Sending Messages over Noisy Channels",
    "section": "Signals and Code",
    "text": "Signals and Code\nBut first, let’s define the noisy channel problem more formally. Let \\(T &gt; 0\\) be a fixed length of time corresponding to the length of a signal transmission.\nA signal is a continuous function \\(s : [0, T] \\to \\mathbb{R},\\) where \\(s(t)\\) is the amplitude of the signal at time \\(t\\), and the frequencies do not surpass some fixed limit \\(W\\). Think of \\(s(t)\\) as the voltage of the signal at time \\(t\\) (for copper wires), or the intensity of a light signal at time \\(t\\) (for fiber optic cables).\nA code is a finite set of signals \\(\\{s_1, s_2, \\ldots, s_n\\}.\\) This can be thought of as a symbolic alphabet for two computers to communicate over a channel. A simple example of a code is \\[\n\\{ s_1(t) = 1,\\; s_2(t) = -1 \\},\n\\] which can be used to send binary messages over a channel.\nThe Shannon–Nyquist Sampling Theorem states that any signal \\(s(t)\\) with frequencies less than \\(W\\) can be uniquely represented by a finite set of samples \\[\n\\left\\{ s(0), s\\!\\left(\\tfrac{1}{W}\\right), s\\!\\left(\\tfrac{1}{2W}\\right), \\ldots, s\\!\\left(\\tfrac{n-1}{2W}\\right) \\right\\},\n\\] where \\(n = 2WT\\). This means that we can represent any signal \\(s(t)\\) as a vector \\(\\vec{s} \\in \\mathbb{R}^n,\\) and any code as a finite subset \\(C \\subseteq \\mathbb{R}^n\\), where each element of \\(C\\) represents a signal.\n\n\n\n\n\nThe continuous signal above \\(S(t)\\) represented by the discrete samples \\(S_i\\). Therefore, we will represent a signal as a vector in the remaining discussion."
  },
  {
    "objectID": "posts/noisy-channels/index.html#the-noisy-channel",
    "href": "posts/noisy-channels/index.html#the-noisy-channel",
    "title": "Sending Messages over Noisy Channels",
    "section": "The Noisy Channel",
    "text": "The Noisy Channel\nIn the real world, the sent signal \\(\\vec{s}\\) is almost never the same as the received signal \\(\\vec{r}\\). This is because the channel introduces noise, which we model as a random perturbation in the input.\nFormally, we assume that the received signal is \\[\n\\vec{r} = \\vec{s} + \\vec{z},\n\\] where \\(\\vec{z}\\) is a random vector with Gaussian entries (each \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\) and is i.i.d). If the receiver wants to determine which signal was sent, a natural decoding strategy is nearest-neighbor decoding: \\[\n\\hat{\\vec{s}} = \\underset{\\vec{s_i} \\in C}{\\text{argmin}} \\| \\vec{r} - \\vec{s_i} \\|_2.\n\\]\nHowever, if two signals in the code are too close together, noise may make decoding ambiguous. For example, in the following situation:\n\n\n\n\n\nwhich signal does \\(\\vec{r}\\) correspond to? Both \\(\\vec{s_1}\\) and \\(\\vec{s_2}\\) are equally likely.\n\n\nTo solve this problem, remember that we assumed \\(\\vec{r} = \\vec{s} + \\vec{z}\\) and that each \\(z_i \\sim \\mathcal{N}(0, \\sigma^2)\\). A fundamental property of the Gaussian distribution is that \\[\n\\mathbb{P}[-3 \\sigma \\leq z_i \\leq 3 \\sigma] \\approx 99.7\\%\n\\] So, we can use this to figure out a bound on the distance between \\(\\vec{r}\\) and \\(\\vec{s}\\) with high probability. In the worst-case, each component is such that \\(|z_i| = 3 \\sigma\\). Since we have \\(n\\) components, this means that \\[\\begin{align*}\n\\| \\vec{r} - \\vec{s} \\|_2 =  \\| \\vec{z}\\|_2 &= \\sqrt{\\sum_{i = 1}^{n} z_i^2} \\\\ &\\leq\\sqrt{\\sum_{i = 1}^{n} (3 \\sigma)^2} \\\\ &\\leq \\sqrt{n \\cdot 9 \\sigma^2} \\\\ &\\leq 3 \\sigma \\sqrt{n}\n\\end{align*}\\] with probability \\(99.7\\%\\). This means that the worst-case noise can push the received signal \\(\\vec{r}\\) up to a distance of \\(3 \\sigma \\sqrt{n}\\) away from the true signal \\(\\vec{s}\\). This creates a sphere of radius \\(3 \\sigma \\sqrt{n}\\) around \\(\\vec{s_1}\\). To make our decoding as unambiguous as possible, we need to ensure that even when noise pushes \\(\\vec{r}\\) to the edge of this sphere, it is still closer to \\(\\vec{s}\\) than any other signal \\(\\vec{s_2} \\in C\\). But signal \\(\\vec{s_2}\\) also has a sphere of radius \\(3 \\sigma \\sqrt{n}\\) around it. Therefore, to guarantee that two spheres don’t overlap, the minimum distance between any two signals must be at least \\(6 \\sigma \\sqrt{n}\\)"
  },
  {
    "objectID": "posts/noisy-channels/index.html#dense-sphere-packing",
    "href": "posts/noisy-channels/index.html#dense-sphere-packing",
    "title": "Sending Messages over Noisy Channels",
    "section": "Dense Sphere packing",
    "text": "Dense Sphere packing\nNow, you might ask: why don’t we just pick signals that are as far apart as possible? For example, we could place just two signals at opposite ends of our signal space.\nBut recall that a signal \\(\\vec{s}\\) represents the amplitude or voltage of a message over time. Physicists have told us the power of a signal is proportional to the square of its amplitude: \\[\n\\text{Power} \\propto \\|\\vec{s}\\|_2^2 = \\sum_{i=1}^{n} s_i^2\n\\]\nIn practice, we cannot transmit signals with arbitrarily large power. There are physical limits on how much voltage we can apply to a copper wire and how much light we can send through a fiber optic cable. If we set a maximum power budget \\(P\\), then all our signals must satisfy \\[\n\\|\\vec{s}\\|_2^2 \\leq nP \\quad \\Longrightarrow \\quad \\|\\vec{s}\\|_2 \\leq \\sqrt{nP}\n\\]\nThis means all our signals must lie within a sphere of radius \\(\\sqrt{nP}\\) in \\(\\mathbb{R}^n\\), which is a compact region. The more signals we can fit in this region, the more information we can transmit per signal. For example, a code with 2 signals transmits only 1 bit per transmission, but a code with 256 signals transmits 8 bits per transmission.\nAt the same time, each signal needs a sphere of radius \\(3\\sigma\\sqrt{n}\\) around it to ensure correct decoding with probability \\(99.7\\%\\). This is exactly the sphere packing problem: what is the maximum number of non-overlapping spheres of radius \\(3\\sigma\\sqrt{n}\\) that we can pack inside a sphere of radius \\(\\sqrt{nP}\\)? Each sphere center represents a signal in our code, and finding the densest packing directly gives us the code that can transmit the most information reliably. Therefore, the problem of designing optimal codes for noisy channels reduces to an entirely abstract problem in math."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "As each new author finds cleverer proofs or treatments of a theory, the treatment evolves towards the one that contains the ‘shortest proofs’. Unfortunately, these are often in a form that causes the new student to ponder ‘How did anyone think this?’ By going back to the original sources, one can usually see the subject evolving naturally.” — Peter Sarnak\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Differentiation using Dual Numbers\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nApr 8, 2025\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nSending Messages over Noisy Channels\n\n\n\nmath\n\nece\n\n\n\n\n\n\n\n\n\nDec 25, 2024\n\n\nKrish Suraparaju\n\n\n\n\n\n\n\n\n\n\n\n\nMetric Spaces, dimension reduction, and Bourgain Embeddings\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\nKrish Suraparaju\n\n\n\n\n\nNo matching items"
  }
]